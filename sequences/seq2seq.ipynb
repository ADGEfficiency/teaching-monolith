{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# seq2seq\n",
    "\n",
    "Sequence to sequence allows variable length input & output sequences\n",
    "- major contribution = variable length sequences\n",
    "- can be used with LSTMs or attention\n",
    "\n",
    "Sutskever et. al (2014) Sequence to Sequence Learningwith Neural Networks - [arvix](https://arxiv.org/abs/1409.3215)\n",
    "- reverses the order of words in the source (not the target)\n",
    "- 12 million Eng -> French sentences\n",
    "- 10 days to train across 8 GPUs\n",
    "\n",
    "Applications\n",
    "- translation, question answering, image captioning\n",
    "\n",
    "One limitation of seq2seq is knowning the set of possible output elements in advance\n",
    "- limits seq2seq to solve problems like sorting or travelling salesman\n",
    "- Pointer Net (Vinyals et. al 2015) to solve this\n",
    "\n",
    "\n",
    "## Embeddings\n",
    "\n",
    "Text / sequence is first tokenized.  \n",
    "- transforming the tokens (that form our corpus) into a vector\n",
    "\n",
    "Special tokens include:\n",
    "- `<EOS>` = end of sentence\n",
    "- `<PAD>` = padding (if you require constant length sequences)\n",
    "\n",
    "Frequency based embeddings include bag of words or TIDF:\n",
    "- these are commonly used to generate features for non-neural network NLP approaches\n",
    "\n",
    "It is more common with neural networks to use vector embeddings:\n",
    "- each word in corpus is mapped to a vector\n",
    "- can be learnt unsupervised by predicting current word - know as Continuous Bag-of-Words (CBOW)\n",
    "\n",
    "Word2Vec (all Mikolov et. al (2013))\n",
    "- Efficient Estimation of Word Representations in Vector Space\n",
    "- Linguistic Regularities in Continuous Space Word Representations\n",
    "\n",
    "![](assets/word-embed.png)\n",
    "*From Linguistic Regularities in Continuous Space Word Representations*\n",
    "\n",
    "Word embeddings can be reused\n",
    "- transfer learning\n",
    "- driving lots of recent progress in NLP\n",
    "\n",
    "Further reading\n",
    "- What are Word Embeddings - ML Mastery - [text](https://machinelearningmastery.com/what-are-word-embeddings/)\n",
    "\n",
    "## Encoder-Decoder architecture\n",
    "\n",
    "![](assets/seq2seq.png)\n",
    "\n",
    "*From Sutskever et. al (2014)*\n",
    "\n",
    "- encoder processes input seq using an LSTM to a fixed length embedding (context vector)\n",
    "- decoder is initialized with context, then generates the output sequence\n",
    "\n",
    "Encoder output = like a sentence embedding\n",
    "\n",
    "Encoder -> fixed length context (aka sentence embedding, thought vector), that summarizes entire sentence\n",
    "\n",
    "Decoder = initialized with context -> output\n",
    "\n",
    "![](assets/enc-dec.png)\n",
    "\n",
    "\n",
    "## Resources used\n",
    "\n",
    "Attention? Attention! - Lilian Wang - [text](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)\n",
    "\n",
    "What are Word Embeddings - ML Mastery - [text](https://machinelearningmastery.com/what-are-word-embeddings/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
