{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequences\n",
    "\n",
    "A simple example of sequence prediction - $[0, 1, 2] \\rightarrow [3, 4, 5]$\n",
    "\n",
    "Applications / examples of sequences problems\n",
    "- language modelling\n",
    "- translation\n",
    "- time series\n",
    "- generating image captions\n",
    "\n",
    "We can have different types of sequence problem structures\n",
    "\n",
    "![](assets/sequences.png)\n",
    "\n",
    "The many to many structure can also be thought of as an encoder-decoder structure:\n",
    "\n",
    "![](assets/quoc-le.png)\n",
    "\n",
    "## Problems with dense networks\n",
    "\n",
    "Stateless\n",
    "\n",
    "Unaware of temporal structure\n",
    "\n",
    "Fixed size inputs & outputs\n",
    "\n",
    "## Promise of recurrent neural networks\n",
    "\n",
    "Network able to learn a mapping from inputs over time\n",
    "- outputs become conditional the context of the sequence\n",
    "\n",
    "Learn the temporal dependence of data\n",
    "\n",
    "An RRN is Turing complete\n",
    "- they can simulate arbitrary programs\n",
    "\n",
    "## Being comfortable in three dimensions\n",
    "\n",
    "We model the temporal structure in data using a dimension in an array - by convention this is the second dimension.\n",
    "\n",
    "Our dimensions then are: \n",
    "- the batch dimension (number of samples)\n",
    "- timesteps\n",
    "- features\n",
    "\n",
    "## Practical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "batch_size = 1000\n",
    "timesteps = 32\n",
    "features = 16\n",
    "\n",
    "samples = tf.random.uniform((batch_size, timesteps, features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select all samples, first timestep, all features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=10, shape=(1000, 16), dtype=float32, numpy=\n",
       "array([[0.46424544, 0.7721001 , 0.3713907 , ..., 0.575696  , 0.7400645 ,\n",
       "        0.75130725],\n",
       "       [0.7625606 , 0.9685751 , 0.42492294, ..., 0.7437755 , 0.18617857,\n",
       "        0.01660204],\n",
       "       [0.99516654, 0.57923126, 0.93059003, ..., 0.21720362, 0.5128175 ,\n",
       "        0.4847033 ],\n",
       "       ...,\n",
       "       [0.6671653 , 0.658352  , 0.10562837, ..., 0.11620092, 0.6198951 ,\n",
       "        0.9504447 ],\n",
       "       [0.15319824, 0.16427588, 0.41125262, ..., 0.0280416 , 0.4160682 ,\n",
       "        0.43512297],\n",
       "       [0.81149054, 0.88620865, 0.29803014, ..., 0.44782937, 0.04601979,\n",
       "        0.2196101 ]], dtype=float32)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples[:, 0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last sample, all timesteps, first feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=15, shape=(32,), dtype=float32, numpy=\n",
       "array([0.81149054, 0.36649883, 0.42218685, 0.15335298, 0.904534  ,\n",
       "       0.66553354, 0.2674117 , 0.0316906 , 0.12106466, 0.5772643 ,\n",
       "       0.0232017 , 0.4743743 , 0.17386997, 0.36009216, 0.49845338,\n",
       "       0.49139953, 0.7374474 , 0.17315185, 0.3671106 , 0.9532969 ,\n",
       "       0.46684217, 0.9984925 , 0.99783885, 0.47813845, 0.8609488 ,\n",
       "       0.3230902 , 0.57820535, 0.98574424, 0.41925073, 0.4170395 ,\n",
       "       0.8973566 , 0.8503436 ], dtype=float32)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples[-1, :, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ninth sample, sixth timestep, all features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=20, shape=(16,), dtype=float32, numpy=\n",
       "array([0.02393913, 0.10989189, 0.81392586, 0.01238775, 0.26136005,\n",
       "       0.8693919 , 0.08746338, 0.8578043 , 0.40168118, 0.8016467 ,\n",
       "       0.9237622 , 0.87369466, 0.7280284 , 0.48793352, 0.51919794,\n",
       "       0.2144723 ], dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples[8, 5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent neural networks\n",
    "\n",
    "A recurrent neural network passes it's outputs to itself at each timestep\n",
    "- the state is the prediction from the last timestep\n",
    "\n",
    "![](assets/unrolled.png)\n",
    "\n",
    "Let's model the forward pass of a recurrent neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'state_weights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-80a1b8ca6b7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprevious_state_weights\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m )\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'state_weights' is not defined"
     ]
    }
   ],
   "source": [
    "#  data\n",
    "samples = np.random.uniform(size=(4, 3, 2))\n",
    "\n",
    "#  architecture & weights\n",
    "nodes = 4\n",
    "previous_state_weights = np.random.uniform(size=(nodes, nodes))\n",
    "feature_weights = np.random.uniform(size=(samples.shape[2], nodes))\n",
    "#state_weights = np.random.uniform(size=(nodes, 8))\n",
    "\n",
    "#  initial state\n",
    "state = np.zeros(nodes)\n",
    "\n",
    "#  update the hidden state\n",
    "#  use tanh to help deal with vanishing gradients\n",
    "#  tanh squeezes between -1 to 1\n",
    "state = np.tanh(\n",
    "    state.dot(previous_state_weights) + samples[0][0].dot(feature_weights)\n",
    ")\n",
    "output = state.dot(state_weights)\n",
    "\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = np.tanh(\n",
    "    state.dot(previous_state_weights) + samples[0][1].dot(feature_weights)\n",
    ")\n",
    "\n",
    "output = state.dot(state_weights)\n",
    "\n",
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backprop through time\n",
    "\n",
    "Backpropagating error requires error to flow backwards in time\n",
    "- error must flow back to the first time step to calculate gradients\n",
    "\n",
    "The loss function for a given layer depends not only on its infulence on layers below it - but also on the layer at the next time step\n",
    "\n",
    "Backproping through time means unrolling, which makes\n",
    "-  the memory footprint of recurrent neural network large\n",
    "- parallel training on multiple sequences inefficient on hardware that shares memory (i.e. GPU)\n",
    "\n",
    "Further reading - see *Truncated Backprop Through Time*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Character level language modeling\n",
    "\n",
    "Lets use a recurrent neural network to predict the next letter in the word *goodbye!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  we will model many to many \n",
    "#  feeding in the entire input sequence then reading the output sequence\n",
    "\n",
    "def encode(alphabet, samples, seq_len):\n",
    "    samples = np.array(samples)\n",
    "    encoding = np.zeros((samples.shape[0], seq_len, len(alphabet)))\n",
    "\n",
    "    for row, sample in enumerate(samples):\n",
    "        for se in range(seq_len):\n",
    "            try: \n",
    "                char = samples[row][se]\n",
    "                idx = alphabet.index(char)\n",
    "                encoding[row, se, idx] = 1\n",
    "            except IndexError:\n",
    "                import pdb; pdb.set_trace()\n",
    "            \n",
    "    assert (np.sum(encoding, axis=2) == 1).all()\n",
    "    return encoding\n",
    "\n",
    "\n",
    "def make_dataset(word, seq_len):\n",
    "    words = word * 50\n",
    "    \n",
    "    indicies = np.random.randint(seq_len, len(words) - seq_len*2, size=100)\n",
    "\n",
    "    f, t = zip(*[\n",
    "        [words[idx:idx+seq_len], words[idx+seq_len:idx+seq_len*2]] for idx in indicies\n",
    "    ])\n",
    "    \n",
    "    alphabet = list(set(word))\n",
    "\n",
    "    return encode(alphabet, f, seq_len), encode(alphabet, t, seq_len), alphabet\n",
    "\n",
    "f, t, alphabet = make_dataset('goodbye!', seq_len=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [keras.layers.SimpleRNN(8, return_sequences=True),\n",
    "     keras.layers.Dense(t.shape[2], activation='softmax')]\n",
    ")\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "h = model.fit(f, t, epochs=100, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(alphabet, encoded):\n",
    "    #  single sample only\n",
    "    return [alphabet[v] for v in encoded.flatten()]\n",
    "\n",
    "test = encode(alphabet, np.array(['goo']), seq_len=3)\n",
    "decode(alphabet, np.argmax(model.predict(test), axis=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = encode(alphabet, np.array(['bye']), seq_len=3)\n",
    "decode(alphabet, np.argmax(model.predict(test), axis=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = encode(alphabet, np.array(['!go']), seq_len=3)\n",
    "decode(alphabet, np.argmax(model.predict(test), axis=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functional Keras API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, t, alphabet = make_dataset('hello', seq_len=3)\n",
    "\n",
    "feat = tf.keras.Input(shape=(3, 4))\n",
    "rnn = keras.layers.SimpleRNN(8, return_sequences=True)(feat)\n",
    "classes = keras.layers.Dense(t.shape[2], activation='softmax')(rnn)\n",
    "\n",
    "model = tf.keras.Model(inputs=feat, outputs=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "h = model.fit(f, t, epochs=100, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(f[0].reshape(1, 3, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical\n",
    "\n",
    "Train a recurrent network to predict the next letter in a word.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
