{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection\n",
    "\n",
    "Probably not as important as you think\n",
    "- it is all about the data\n",
    "\n",
    "**Data drives model selection**\n",
    "- key question = what structure does my data have (spatial, temporal)\n",
    "\n",
    "## Cheat sheet\n",
    "\n",
    "Tables = XGBoost\n",
    "\n",
    "Images = pretrained conv. net\n",
    "\n",
    "Text = TIDF + XGBoost, transformers, pretrained embeddings\n",
    "\n",
    "Temporal (time series) = classical / XGBoost / recurrent / transformers\n",
    "\n",
    "Geospatial = ? (probably convolution)\n",
    "\n",
    "[Lecture 8: Troubleshooting Deep Neural Networks - Full Stack Deep Learning - March 2019](https://youtu.be/GwGTwPcG0YM):\n",
    "\n",
    "![](assets/arch.png)\n",
    "\n",
    "## Should I use machine learning?\n",
    "\n",
    "First use a manageable set of deterministic rules\n",
    "- understandable, easy to build & maintain, don't need data\n",
    "\n",
    "Then a baseline / naive model\n",
    "- implement first\n",
    "- use for comparison\n",
    "- this can be a feature later on\n",
    "\n",
    "Then increasingly complex models\n",
    "\n",
    "Model development process should be iterative\n",
    "- iterations of minimum viable products that are tested with customers\n",
    "\n",
    "![](assets/iteration.png)\n",
    "\n",
    "[Building Machine Learning Powered Applications](https://www.oreilly.com/library/view/building-machine-learning/9781492045106/) - original drawing from Henrik Kniberg\n",
    "\n",
    "## Model qualities\n",
    "\n",
    "Accuracy\n",
    "\n",
    "Inference time (latency)\n",
    "\n",
    "Ease of implementation\n",
    "- dependenices\n",
    "- GPU\n",
    "\n",
    "Interpretability\n",
    "\n",
    "## No free lunch\n",
    "\n",
    "[Fran√ßois Chollet (2019) On the Measure of Intelligence](https://arxiv.org/abs/1911.01547)\n",
    "\n",
    "Two optimization algorithms (including human intelligence) are equivilant when their performance is averaged across all possible problems\n",
    "- our universe is only a subset of all possible problems!\n",
    "\n",
    "Choosing an appropriate algorithm requires **making assumptions** about the kinds of target functions the algorithm is being used for\n",
    "- same as building priors into a model (such as using convolution)\n",
    "- inductive bias\n",
    "\n",
    "## Types of data science problems\n",
    "\n",
    "Regression or classification\n",
    "\n",
    "Knowledge extraction from unstructured data\n",
    "- sentiment from text\n",
    "- object detection\n",
    "- feature maps\n",
    "\n",
    "Catalog organization\n",
    "- search\n",
    "- collaborative recommendation (based on user data)\n",
    "- content based recommendation (based on metadata)\n",
    "\n",
    "Generative\n",
    "- style transfer\n",
    "\n",
    "Questions to class - what are\n",
    "- speech recognition\n",
    "- fraud detection\n",
    "- question / answering\n",
    "- part of speech tagging\n",
    "\n",
    "## Neural networks\n",
    "\n",
    "[Andrej Karpathy Twitter](https://twitter.com/karpathy/status/1013244313327681536?lang=en)\n",
    "\n",
    "Most common neural net mistakes: \n",
    "1. you didn't try to overfit a single batch first. \n",
    "2. you forgot to toggle train/eval mode for the net. \n",
    "3. you forgot to .zero_grad() (in pytorch) before .backward(). \n",
    "4. you passed softmaxed outputs to a loss that expects raw logits.\n",
    "5. you didn't use bias=False for your Linear/Conv2d layer when using BatchNorm, or conversely forget to include it for the output layer .This one won't make you silently fail, but they are spurious parameters\n",
    "6. thinking view() and permute() are the same thing (& incorrectly using view)\n",
    "7. not turning dropout off at test time\n",
    "\n",
    "[A Recipe for Training Neural Networks - Andrej Karpathy blog](http://karpathy.github.io/2019/04/25/recipe/)\n",
    "\n",
    "- Neural net training fails silently\n",
    "- use Adam with a learning rate of 3e-4\n",
    "- random over grid search - best to use random search instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
