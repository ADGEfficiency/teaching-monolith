{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uq requests bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping\n",
    "\n",
    "## What is web scraping?\n",
    "\n",
    "**Web scraping is two sequential steps**\n",
    "1. fetching a webpage HTML\n",
    "2. extracting data from the HTML\n",
    "\n",
    "## Am I allowed to scrape?\n",
    "\n",
    "*I'm not a lawyer, and don't play one on the internet*\n",
    "\n",
    "Web scraping involves extracting data from HTML\n",
    "- this HTML (& data) is publically available through an HTTP request\n",
    "- you only get back what they send you\n",
    "\n",
    "`robots.txt` \n",
    "- way for websites to tell crawlers & webscrapers what is allowed or not\n",
    "- for example - https://www.theguardian.com/robots.txt\n",
    "\n",
    "You should be polite\n",
    "- tell the website who you are (`user-agent`)\n",
    "- don't spam requests - consider adding a `time.sleep` in between requests\n",
    "- spamming the server is not polite\n",
    "- if they offer an API, use that instead\n",
    "\n",
    "If you ever use data from web scraping commercially\n",
    "\n",
    "- check for copyright\n",
    "- i.e. couldn't scrape videos from YouTube & repost \n",
    "\n",
    "## Fetching HTML\n",
    "\n",
    "Web scraping is two steps\n",
    "1. fetching a webpage HTML\n",
    "2. extracting data from the HTML\n",
    "\n",
    "We will be scraping the Wikipedia page for [Yann LeCun](https://en.wikipedia.org/wiki/Yann_LeCun) - one of the three recipients of the 2018 Turing award for work in Deep Learning - the other two being [Geoffery Hinton](https://en.wikipedia.org/wiki/Geoffrey_Hinton) and [Yoshua Bengio](https://en.wikipedia.org/wiki/Yoshua_Bengio).\n",
    "\n",
    "Let's do an HTTP request to the Wikipedia URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://en.wikipedia.org/wiki/Yann_LeCun')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the HTML content we get back using `.text`.\n",
    "\n",
    "This is the same HTML that your browser uses to render a page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.text[:250]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the web?\n",
    "\n",
    "We think of the web as a collection of pages\n",
    "\n",
    "- in fact, the web is a collection of users (usually web browsers, can also be servers) and servers\n",
    "\n",
    "A better mental for the web is a conversation between users & servers\n",
    "\n",
    "## What is a server?\n",
    "\n",
    "It's just a computer running a program\n",
    "\n",
    "- i.e. Flask, which is a Python program\n",
    "\n",
    "Servers can also run & be accessed locally\n",
    "\n",
    "- this is how we use Jupyter Lab :)\n",
    "\n",
    "## HTTP - what happens when you visit a website?\n",
    "\n",
    "This is the kind of conversation that happens when you access a page on the internet:\n",
    "\n",
    "*CLIENT* - request to https://www.reddit.com\n",
    "\n",
    "*SERVER* - I'm the server hosting reddit.com - what page would you like?\n",
    "\n",
    "*CLIENT* - Please give me https://www.reddit.com/r/MachineLearning/\n",
    "\n",
    "*SERVER* - Sure -> sends text files\n",
    "\n",
    "*CLIENT* - Thanks! -> renders text files in browser\n",
    "\n",
    "This kind of conversation is had every time you access a webpage\n",
    "\n",
    "- **it's also the same thing that happens when we do `requests.get`!\n",
    "\n",
    "*Further reading*\n",
    "[Interactive Data Visualization for the Web - Scott Murray](oreilly.com/library/view/interactive-data-visualization/9781449340223/) - in particular Chapter 3\n",
    "\n",
    "## What text files are common on the internet?\n",
    "\n",
    "What do you expect to get back when you send a request\n",
    "1. HTML (`.html`)\n",
    "2. CSS (`.css`)\n",
    "3. Javascript (`.js`)\n",
    "\n",
    "### HTML\n",
    "\n",
    "HTML is a markup language used to format text.  \n",
    "\n",
    "The fundamental primitive is an element.  Elements can have different tags, such as:\n",
    "- `<p>` paragraph\n",
    "- `<h1>` heading\n",
    "- `<a>` link\n",
    "- `<img>` image\n",
    "- `<script>` Javascript\n",
    "\n",
    "These elements can be nested to create complex structure (particularly parent - child, or inheritance relationships).\n",
    "\n",
    "Take a look at `example.html` to see a full HTML document.  You can also use HTML within notebooks (like this one).\n",
    "\n",
    "HTML elements have optional attributes\n",
    "- property `<a property=\"value\">`\n",
    "- class `<a class=\"myClass\">`\n",
    "- id `<a id=\"myID>`\n",
    "\n",
    "Properties are usually stuff like color, and change how the HTML renders\n",
    "- classes & ID's are used to identify\n",
    "\n",
    "### CSS\n",
    "\n",
    "Used to style HTML\n",
    "- you don't need to know this for web scraping\n",
    "\n",
    "### Javascript\n",
    "\n",
    "Dynamic, weakly untyped language\n",
    "\n",
    "- executes in the browser\n",
    "- do fancy stuff like calling API's, dynamically rendering HTML, responding to user input\n",
    "\n",
    "While you don't need to know Javascript for web scraping, it is useful to look out for JSON strings\n",
    "- these can hold useful infomation\n",
    "- example - always check for a `<script type=\"application/ld+json\">`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML 101\n",
    "\n",
    "\n",
    "Tags can have **attributes** - for example the `<a>` usually has an attribute of `href` that holds the link:\n",
    "\n",
    "`<a href=\"https://adgefficiency.com/\">My personal blog</a>`\n",
    "\n",
    "This is rendered as:\n",
    "\n",
    "<a href=\"https://adgefficiency.com/\">My personal blog</a>\n",
    "\n",
    "A common attribute for HTML elements to have is a **class** - this is used to specify the styling of the object to a CSS class.\n",
    "\n",
    "## Parsing HTML\n",
    "\n",
    "We need some way to parse this HTML text - to do this we will use **Beautiful Soup**:\n",
    "\n",
    "We can use Beautiful Soup to parse the HTML for specific tags.  First we create an instance of the `BeautifulSoup` class, taking the HTML text we got using `requests`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **title** tag is a special tag required in all HTML documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Beautiful Soup to find all the `p` tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = soup.find_all('p')\n",
    "\n",
    "p[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or to find all the links (`a`) in a page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = soup.find_all('a')\n",
    "p[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developer tools\n",
    "\n",
    "One useful tool in web development are the **Developer Tools** included in modern browsers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/dev1.png)\n",
    "\n",
    "The **Inspect elements** tool allows us to find the HTML block for the biography table:\n",
    "\n",
    "![](assets/dev2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = soup.find('table', attrs={'class': 'infobox biography vcard'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tables in HTML\n",
    "\n",
    "`tr` = row\n",
    "\n",
    "`th` = header cell\n",
    "\n",
    "`td` = data cell\n",
    "\n",
    "Let's take a look at the third row (**Born**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = [r for r in table.find_all('tr')]\n",
    "row = rows[2]\n",
    "row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The header:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row.find('th')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row.find('td')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get the text from these HTML elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row.find('td').text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can store this data in a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "\n",
    "data[row.find('th').text] = row.find('td').text\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - clean the biography table\n",
    "\n",
    "Let's iterate over the rows in the biography table and store each row in a list of dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from answers import store_biography_table\n",
    "#store_biography_table(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding links\n",
    "\n",
    "Another common task when parsing HTML is to look for links - in HTML links have an `a` tag.  \n",
    "\n",
    "Let's find all the links in the **References** section - which is a `div` element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = soup.find('div', 'mw-references-wrap mw-references-columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = []\n",
    "for link in table.find_all('a'):\n",
    "    links.append(link)\n",
    "\n",
    "links = [link for link in table.find_all('a')]\n",
    "\n",
    "li = links[1]\n",
    "\n",
    "li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Create a list of the links from the External Links section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from answers import all_external_links\n",
    "\n",
    "_ = all_external_links('https://en.wikipedia.org/wiki/Yann_LeCun')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading images\n",
    "\n",
    "Now we are familiar with Beautiful Soup, we know we can find all the images in a page eaisly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('img')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download the first one - note that we use the `src` attribute, and have to append `'https:'` onto the url:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = soup.find_all('img')[0]\n",
    "url = 'https:' + img['src']\n",
    "url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use `requests` again to get the bytes for this image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.content[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's download this image into a file.  \n",
    "\n",
    "Note that we use Python's context management to automatically close the file, and the `iter_content` method to download the file in chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/le-cun.png', 'wb') as fi:\n",
    "    for chunk in res.iter_content(100000):\n",
    "        fi.write(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see the image (you may need to run this cell again):\n",
    "\n",
    "![](data/le-cun.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise (together) - scraping public-apis/public-apis\n",
    "\n",
    "Scrape https://github.com/public-apis/public-apis\n",
    "- I'd like to be able to filter API's based on their `AUTH`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise (individual) - downloading XKCD comics\n",
    "\n",
    "Now let's try another use of web scraping - downloading XKCD comics - this exercise is taken from the excellent [Automate the Boring Stuff with Python](https://automatetheboringstuff.com/).\n",
    "\n",
    "The basic workflow will be to:\n",
    "1. download a page (start with https://xkcd.com/)\n",
    "2. find the `img` tag\n",
    "3. download the image\n",
    "4. find the url of the previous comic & repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from answers import main\n",
    "#urls = main()\n",
    "#main??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from answers import xckd_simple\n",
    "#xckd_simple()\n",
    "#xckd_simple??"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
