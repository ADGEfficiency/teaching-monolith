{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequences\n",
    "\n",
    "A simple example of sequence prediction - $[0, 1, 2] \\rightarrow [3, 4, 5]$\n",
    "\n",
    "Applications / examples of sequences problems\n",
    "- language modelling\n",
    "- machine translation (text or audio)\n",
    "- generating image captions\n",
    "- question answering\n",
    "- parsing sentences into grammar trees\n",
    "- generating solutions to symbolic math problems\n",
    "- time series\n",
    "\n",
    "We can have different types of sequence problem structures\n",
    "\n",
    "![](assets/sequences.png)\n",
    "\n",
    "The many to many structure can also be thought of as an encoder-decoder structure:\n",
    "\n",
    "![](assets/quoc-le.png)\n",
    "\n",
    "## Problems with dense networks\n",
    "\n",
    "Stateless\n",
    "\n",
    "Unaware of temporal structure\n",
    "\n",
    "Fixed size inputs & outputs\n",
    "\n",
    "## Promise of recurrent neural networks\n",
    "\n",
    "Network able to learn a mapping from inputs over time\n",
    "- outputs become conditional the context of the sequence\n",
    "\n",
    "Learn the temporal dependence of data\n",
    "\n",
    "An RRN is Turing complete\n",
    "- they can simulate arbitrary programs\n",
    "\n",
    "## Being comfortable in three dimensions\n",
    "\n",
    "We model the temporal structure in data using a dimension in an array - by convention this is the second dimension.\n",
    "\n",
    "Our dimensions then are: \n",
    "- the batch dimension (number of samples)\n",
    "- timesteps\n",
    "- features\n",
    "\n",
    "## Practical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "batch_size = 1000\n",
    "timesteps = 32\n",
    "features = 16\n",
    "\n",
    "samples = tf.random.uniform((batch_size, timesteps, features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select all samples, first timestep, all features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples[:, 0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last sample, all timesteps, first feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples[-1, :, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ninth sample, sixth timestep, all features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples[8, 5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent neural networks\n",
    "\n",
    "A recurrent neural network passes it's outputs to itself at each timestep\n",
    "- the state is the prediction from the last timestep\n",
    "\n",
    "![](assets/unrolled.png)\n",
    "\n",
    "Let's model the forward pass of a recurrent neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  data\n",
    "samples = np.random.uniform(size=(4, 3, 2))\n",
    "\n",
    "#  architecture & weights\n",
    "nodes = 4\n",
    "previous_state_weights = np.random.uniform(size=(nodes, nodes))\n",
    "feature_weights = np.random.uniform(size=(samples.shape[2], nodes))\n",
    "state_weights = np.random.uniform(size=(nodes, 8))\n",
    "\n",
    "#  initial state\n",
    "state = np.zeros(nodes)\n",
    "\n",
    "#  update the hidden state\n",
    "#  use tanh to help deal with vanishing gradients\n",
    "#  tanh squeezes between -1 to 1\n",
    "state = np.tanh(\n",
    "    state.dot(previous_state_weights) + samples[0][0].dot(feature_weights)\n",
    ")\n",
    "output = state.dot(state_weights)\n",
    "\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = np.tanh(\n",
    "    state.dot(previous_state_weights) + samples[0][1].dot(feature_weights)\n",
    ")\n",
    "\n",
    "output = state.dot(state_weights)\n",
    "\n",
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backprop through time\n",
    "\n",
    "Backpropagating error requires error to flow backwards in time\n",
    "- error must flow back to the first time step to calculate gradients\n",
    "\n",
    "The loss function for a given layer depends not only on its infulence on layers below it - but also on the layer at the next time step\n",
    "\n",
    "Backproping through time means unrolling, which makes\n",
    "-  the memory footprint of recurrent neural network large\n",
    "- parallel training on multiple sequences inefficient on hardware that shares memory (i.e. GPU)\n",
    "\n",
    "Further reading - see *Truncated Backprop Through Time*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Character level language modeling\n",
    "\n",
    "Lets use a recurrent neural network to predict the next letter in the word *goodbye!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  we will model many to many \n",
    "#  feeding in the entire input sequence then reading the output sequence\n",
    "\n",
    "def encode(alphabet, samples, seq_len):\n",
    "    samples = np.array(samples)\n",
    "    encoding = np.zeros((samples.shape[0], seq_len, len(alphabet)))\n",
    "\n",
    "    for row, sample in enumerate(samples):\n",
    "        for se in range(seq_len):\n",
    "            try: \n",
    "                char = samples[row][se]\n",
    "                idx = alphabet.index(char)\n",
    "                encoding[row, se, idx] = 1\n",
    "            except IndexError:\n",
    "                import pdb; pdb.set_trace()\n",
    "            \n",
    "    assert (np.sum(encoding, axis=2) == 1).all()\n",
    "    return encoding\n",
    "\n",
    "\n",
    "def make_dataset(word, seq_len):\n",
    "    words = word * 50\n",
    "    \n",
    "    indicies = np.random.randint(seq_len, len(words) - seq_len*2, size=100)\n",
    "\n",
    "    f, t = zip(*[\n",
    "        [words[idx:idx+seq_len], words[idx+seq_len:idx+seq_len*2]] for idx in indicies\n",
    "    ])\n",
    "    \n",
    "    alphabet = list(set(word))\n",
    "\n",
    "    return encode(alphabet, f, seq_len), encode(alphabet, t, seq_len), alphabet\n",
    "\n",
    "f, t, alphabet = make_dataset('goodbye!', seq_len=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [keras.layers.SimpleRNN(8, return_sequences=True),\n",
    "     keras.layers.Dense(t.shape[2], activation='softmax')]\n",
    ")\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "h = model.fit(f, t, epochs=100, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(alphabet, encoded):\n",
    "    #  single sample only\n",
    "    return [alphabet[v] for v in encoded.flatten()]\n",
    "\n",
    "test = encode(alphabet, np.array(['goo']), seq_len=3)\n",
    "decode(alphabet, np.argmax(model.predict(test), axis=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = encode(alphabet, np.array(['bye']), seq_len=3)\n",
    "decode(alphabet, np.argmax(model.predict(test), axis=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = encode(alphabet, np.array(['!go']), seq_len=3)\n",
    "decode(alphabet, np.argmax(model.predict(test), axis=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functional Keras API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, t, alphabet = make_dataset('hello', seq_len=3)\n",
    "\n",
    "feat = tf.keras.Input(shape=(3, 4))\n",
    "rnn = keras.layers.SimpleRNN(8, return_sequences=True)(feat)\n",
    "classes = keras.layers.Dense(t.shape[2], activation='softmax')(rnn)\n",
    "\n",
    "model = tf.keras.Model(inputs=feat, outputs=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "h = model.fit(f, t, epochs=100, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(f[0].reshape(1, 3, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical\n",
    "\n",
    "Train a recurrent network to predict the next letter in a word.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
