{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's so important about backprop\n",
    "\n",
    "Backprop is a cross-disciplinary computational tool (it's been rediscovered at least dozens of times)\n",
    "\n",
    "**Speeds up** training of neural networks with gradient descent ~ 10 million times\n",
    "\n",
    "## What is backprop\n",
    "\n",
    "Mathematical technique for quickly calculating derivatives\n",
    "\n",
    "Application of *reverse-mode differentiation* (the application independent name for backprop) to neural networks\n",
    "\n",
    "## Gradients\n",
    "\n",
    "The gradient is\n",
    "- a direction\n",
    "- a rate of change\n",
    "\n",
    "Both of these are valuable\n",
    "- direction that reduces error\n",
    "- largest (steepest) rate of change to reduce error\n",
    "\n",
    "The change is always in the context of something else \n",
    "- we are interested in the how **error changes with respect to our parameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculus 101\n",
    "\n",
    "If I have a function\n",
    "\n",
    "$$ f(x) = x^2 $$\n",
    "\n",
    "The algorithm to find the derivative is known as the **Power Law**\n",
    "1. multiply by the power\n",
    "2. subtract the power by one\n",
    "\n",
    "$$f'(x) = 2x $$\n",
    "\n",
    "More examples\n",
    "\n",
    "$$ f(x) = 3x^4 + 2x^{2} -> f'(x) = 12x^3 + 4x $$\n",
    "\n",
    "If I take the derivative of a constant, it is always zero\n",
    "\n",
    "If I take the derivative of a term that doesn't depend on the thing I'm deriving with respect to, that also is zero\n",
    "\n",
    "## Notation\n",
    "\n",
    "Calculus has a number of competing notations\n",
    "\n",
    "$$ \\nabla_{x} f = f' = \\frac{df}{dx} $$\n",
    "\n",
    "## Minimizing a function\n",
    "\n",
    "We want to find the minimum of a simple function \n",
    "\n",
    "$$ f(x) = x^2 $$\n",
    "\n",
    "We can **sample data** from this function\n",
    "- it is clear where the minimum is (but lets pretend it isn't!)\n",
    "\n",
    "| x  | f(x) |\n",
    "|----|------|\n",
    "| -5 | 25   |\n",
    "| -2 | 4    |\n",
    "| 0  | 0    |\n",
    "| 2  | 4    |\n",
    "| 5  | 25   |\n",
    "\n",
    "Taking the derivative of this function\n",
    "\n",
    "$$ f'(x) = \\frac{df}{dx} = 2x $$\n",
    "\n",
    "| x  | f(x) | f'(x) |\n",
    "|----|------|-------|\n",
    "| -5 | 25   | -10   |\n",
    "| -2 | 4    | -4    |\n",
    "| 0  | 0    | 0     |\n",
    "| 2  | 4    | 4     |\n",
    "| 5  | 25   | 10    |\n",
    "\n",
    "This shows us the value of a gradient \n",
    "- it shows us the direction towards maximizing a function (we take the negative to minimize it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a function\n",
    "\n",
    "The example above shows how we can use gradients to find the minimum of a function\n",
    "\n",
    "In machine learning, the function we want to minimise is an **error or loss function**\n",
    "\n",
    "This error is the difference between two functions\n",
    "- a function that we parametrize $f(x; \\theta)$ with weights $\\theta$\n",
    "- a function that we want to learn $F(x)$\n",
    "\n",
    "We don't have access to $F(x)$ (if we did, we wouldn't need to learn it) - what we do have access to is the ability to sample:\n",
    "- $x$ - features (inputs)\n",
    "- $y$ - target / label (outputs)\n",
    "\n",
    "So we have three things:\n",
    "1. a function parametrized by weights $\\theta$\n",
    "2. samples of $x$ (features)\n",
    "3. samples $y$ (target)\n",
    "\n",
    "Returning to our simple example with our updated nomenclature\n",
    "\n",
    "A parameterized function with a single feature\n",
    "\n",
    "$$ f(x; \\theta) = x^2 \\cdot \\theta_{0} $$\n",
    "\n",
    "Samples\n",
    "\n",
    "| x  | y  |\n",
    "|----|----|\n",
    "| -2 | 12 |\n",
    "| 0  | 0  |\n",
    "| 1  | 3  |\n",
    "\n",
    "Mean square error\n",
    "\n",
    "$$E = \\frac{1}{2} (f(x; \\theta) - y)^2 = \\frac{1}{2} (x^2 \\cdot \\theta_{0} - y)^2 $$\n",
    "\n",
    "Derivative of the error\n",
    "\n",
    "$$E' = \\frac{dE}{d\\theta} = x^2 - y $$\n",
    "\n",
    "We can now perform an iterative process to update our parameter $\\theta$ - starting from an initial $\\theta_{0} = 0 $\n",
    "\n",
    "| x  | y  | E' |\n",
    "|----|----|----|\n",
    "| -2 | 12 | -8 |\n",
    "| 0  | 0  | 0  |\n",
    "| 1  | 3  | -2 |\n",
    "\n",
    "How do we update our parameter?\n",
    "- lets average over the three samples\n",
    "\n",
    "$$E' = -10 / 3$$\n",
    "\n",
    "As we are minimizing the error, we take the negative of the gradient and use it to update our parameter:\n",
    "\n",
    "$$\\theta_{1} = \\theta_{0} + E' = 0 + 3.3 = 3.3 $$\n",
    "\n",
    "Which is not so far from the true value of $3.0$\n",
    "\n",
    "## Practical\n",
    "\n",
    "Do this (on paper!) for $f(x) = 5 x^3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial derivatives\n",
    "\n",
    "In the example above we used our prior knowledge of the true function $F(x)$ to engineer a single feature $x^2$ \n",
    "- this is known as inductive bias - use it if you can (it's one reason why convolution works so well!)\n",
    "\n",
    "What happens if we don't encode this knowledge, and instead have multiple parameters\n",
    "\n",
    "$$ f(x;\\theta) = \\theta_{0} x^2 + \\theta_{1} x + \\theta_{2} $$\n",
    "\n",
    "We now need partial derivatives\n",
    "\n",
    "$$ \\frac{\\partial f}{\\partial \\theta_{0}} = x^2 $$\n",
    "\n",
    "$$ \\frac{\\partial f}{\\partial \\theta_{1}} = x $$\n",
    "\n",
    "$$ \\frac{\\partial f}{\\partial \\theta_{2}} = 1 $$\n",
    "\n",
    "Lets pick some parameters\n",
    "\n",
    "$$\\theta = [1,1,1]$$\n",
    "\n",
    "## Practical\n",
    "\n",
    "Calculate the partial derivatives and update the parameters using the following data\n",
    "\n",
    "| x  | y  |\n",
    "|----|----|\n",
    "| -2 | -5 |\n",
    "| 0  | 1  |\n",
    "| 1  | 4  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The linear perceptron\n",
    "\n",
    "If we have a feature of length 3 \n",
    "\n",
    "$$x = [x_{0}, x_{1}, x_{2}]$$\n",
    "\n",
    "Let give each feature its own parameter \n",
    "\n",
    "$$ \\theta = [\\theta_{0}, \\theta_{1}, \\theta_{2}] $$\n",
    "\n",
    "And combine them together using a linear combination\n",
    "\n",
    "$$ f(x; \\theta) = x_{0} \\cdot \\theta_{0} + x_{1} \\cdot \\theta_{1} + x_{2} \\cdot \\theta_{2} $$\n",
    "\n",
    "$$ f(x; \\theta) = \\sum x \\cdot \\theta $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The perceptron\n",
    "\n",
    "This linear combination won't be much use for learning a non-linear function.  Lets adjust notation in anticipation of complexity\n",
    "\n",
    "$$ z(x) = \\sum x \\theta $$\n",
    "\n",
    "Lets add an activation function after the linear combination - lets use a sigmoid (which is a special case of the logistic function)\n",
    "\n",
    "$$ a(z) = \\frac{1}{1 + e^{-z}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A single hidden layer neural network\n",
    "\n",
    "Three layers in total - input, hidden & output\n",
    "\n",
    "Parameters from input -> hidden layer \n",
    "\n",
    "$$w_{0}, b_{0}$$\n",
    "\n",
    "Linear combination of parameters\n",
    "\n",
    "$$z_{0} = \\sum X \\cdot w_{0} + b_{0}$$\n",
    "\n",
    "Hidden layer activation function (sigmoid)\n",
    "\n",
    "$$a_{0} = \\frac{1}{1 + \\exp^{-z}} $$\n",
    "\n",
    "Output layer linear combination\n",
    "\n",
    "$$z_{1} = \\sum a_{0} \\cdot w_{1} + b_{1}$$\n",
    "\n",
    "Error function\n",
    "\n",
    "$$E = \\frac{1}{2} (z_{1} - y)^2$$\n",
    "\n",
    "## Partial derivatives of these components\n",
    "\n",
    "Partial derivative of the linear combination of the input layer wrt a weight or bias in that layer\n",
    "\n",
    "$$ \\frac{\\partial z_{0}}{\\partial  w_{0}} = X $$\n",
    "\n",
    "$$ \\frac{\\partial z_{0}}{\\partial  b_{0}} = 1 $$\n",
    "\n",
    "Partial derivative of the sigmoid activation on the hidden layer wrt the linear combination\n",
    "\n",
    "$$ \\frac{\\partial a_{0}}{\\partial  z_{0}} = a_{0}(z_{0}) * (1-a_{0}(z_{0})) $$\n",
    "\n",
    "Partial derivative of the output layer linear combination wrt the activation\n",
    "\n",
    "$$ \\frac{\\partial z_{1}}{\\partial a_{0}} = w_{1} $$\n",
    "\n",
    "Partial derivative of the error wrt the output layer\n",
    "\n",
    "$$ \\frac{\\partial E}{\\partial z_{1}} = z_{1} - y $$\n",
    "\n",
    "Our full model can now be written as a composition of functions\n",
    "\n",
    "$$ f(x; \\theta) = z_{1}(a_{0}(z_{0}(x))) $$\n",
    "\n",
    "We want change each of our parameters to minimize the error\n",
    "\n",
    "$$ \\frac{\\partial E}{\\partial w_{0}}, \\frac{\\partial E}{\\partial b_{0}}, \\frac{\\partial E}{\\partial w_{1}}, \\frac{\\partial E}{\\partial b_{1}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The chain rule\n",
    "\n",
    "When we have compositions of functions\n",
    "\n",
    "$$ f(x) = a(z(x)) $$\n",
    "\n",
    "The **chain rule** shows us how gradients flow through these compositions of functions\n",
    "\n",
    "$$ \\frac{df}{dz} = \\frac{df}{da} \\cdot \\frac{da}{dz} $$\n",
    "\n",
    "## Practical\n",
    "\n",
    "You now have all the tools to derive update equations for all our weights and biases - do so on paper\n",
    "\n",
    "Afterwards I will write the solution on the whiteboard :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
