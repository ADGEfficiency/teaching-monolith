{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.stats import boxcox, zscore\n",
    "\n",
    "from create_dataset import ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning\n",
    "\n",
    "## Resources\n",
    "\n",
    "[Becoming One With the Data](https://blog.floydhub.com/becoming-one-with-the-data/)\n",
    "\n",
    "[sklearn preprocessing documentation](https://scikit-learn.org/stable/modules/preprocessing.html)\n",
    "\n",
    "[Compare the effect of different scalers on data with outliers - sklearn](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py)\n",
    "\n",
    "[When and Why Should You Normalize / Standardize / Rescale Your Data?](https://medium.com/@swethalakshmanan14/how-when-and-why-should-you-normalize-standardize-rescale-your-data-3f083def38ff)\n",
    "\n",
    "[Scaling vs Normalization](https://kharshit.github.io/blog/2018/03/23/scaling-vs-normalization)\n",
    "\n",
    "## What is data\n",
    "\n",
    "Tables\n",
    "\n",
    "Images\n",
    "\n",
    "Text\n",
    "\n",
    "Temporal\n",
    "\n",
    "Geospatial\n",
    "\n",
    "Always remember\n",
    "- it has to become a number eventually\n",
    "- machines think in 0 or 1\n",
    "\n",
    "## What is tabular data\n",
    "\n",
    "Continuous\n",
    "\n",
    "Categorical\n",
    "\n",
    "Ordinal\n",
    "\n",
    "Binary\n",
    "\n",
    "Time\n",
    "\n",
    "## Iterate on data\n",
    "\n",
    "- not only on models!\n",
    "- data gathering, labelling, preparation are all iterative\n",
    "- datasets are often fixed in academia, iterative in industry\n",
    "\n",
    "![](assets/karpathy.png)\n",
    "\n",
    "From [Building the Software 2.0 Stack - Andrej Karpathy, Director of AI, Tesla](https://www.youtube.com/watch?v=zywIvINSlaI)\n",
    "\n",
    "## Characterizing data\n",
    "\n",
    "Quality = outliers / missing values\n",
    "\n",
    "Quantity = rows & columns\n",
    "\n",
    "Diversity = does the distribution match the test set\n",
    "\n",
    "Cardinality - number of unique values\n",
    "\n",
    "Dimensionality\n",
    "\n",
    "Sparsity\n",
    "\n",
    "Stationarity\n",
    "- iterating on data\n",
    "- new / different / more customers\n",
    "- environment (interest rates changing)\n",
    "- model predictions infulencing the data (reccomendation, fraud)\n",
    "\n",
    "Duplicates\n",
    "\n",
    "Class imbalance\n",
    "\n",
    "Label noise\n",
    "\n",
    "Biased sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample dataset\n",
    "\n",
    "A simple dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicates\n",
    "\n",
    "Important to not have duplicates in both your test & train data\n",
    "- but you should leave duplicate samples if they are only in the train data\n",
    "- the repetitions provide weight of evidence (especially important in Bayesian methods)\n",
    "- the frequency infomation is important\n",
    "\n",
    "Calling `.duplicated()` returns a boolean mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.duplicated()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `.duplicated()` to filter out the duplicate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[~ds.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values\n",
    "\n",
    "Not always a `np.Nan` - sometimes this can be encoded as a `0` or a `null` string\n",
    "- know how missing is represented in your dataset\n",
    "\n",
    "Always ask - **why is this data missing?**\n",
    "- is there bias (a pattern) in the missing values?\n",
    "\n",
    "Dealing with missing values = **detection + replacement**\n",
    "\n",
    "Detection depends on how the missing value is encoded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ds.columns:\n",
    "    print(col, sum(ds.loc[:, col].isnull()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping rows or columns\n",
    "\n",
    "One solution to missing data is to drop the row (sample) or the column (feature)\n",
    "\n",
    "- quick & dirty way to deal with missing values\n",
    "- lose data\n",
    "- should be done on the first iteration (if working in an agile way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation for continuous variables\n",
    "\n",
    "Using a statistic like mean or median:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ds.loc[:, 'contract-length']\n",
    "filled = data.fillna(data.median())\n",
    "\n",
    "pd.concat([data, filled], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An improvement may be to fill with a statistic that is conditional on another feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ds.loc[:, ['contract-length', 'customers-category']]\n",
    "\n",
    "for customer in set(data.loc[:, 'customers-category']):\n",
    "    #  get rows with this customer type\n",
    "    mask = data.loc[:, 'customers-category'] == customer\n",
    "    \n",
    "    #  fill the na's using the mean of this customer type\n",
    "    filled = data.copy()\n",
    "    filled.loc[:, 'contract-length'].fillna(np.mean(data.loc[mask, 'contract-length']), inplace=True)\n",
    "    \n",
    "pd.concat([data, filled], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another solution is to **train a model to predict the missing value**\n",
    "- perhaps a linear model using other columns as features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation for categorical variables\n",
    "\n",
    "Most common class (similar to mean / median)\n",
    "\n",
    "Missing value token\n",
    "- tell the model it was missing\n",
    "\n",
    "Fill with the most common label conditional on another feature (as with continuous)\n",
    "\n",
    "Train a model to predict it (as with continuous)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outliers\n",
    "\n",
    "Within distribution outliers, versus outliers that are due to mistakes / measurement errors\n",
    "\n",
    "Require both detection & replacement\n",
    "- detection is harder than missing values\n",
    "\n",
    "### Detecting outliers\n",
    "\n",
    "Using standard deviation\n",
    "- multiple on the z-score (2 - 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zsc = zscore(ds.loc[:, 'contract-length'].fillna(np.mean(ds.loc[:, 'contract-length'])))\n",
    "zsc_mask = zsc > 2.0\n",
    "\n",
    "pd.concat([\n",
    "    ds, \n",
    "    pd.DataFrame({'zsc_mask': zsc_mask, 'zscore': zsc})\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detection using percentiles\n",
    "- always lose data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  calculate the 95th percentile\n",
    "pct = np.percentile(ds.loc[:, 'contract-length'].fillna(np.mean(ds.loc[:, 'contract-length'])), 95)\n",
    "pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  drop all rows less than this number\n",
    "ds.loc[ds.loc[:, 'contract-length'] < pct]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clipping / capping\n",
    "- this will change the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_val = 50\n",
    "np.clip(ds.loc[:, 'contract-length'], None, max_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String cleaning\n",
    "\n",
    "Consistent naming\n",
    "- U.K., UK, United Kingdom, Britian, Great Britian etc\n",
    "\n",
    "Tokenization\n",
    "- breaking text into smaller chunks (often splitting on spaces)\n",
    "- word or sentence level\n",
    "\n",
    "Normalization\n",
    "- removing stop words\n",
    "- lower case\n",
    "- stemming / lemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "\n",
    "A topic where many people use the various names for transformations in different ways\n",
    "\n",
    "Scale the validation using the statistics from the training set only\n",
    "- this is what you have to do in production on single samples!\n",
    "\n",
    "### Min-max scaling (normalization)\n",
    "\n",
    "`from sklearn.preprocessing import MinMaxScaler`\n",
    "\n",
    "Scales to `[0, 1]`\n",
    "- doesn't shift/center the data\n",
    "- retains sparsity\n",
    "- retains zero values\n",
    "\n",
    "$$ y = \\frac{x - x_{min}}{x_{max} - x_{min}} $$\n",
    "\n",
    "- use when you do not know the distribution of your data\n",
    "- when you know the distribution is not Gaussian\n",
    "- algorithm you are using does not make assumptions about the distribution of your data, such as k-nearest neighbors and artificial neural networks\n",
    "- sensitive to outliers\n",
    "\n",
    "Use normalization for images\n",
    "\n",
    "- divide by 255\n",
    "- pixel values are stored as an 8-bit integer giving a range of possible values from 0 to 255\n",
    "\n",
    "### Max-abs scaling\n",
    "\n",
    "`from sklearn.preprocessing import MaxAbsScaler`\n",
    "\n",
    "Scales to `[-1, 1]`\n",
    "- divide by largest maximum value\n",
    "\n",
    "### Standardization\n",
    "\n",
    "`from sklearn.preprocessing import StandardScaler`\n",
    "\n",
    "Removing the mean and scaling by the standard deviation\n",
    "\n",
    "$$ y = \\frac{x-\\mu}{\\sigma} $$\n",
    "\n",
    "- assumes that your data has a Gaussian distribution\n",
    "- the algorithm you are using does make assumptions about your data having a Gaussian distribution, such as linear regression, logistic regression, and linear discriminant analysis\n",
    "\n",
    "### Robust Scalar\n",
    "\n",
    "`from sklearn.preprocessing import RobustScaler`\n",
    "\n",
    "$$ y = x - \\frac{x_{median}}{x_{IQR}} $$\n",
    "\n",
    "- subtracting the median to all the observations and then dividing by the interquartile difference. Scales features using statistics that are robust to outliers\n",
    "- IQR = difference between 75th and 25th percentiles\n",
    "\n",
    "### Boxcox\n",
    "\n",
    "Make normally distributed\n",
    "\n",
    "$$ y = \\frac{x - \\mu}{x_{max} - x_{min}} $$\n",
    "\n",
    "- you can do predictions on the transformed data (if your algorithm likes normal looking data), and then inverse-transform back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = np.concatenate([np.random.exponential(size=1000), np.array([10]*20)])\n",
    "\n",
    "minmax = (raw - np.min(raw)) / (np.max(raw) - np.min(raw))\n",
    "standard = (raw - np.mean(raw)) / np.var(raw)\n",
    "\n",
    "f, a = plt.subplots(nrows=4, figsize=(5, 10), sharex=True)\n",
    "\n",
    "plots = {\n",
    "    'raw': raw, 'minmax': minmax, 'standardized': standard, 'boxcox': boxcox(raw)\n",
    "}\n",
    "\n",
    "for ax, (name, data) in zip(a, plots.items()):\n",
    "    ax.hist(data)\n",
    "    ax.set_title(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing target labeling\n",
    "\n",
    "Most targets have errors/noise.\n",
    "\n",
    "Sometimes these can be fixed - it depends on the business problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
