{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Hypothesis Testing\n",
    "\n",
    "References:\n",
    "- Chapter 9 of [Think Stats 2nd Edition](https://greenteapress.com/wp/think-stats-2e/)\n",
    "- [Statistics Done Wrong - Alex Reinhart](https://www.statisticsdonewrong.com/)\n",
    "- [Statistics for Hackers - PyCon 2016 - Jake Vanderplas](https://www.youtube.com/watch?v=Iq9DzN6mvYA)\n",
    "\n",
    "## Did I get lucky?\n",
    "\n",
    "You will always observe effects in data due to chance.  \n",
    "\n",
    "What is the probability of seeing this effect by chance?\n",
    "\n",
    "Is this effect **likely to appear in the larger population**?\n",
    "\n",
    "No tool can tell you if your hypothesis is really true - only if is is **consistent with the data**.\n",
    "\n",
    "## Analytic computation versus simulating\n",
    "\n",
    "Computing the sampling distribution is hard - **simulating** the sampling distribution is easy.\n",
    "\n",
    "With analytical statistical testing (i.e. Welch's t-testing) we lose track of what question we are asking.\n",
    "\n",
    "Sampling/simulation methods\n",
    "- allow intuition in the place of statistical rules\n",
    "- can use for loops to do statistical analysis (often averaging over the runs)\n",
    "\n",
    "This notebook takes the simulation approach\n",
    "\n",
    "Four recipes\n",
    "1. direct simulation\n",
    "2. shuffling\n",
    "3. bootstrapping\n",
    "4. cross-validation\n",
    "\n",
    "## Classical hypothesis testing\n",
    "\n",
    "Proof by *reductio ad absurdum* - assume that A is false, find a contradiction -> A is true\n",
    "\n",
    "Procedure\n",
    "1. choose a test statistic\n",
    "2. define a null hypothesis\n",
    "3. compute a p-value\n",
    "4. interpret the result\n",
    "\n",
    "The assumption we make is the **null hypothesis** - that the effect is not real\n",
    "- we compute a probability (**p value**) based on that assumption\n",
    "- low probability -> null nypothesis false -> effect is real\n",
    "\n",
    "Limitations\n",
    "- only tells you if the effect is larger than what could be produced by luck\n",
    "- not that the effect is actually significant\n",
    "\n",
    "Possible to measure a small effect with great certanitiy\n",
    "- we can use hypothesis testing to confirm that a 0.001 difference in customers is statistically significant with a p-value of 96%\n",
    "\n",
    "## The p-value\n",
    "\n",
    "The probability of collecting of collecting data that shows an effect equal/greater than observed.  \n",
    "\n",
    "How likely is the dataset, if we assume the true effect is zero.  It is a **measurement of suprise**.\n",
    "\n",
    "## Significance\n",
    "\n",
    "Probability threshold for significance - 5% by convention.  P-values less than the threshold are very unlikely & hence significant.\n",
    "\n",
    "$$ p < 0.05 $$\n",
    "\n",
    "No comment on the size of the effect - possible to measure a tiny effect with great certantity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jacob Bernoulli \n",
    "\n",
    "1655 - 1705.  Swiss mathematician.  Derived the first version of the Law of Large Numbers.  [Wikipedia](https://en.wikipedia.org/wiki/Jacob_Bernoulli).\n",
    "\n",
    "![](assets/bernoulli.jpg)\n",
    "\n",
    "The Bernoulli distribution is binary - the outcome is a single **bit of infomation** (0 or 1 - more on this in [entropy.ipynb]()):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from common import load_iris, make_cdf\n",
    "\n",
    "features, target = load_iris()\n",
    "\n",
    "def bernoulli(prob=0.5):\n",
    "    return np.random.choice([0, 1], p=[1-prob, prob])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value for this binary variable is a simple ratio (an example of frequentist probability):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_value(samples):\n",
    "    return sum(samples) / len(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 200 \n",
    "prob = 0.4\n",
    "\n",
    "samples = [bernoulli(prob) for _ in range(num)]\n",
    "samples[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_val = p_value(samples)\n",
    "p_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets calculate the p-value for a biased coin, using a null hypothesis of a fair coin: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_deviation(observed, expected):\n",
    "    #  absolute error\n",
    "    return abs(observed - expected)\n",
    "\n",
    "def fair_coin(n):\n",
    "    #  the simulation of the null hypothesis\n",
    "    sample = [bernoulli(0.5) for _ in range(n)]\n",
    "    return (n - sum(sample), sum(sample))\n",
    "\n",
    "def calc_p_val(observed, n_trials, test_stat, null_hypothesis, iters=100):\n",
    "    \n",
    "    observed = total_deviation(*observed)\n",
    "\n",
    "    test_stats = np.array([\n",
    "        total_deviation(*null_hypothesis(n_trials)) for _ in range(iters)\n",
    "    ])\n",
    "\n",
    "    p_val = sum(test_stats > observed) / test_stats.shape[0]\n",
    "    print('If the null hypothesis is true, we expect to see the effect of {} {} % of the time'.format(\n",
    "        observed, p_val*100))\n",
    "    \n",
    "    return p_val\n",
    "  \n",
    "biased_coin = (140, 130)\n",
    "\n",
    "p_val = calc_p_val(biased_coin, sum(biased_coin), total_deviation, fair_coin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the difference in means\n",
    "\n",
    "Let's do some hypothesis testing on some of the effects we can see in the Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, target = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  feature statistics per class\n",
    "flowers = target.columns\n",
    "\n",
    "for flower in flowers:\n",
    "    print(flower)\n",
    "    print(features[target[flower] == 1].describe().loc['mean', :])\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets investigate the difference in **sepal length** for **setosa versus virginica**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for flower in flowers:\n",
    "    print(flower, features[target[flower] == 1].describe().loc['mean', 'sepal length (cm)'])\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our null hypothesis is that there is no difference\n",
    "- we can model the null hypothesis by **shuffling**\n",
    "- this is also known as permutation\n",
    "\n",
    "Shuffling / permutation\n",
    "- if the labels dont matter, then switching them shouldnt change the result\n",
    "- only works with representative samples (non biased sampling)\n",
    "\n",
    "Let's run a hypothesis test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_means(observed, expected):\n",
    "    return abs(np.mean(observed) - np.mean(expected))\n",
    "\n",
    "def run_shuffle(pool):\n",
    "    mask = np.random.randint(0, 2, size=pool.shape[0]).astype(bool)\n",
    "    return pool[mask], pool[~mask]\n",
    "\n",
    "def calc_p_val_mean_diff(first, second, iters=100, test_stat=test_means):\n",
    "    \n",
    "    observed = test_stat(first, second)\n",
    "    pool = np.concatenate([first, second])\n",
    "    \n",
    "    test_diff = np.array([\n",
    "        test_stat(*run_shuffle(pool)) for _ in range(iters)\n",
    "    ])\n",
    "\n",
    "    p_val = sum(test_diff > observed) / test_diff.shape[0]\n",
    "    return p_val, test_diff\n",
    "    \n",
    "first = features[target['setosa'] == 1].loc[:, 'sepal length (cm)'].values\n",
    "second = features[target['virginica'] == 1].loc[:, 'sepal length (cm)'].values\n",
    "\n",
    "p_val, test_diff = calc_p_val_mean_diff(first, second, iters=100)\n",
    "p_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical\n",
    "\n",
    "Do the same for sub samples from **setosa** only (what kind of p-value are you expecting?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "mask = np.random.randint(0, 2, size=50).astype(bool)\n",
    "setosa = features[target['setosa'] == 1]\n",
    "first = setosa.loc[mask, 'sepal length (cm)']\n",
    "second = setosa.loc[~mask, 'sepal length (cm)']\n",
    "\"\"\"\n",
    "\n",
    "p_val, test_same = calc_p_val_mean_diff(first, second, iters=100)\n",
    "p_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The confidence interval approach\n",
    "\n",
    "In [Statistics Done Wrong](https://www.statisticsdonewrong.com/), Alex Reinhart advocates for using **confidence intervals** to test for significance.\n",
    "\n",
    "Confidence interval = level of confidence a statistic lies in an interval\n",
    "- range of potential values of the population statistic\n",
    "- check significance by checking that the interval doesn't include zero\n",
    "\n",
    "Lets look at the CDF of the difference in average petal length (under the null hypothesis) for our different & same classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, x = zip(*make_cdf(test_diff))\n",
    "\n",
    "y = np.array(y)\n",
    "x = np.array(x)\n",
    "\n",
    "start = x[y == 0.05]\n",
    "end = x[y == 0.95]\n",
    "print('different class 95% CI - {} {}'.format(start, end))\n",
    "\n",
    "f, ax = plt.subplots(nrows=2, sharex=True, figsize=(10, 4))\n",
    "ax = ax.reshape(-1)\n",
    "\n",
    "ax[0].plot(x, y)\n",
    "ax[0].axvline(start, color='red')\n",
    "ax[0].axvline(end, color='red')\n",
    "ax[0].set_title('different class')\n",
    "\n",
    "y, x = zip(*make_cdf(test_same))\n",
    "\n",
    "y = np.array(y)\n",
    "x = np.array(x)\n",
    "\n",
    "start = x[y == 0.05]\n",
    "end = x[y == 0.95]\n",
    "print('same class 95% CI - {} {}'.format(start, end))\n",
    "\n",
    "ax[1].plot(x, y)\n",
    "ax[1].axvline(list(set(start))[0], color='red')\n",
    "ax[1].axvline(list(set(end))[0], color='red')\n",
    "_ = ax[1].set_title('same class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical - testing a correlation\n",
    "\n",
    "Use the same structure above to get a p-value for the Pearson correlation\n",
    "- the correlation between setosa and versicolor on mean sepal width (cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chi-Squared tests\n",
    "\n",
    "A simple variation on how we calculate the test statistic.  **We square the deviations** rather than take the absolute (this gives more weight to outliers):\n",
    "\n",
    "$$ \\chi^{2} = \\sum_{i} \\frac{(observed-expected)^2}{expected} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_sq(observed, expected):\n",
    "    observed, expected = np.mean(observed), np.mean(expected)\n",
    "    return (observed - expected)**2 / expected\n",
    "\n",
    "p_val, test_chi = calc_p_val_mean_diff(first, second, iters=100, test_stat=chi_sq)\n",
    "p_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical power\n",
    "\n",
    "Power of a test to detect an effect.  Depends on\n",
    "- size of the effect you are looking for\n",
    "- number of samples\n",
    "- measurement error\n",
    "\n",
    "**How much data do I need to measure an effect**?\n",
    "- large effect with small data = underpowered\n",
    "\n",
    "If we think about false positives & negatives in the context of p-value hypothesis testing\n",
    "- false positive = concluding that no effect is actually an effect\n",
    "- false negative = concluding that a real effect is no effect\n",
    "\n",
    "The false positive rate (how often we see chance as reality) is easy to compute - it is the threshold for significance (say 5%).\n",
    "\n",
    "What about the false negative rate - how often will we miss seeing a real effect?\n",
    "- this is harder to calculate because we need to know the true effect size\n",
    "- what we can do is calculate a false negative rate conditioned on an assumed effect size\n",
    "\n",
    "Lets assume our observed effect size.  How often do we miss this size effect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_statistical_power(first, second, trials = 1000):\n",
    "    count = 0\n",
    "    for _ in range(trials):\n",
    "        first_sample = np.random.choice(first, first.shape[0], replace=True)\n",
    "        second_sample = np.random.choice(second, second.shape[0], replace=True)\n",
    "\n",
    "        p_val, test_diff = calc_p_val_mean_diff(first_sample, second_sample, iters=100)\n",
    "\n",
    "        if p_val > 0.05:\n",
    "            count += 1\n",
    "\n",
    "    print('If the actual effect size is {:.2f}, we expect to miss it {:.2f} % of the time'.format(\n",
    "        np.mean(first) - np.mean(second), 100 * count / trials\n",
    "    ))\n",
    "    \n",
    "first = features[target['setosa'] == 1].loc[:, 'sepal length (cm)'].values\n",
    "second = features[target['virginica'] == 1].loc[:, 'sepal length (cm)'].values\n",
    "\n",
    "calc_statistical_power(first, second, trials = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.random.randint(0, 2, size=50).astype(bool)\n",
    "setosa = features[target['setosa'] == 1]\n",
    "first = setosa.loc[mask, 'sepal length (cm)']\n",
    "second = setosa.loc[~mask, 'sepal length (cm)']\n",
    "\n",
    "calc_statistical_power(first, second, trials = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base rate fallacy\n",
    "\n",
    "[Chapter 4 of Statistics Done Wrong](https://www.statisticsdonewrong.com/).\n",
    "\n",
    "Low base rate -> many chances for false p\n",
    "\n",
    "P-value tells you how probable your data is\n",
    "- it doesn't tell you the probability of the effect\n",
    "\n",
    "The probability of a false positive is almost always higher than the p-value\n",
    "- in areas with low base rates (i.e. early stage drug trials) it is likely that many p < 0.05 results are false positives\n",
    "\n",
    "## Examples\n",
    "\n",
    "Testing drugs\n",
    "- samples = 100\n",
    "- thohold p = 0.05\n",
    "- base rate = 1% \n",
    "- statistical power = 80%\n",
    "- we find 13 positives\n",
    "\n",
    "How many of drugs work?\n",
    "\n",
    "How many will we detect? (8)\n",
    "\n",
    "How many false positives? (5)\n",
    "\n",
    "What is the false positive rate?\n",
    "\n",
    "\n",
    "Breast cancer\n",
    "- 1000 samples\n",
    "- p = 0.07\n",
    "- 0.8% base rate\n",
    "- 0.9 statistical power\n",
    "\n",
    "How many true positives are there? 8\n",
    "\n",
    "How many will be discovered? 7\n",
    "\n",
    "How many false positives? 70 = 992 * 7\n",
    "\n",
    "How many with positive test results actually have cancer? 7 / 77+8\n",
    "\n",
    "## Multiple comparisons\n",
    "\n",
    "More tests = greater chance of false positives\n",
    "\n",
    "Functional MRI scans\n",
    "- divide the brain scans into voxels\n",
    "- compare blood flow in sequences of images\n",
    "- many voxels -> many chances for false positives\n",
    "- 'dead salmon' experiment - p=0.001 on an 81 mm3 area of the brain of a dead fish\n",
    "\n",
    "Look-elsewhere effect an apparently statistically significant observation arises by chance because of the sheer size of the search\n",
    "\n",
    "More tests = more chances for false positives\n",
    "- tracking paitent symptoms over 12 weeks = 12 chances for false positives\n",
    "- survey with 20 questions -> one false positive (at p=0.05)\n",
    "\n",
    "Bonferroni correction rate\n",
    "- threshold becomes $p/n$\n",
    "- lowers false positive chance (but alse reduces power)\n",
    "\n",
    "Benjamini-Hochberg procedure\n",
    "1. get p-value for $m$ comparisons\n",
    "2. choose a false discovery rate $q$\n",
    "3. find the largest $p$ such that $p \\leq iq/m$ ($i$ = rank in list)\n",
    "4. that value (and all smaller) are significant\n",
    "\n",
    "Gives an upper bound on fale posities of $q$\n",
    "\n",
    "Cut off becomes more conservative if you \n",
    "- have a smaller false positive threshold ($q$)\n",
    "- making more comparisons ($m$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values = np.linspace(0, 0.3, 10)\n",
    "q = 0.3\n",
    "m = len(p_values)\n",
    "\n",
    "ranks = np.array([q * r / m for r in np.arange(m)])\n",
    "mask = ranks <= p_values\n",
    "np.max(p_values[mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(p_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
