{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from answers import expectation, ucb, run_ucb_expt\n",
    "from common import generate_bandit_dataset\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Armed Bandits\n",
    "\n",
    "## Resources\n",
    "\n",
    "- Chapter 3 of [Practical Statistics for Data Scientists](https://www.oreilly.com/library/view/practical-statistics-for/9781491952955/)\n",
    "- Chapter 2 of [Sutton & Barto - Reinforcement Learning: An Introduction](http://incompleteideas.net/book/RLbook2018.pdf).\n",
    "\n",
    "## When is classical hypothesis testing not enough?\n",
    "\n",
    "In classical hypothesis testing we\n",
    "- run an experiment, collecting a dataset of A & B\n",
    "- perform a hypothesis test on observed effects (differences between A and B)\n",
    "\n",
    "There are a few problems with this\n",
    "- we only compare two options (one of which is usually a default)\n",
    "- we might lose money by showing customers a suboptimal option \n",
    "- the hypothesis test only shows us if an observed effect is unlikely, not if the effect is large\n",
    "- in a real business, there is no 'experiment over' date\n",
    "- the real world is non-stationary - the results we collect might be from a distribution that is changing\n",
    "\n",
    "We want an experiment where we can **take advantage of the results as we learn** - not have to wait until the experiment results can be used\n",
    "\n",
    "In a business context we are not concerned with statistical significance - we are (often) concerned with optimizing money as quickly as possible\n",
    "\n",
    "## The mulit-armed bandit\n",
    "\n",
    "Bandits allow\n",
    "- testing mulitple options at once\n",
    "- reach conclusions faster\n",
    "\n",
    "The term bandit comes from slot machines \n",
    "- one armed bandits for their ability to extract money from gamblers\n",
    "\n",
    "The goal of a multi armed bandit problem is to vin as much money as possible\n",
    "- this is the same as figuring out which arm is best as quick as possible\n",
    "\n",
    "Bandits are a simplification of the full reinforcement learing problem\n",
    "\n",
    "## Exploration versus exploitation\n",
    "\n",
    "Favourice resturant or somewhere new?\n",
    "- if you know all the resturants well, trust your judgement\n",
    "- if you don't, just randomly pick\n",
    "\n",
    "## Reinforcement learning context\n",
    "\n",
    "Bandits share in common with reinforcement learning\n",
    "- exploration verses exploitation problem (which arm to pull)\n",
    "- potentially non-stationary\n",
    "\n",
    "Is supervised learning we use data learn a function to use for prediction on unseen data\n",
    "- in a bandit problem we could predict the expected value of each arm\n",
    "- this is **prediction** / evaluation\n",
    "\n",
    "Use data (the results we get from pulling an arm) to select an action\n",
    "- this is **control**\n",
    "\n",
    "We can define the value of an action (pulling a specific arm) as an expectation\n",
    "- the expected reward of an action\n",
    "\n",
    "$q_{*}(a) = \\mathbb{E}[r(a)]$\n",
    "\n",
    "Reinforcement learning has a **convienent goal** - maximizing expected reward\n",
    "- if we did know the true expectation of each action, maximization is an argmax\n",
    "\n",
    "The bandit problem is one step short of the full reinforcement learning problem\n",
    "- the bandit is a single state, no transitions of state happen\n",
    "\n",
    "## Example\n",
    "\n",
    "You have the following results from comparing different landing pages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params, results = generate_bandit_dataset(arms=20, samples=3)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical \n",
    "\n",
    "Write a function to take an expectation over the results\n",
    "- one number for each arm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer\n",
    "# expectation(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One approach would be to conclude that one of the arms is optimal and send all our users there\n",
    "- this is a **greedy** solution to the exploration & exploitation dilemma\n",
    "\n",
    "## Practical\n",
    "\n",
    "Take a greedy action based on the results\n",
    "- take the argmax across expected reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with a greedy stragety is that we might have noise in our samples that\n",
    "- our expectation has variance\n",
    "\n",
    "## Question to class\n",
    "\n",
    "Is the expectation above biased?\n",
    "\n",
    "## epsilon-greedy\n",
    "\n",
    "Another solution would be to favour the option that appears optimal, while still sampling from the options that appear sub-optimal.\n",
    "\n",
    "A simple algorithm to tackle the exploration-exploitation dilemma is known as **epsilon-greedy** - it is the method used for exploration in DeepMind's 2013 DQN.\n",
    "\n",
    "The algorithm has a single parameter $\\epsilon$, which controls how greedy we are.  \n",
    "\n",
    "- $\\epsilon$ = 1 -> standard A/B test\n",
    "- $\\epsilon$ = 0 -> greedy\n",
    "\n",
    "In reinforcement learning $\\epsilon$ is often decayed from 1 to a 0.05 over an agents lifetime.  Proper selection of $\\epsilon$ depends on\n",
    "- how accurate your greedy estimate is\n",
    "- how non-stationary the process is\n",
    "\n",
    "The basic algorithm is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance(results):\n",
    "    d = []\n",
    "    for arm, data in results.items():\n",
    "        d.extend(data)\n",
    "    return np.mean(d)\n",
    "\n",
    "results = {\n",
    "    arm: list(np.random.normal(*stats))\n",
    "    for arm, stats in params.items()\n",
    "}\n",
    "\n",
    "eps = 0.3\n",
    "choices = list(params.keys())\n",
    "\n",
    "steps = 1000\n",
    "values = np.zeros((steps, len(choices)))\n",
    "actions = np.empty((steps)).astype(str)\n",
    "eps_performance = np.zeros(steps)\n",
    "\n",
    "for step in range(steps):\n",
    "    prob = np.random.rand()\n",
    "    if prob < eps:\n",
    "        strat = 'random'\n",
    "        action = np.random.choice(choices)\n",
    "\n",
    "    else:\n",
    "        strat = 'greedy'\n",
    "        expectations = expectation(results)\n",
    "        values[step, :] = list(expectations.values())\n",
    "        action = max(expectations, key=expectations.get)\n",
    "        \n",
    "    actions[step] = action\n",
    "    \n",
    "    p = params[action]\n",
    "    results[action].append(float(np.random.normal(p.loc, p.scale, 1)))\n",
    "    eps_performance[step] = get_performance(results)\n",
    "    \n",
    "plt.plot(eps_performance, label='eps {}'.format(eps))\n",
    "_ = plt.legend()\n",
    "\n",
    "#TODO put min & max lines of arms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Upper Confidence Bound (UCB)\n",
    "\n",
    "2.7 in [Sutton & Barto - Reinforcement Learning: An Introduction](http://incompleteideas.net/book/RLbook2018.pdf)\n",
    "\n",
    "Select an action based on it's historical mean + an exploration bonus\n",
    "\n",
    "$a = \\underset{x}{\\text{argmax}} \\left[ q(a) + c \\cdot \\sqrt{\\frac{\\ln t}{N(a)}} \\right] $\n",
    "\n",
    "$t$ = timestep\n",
    "\n",
    "$N(a)$ = number of times action $a$ taken\n",
    "\n",
    "## Practical\n",
    "\n",
    "Implement a function that performs a UCB update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer\n",
    "# ucb(results, step, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical\n",
    "\n",
    "Implement a UCB experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer\n",
    "# ucb_performance = run_ucb_expt(5)\n",
    "\n",
    "#plt.plot(eps_performance, label='eps') \n",
    "#plt.plot(ucb_performance, label='ucb')\n",
    "#_ = plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
