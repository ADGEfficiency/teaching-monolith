# Sequences

## Content

[keras.ipynb](https://github.com/ADGEfficiency/teaching-monolith/blob/master/sequences/keras.ipynb)
- sequential API
- MNIST
- functional API

[recurrent.ipynb](https://github.com/ADGEfficiency/teaching-monolith/blob/master/sequences/recurrent.ipynb)
- recurrent neural network - motivations & mechanics
- character level language modeling

[lstm.ipynb](https://github.com/ADGEfficiency/teaching-monolith/blob/master/sequences/lstm.ipynb)
- LSTMs - motivations & mechanics
- GRUs
- sin wave prediction

[seq2seq.ipynb](https://github.com/ADGEfficiency/teaching-monolith/blob/master/sequences/seq2seq.ipynb)
- embeddings
- encoder-decoder architecture

[attention.ipynb](https://github.com/ADGEfficiency/teaching-monolith/blob/master/sequences/attention.ipynb)
- additive attention
- dot-product attention
- transformers


## Reccomended reading

[Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)

Attention? Attention! - Lilian Wang - [text](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)

## Further reading

[Transformers from Scratch](http://peterbloem.nl/blog/transformers)

[Visualizing memorization in RNNs](https://distill.pub/2019/memorization-in-rnns/)

[When Recurrent Models Don't Need to be Recurrent](https://bair.berkeley.edu/blog/2018/08/06/recurrent/)

[Recurrent Neural Networks (RNN) and Long Short-Term Memory (LSTM) - Brandon Rohrer - YouTube](https://youtu.be/WCUNPb-5EYI)
