{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras\n",
    "\n",
    "## Tensorflow 1 versus Tensorflow 2\n",
    "\n",
    "Although TF1 will replace TF2, there is still lots of TF1 code around.  It is useful to know what you are looking at.  \n",
    "\n",
    "If you ever see `tf.Session as sess`, `tf.placeholder` or `sess`. you are looking at tf1 code.\n",
    "\n",
    "Some example TF1 code:\n",
    "```python\n",
    "\n",
    "features = tf.placeholder(tf.float32, shape=(32,))\n",
    "output = tf.add(features, 2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    out = sess.run(output, {features: np.zeros(32)})\n",
    "```\n",
    "\n",
    "And the same thing in TF2:\n",
    "```python\n",
    "features = tf.zeros(32)\n",
    "output = features + 2\n",
    "```\n",
    "\n",
    "## Keras\n",
    "\n",
    "Keras is a library that originally wrapped around two deep learning frameworks (Tensorflow & Theano).  In TF2 the integration between Keras & Tensorflow is very tight.\n",
    "\n",
    "In TF2 Keras offers two API's - the higher level **Sequential API** and a lower level **Functional API**. \n",
    "\n",
    "Benefits of Sequential\n",
    "- eaiser & quicker to develop models\n",
    "- less flexible\n",
    "\n",
    "Benefits of Functional\n",
    "- can handle models with non-linear topology\n",
    "- weight sharing\n",
    "- multiple inputs or outputs\n",
    "\n",
    "Start first with Sequential, then move to the Functional if required.\n",
    "\n",
    "## Keras Sequential API\n",
    "\n",
    "Let's make a simple feedforward neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "layers = [tf.keras.layers.Dense(n, activation='relu') for n in [16, 8, 4]]\n",
    "\n",
    "model = tf.keras.Sequential(layers)\n",
    "\n",
    "#  note the use of gradient clipping here!\n",
    "opt = tf.keras.optimizers.Adam(lr=0.001, clipnorm=1.0)\n",
    "\n",
    "model.compile(loss='mse', optimizer=opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our input & output dimensions have been defined for us in the structure of the network. \n",
    "\n",
    "We can do predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(np.zeros([40, 24]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(x=np.zeros([50, 24]), y=np.ones([50, 4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  MNIST\n",
    "\n",
    "MNIST is a classic machine learning dataset \n",
    "- a contribution from [LeCun, Cortes, Bridges](http://yann.lecun.com/exdb/mnist/) in 1998\n",
    "- datasets & benchmarks drive ML progress - curating & contributing datasets is important work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "idx = 7777\n",
    "print(y_train[idx])\n",
    "plt.imshow(x_train[idx], cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to normalize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype(np.float32) / 255\n",
    "x_test = x_test.astype(np.float32) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataset(x_train, y_train, x_test, y_test):\n",
    "    print(x_train.shape, y_train.shape)\n",
    "    print(x_test.shape, y_test.shape)\n",
    "    \n",
    "    #assert np.max(x_train) == np.max(x_test) == 1.0\n",
    "    \n",
    "print_dataset(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try a dense network.  First we need the shapes of the input & output layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  flattening the image \n",
    "input_shape = x_train.reshape(x_train.shape[0], -1).shape[1]\n",
    "\n",
    "#  one node for each class in the output layer\n",
    "num_classes = len(set(y_train))\n",
    "\n",
    "print(input_shape, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  using a scale hyperparameter can be useful to eaisly increase model capacity\n",
    "scale = 6\n",
    "nodes = np.multiply([8, 4, 2], scale)\n",
    "\n",
    "layers = [tf.keras.layers.Dense(nodes[0], input_shape=(input_shape,), activation='relu')]\n",
    "\n",
    "layers += [tf.keras.layers.Dense(n, activation='relu') for n in nodes[1:]]\n",
    "\n",
    "layers += [tf.keras.layers.Dense(num_classes, activation='softmax')]\n",
    "\n",
    "dense = tf.keras.Sequential(layers)\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(lr=0.001, clipnorm=1.0)\n",
    "\n",
    "dense.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "dense.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = dense.fit(\n",
    "    x_train.reshape(x_train.shape[0], -1),\n",
    "    y_train,\n",
    "    epochs=15,\n",
    "    validation_data=(x_test.reshape(x_test.shape[0], -1), y_test),\n",
    "    batch_size=128,\n",
    "    callbacks=[tf.keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=0, batch_size=32, write_graph=True, write_grads=True, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=None, update_freq='epoch')\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(hist.history).loc[:, ['accuracy', 'val_accuracy']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(hist.history).loc[:, ['loss', 'val_loss']].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Early stopping\n",
    "\n",
    "A no-brainer :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.callbacks.EarlyStopping?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 5\n",
    "nodes = np.multiply([8, 4, 2], scale)\n",
    "\n",
    "layers = [tf.keras.layers.Dense(nodes[0], input_shape=(input_shape,), activation='relu')]\n",
    "\n",
    "layers += [tf.keras.layers.Dense(n, activation='relu') for n in nodes[1:]]\n",
    "\n",
    "layers += [tf.keras.layers.Dropout(0.2)]\n",
    "\n",
    "layers += [tf.keras.layers.Dense(num_classes, activation='softmax')]\n",
    "\n",
    "dense = tf.keras.Sequential(layers)\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(lr=0.001)\n",
    "\n",
    "dense.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "dense.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = dense.fit(\n",
    "    x_train.reshape(x_train.shape[0], -1),\n",
    "    y_train,\n",
    "    epochs=20,\n",
    "    validation_data=(x_test.reshape(x_test.shape[0], -1), y_test),\n",
    "    batch_size=128,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(restore_best_weights=True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(hist.history).loc[:, ['loss', 'val_loss']].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Functional API\n",
    "\n",
    "A simple feedforward neural network, setup for MNIST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = tf.keras.Input(shape=(784,))\n",
    "\n",
    "h1 = tf.keras.layers.Dense(64, activation='relu')(features)\n",
    "h2 = tf.keras.layers.Dense(32, activation='relu')(h1)\n",
    "classes = tf.keras.layers.Dense(10, activation='softmax')(h2)\n",
    "\n",
    "model = tf.keras.Model(inputs=features, outputs=classes)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(x_train.reshape(x_train.shape[0], -1), y_train,)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
