{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from common import load_iris, make_cdf\n",
    "\n",
    "from answers import flower_mean_hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Hypothesis Testing\n",
    "\n",
    "References:\n",
    "- Chapter 9 of [Think Stats 2nd Edition](https://greenteapress.com/wp/think-stats-2e/)\n",
    "- Chapter 3 of [Practical Statistics for Data Scientists](https://www.oreilly.com/library/view/practical-statistics-for/9781491952955/).\n",
    "- [Statistics Done Wrong - Alex Reinhart](https://www.statisticsdonewrong.com/)\n",
    "- [Statistics for Hackers - PyCon 2016 - Jake Vanderplas](https://www.youtube.com/watch?v=Iq9DzN6mvYA)\n",
    "\n",
    "## Did I get lucky?\n",
    "\n",
    "You will always observe effects in data due to chance.  \n",
    "- what is the probability of seeing this effect by chance?\n",
    "- **is this effect likely to appear in the larger population**?\n",
    "\n",
    "No tool can tell you if your hypothesis is really true\n",
    "- only if is is consistent with the data\n",
    "\n",
    "## Analytic computation versus simulating\n",
    "\n",
    "Computing the sampling distribution is hard \n",
    "- **simulating the sampling distribution is easy**\n",
    "\n",
    "With analytical (i.e. classical) statistical testing (i.e. Welch's t-testing) \n",
    "- we lose track of what question we are asking\n",
    "- often limited by assumptions of normality (relying on the Central Limit Theorem) or to work with certain data types\n",
    "\n",
    "Sampling/simulation methods\n",
    "- allow intuition in the place of statistical rules\n",
    "- can use for loops to do statistical analysis (often averaging over the runs)\n",
    "\n",
    "Four recipes\n",
    "1. direct simulation\n",
    "2. shuffling - permutation\n",
    "3. bootstrapping\n",
    "4. cross-validation\n",
    "\n",
    "## Hypothesis testing\n",
    "\n",
    "We design experiments to **accept or reject a hypothesis**\n",
    "- in the context of experiment design, *inference* refers to applying the experiment results to a larger population after the conclusion is made\n",
    "\n",
    "Proof by *reductio ad absurdum* \n",
    "- assume that A is false\n",
    "- find a contradiction -> A is true\n",
    "\n",
    "Procedure\n",
    "1. choose a test statistic\n",
    "2. define a null hypothesis\n",
    "3. compute a p-value\n",
    "4. interpret the result\n",
    "\n",
    "The assumption we make is the **null hypothesis** - that the effect is not real\n",
    "- we compute a probability (**p value**) based on that assumption\n",
    "- low probability -> null hypothesis false -> effect is real\n",
    "\n",
    "Limitations\n",
    "- only tells you if the effect is larger than what could be produced by luck\n",
    "- not that the effect is actually significant\n",
    "\n",
    "Possible to measure a small effect with great certainty\n",
    "- we can use hypothesis testing to confirm that a 0.001 difference in customers is statistically significant with a p-value of 96%\n",
    "\n",
    "## The p-value\n",
    "\n",
    "The probability of collecting of collecting data that shows an effect equal/greater than observed.  \n",
    "\n",
    "How likely is the dataset, if we assume the true effect is zero.  It is a **measurement of surprise**.\n",
    "\n",
    "The p-value is controversial \n",
    "- in 2015 the journal Basic and Applied Social Psychology (BASP) stopped publishing papers containing P values because the statistics were too often used to support lower-quality research\n",
    "- p-value hacking (a form of sample bias) is common\n",
    "\n",
    "We want a p-value to give us the probability that an observed effect is due to chance\n",
    "- in reality, the p-value represents the probability that given a random model, results as extreme as we observe can occur\n",
    "\n",
    "For industry based data scientists, who are free from pressure to publish, the p-value is a useful metric that can inform about whether an observed effect is due to chance or not.  \n",
    "- it is a single point of information (among many others) used to inform an decision.\n",
    "\n",
    "## [American Statistical Association 2016 Statement on p-values](https://www.amstat.org/asa/files/pdfs/P-ValueStatement.pdf)\n",
    "\n",
    "1. p-values can indicate how incompatible the data are with a specified statistical model.\n",
    "2. p-values do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone.\n",
    "3. Scientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold.\n",
    "4. Proper inference requires full reporting and transparency.\n",
    "5. A p-value, or statistical significance, does not measure the size of an effect or the importance of a result.\n",
    "6. By itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis. \n",
    "\n",
    "## Significance\n",
    "\n",
    "Probability threshold for significance (alpha) \n",
    "- 5% by convention.\n",
    "- p-values less than an arbitrary threshold are very unlikely & hence significant\n",
    "\n",
    "$ p < 0.05 $\n",
    "\n",
    "No comment on the size of the effect \n",
    "- possible to measure a tiny effect with great certainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jacob Bernoulli \n",
    "\n",
    "1655 - 1705.  Swiss mathematician.  Derived the first version of the Law of Large Numbers.  [Wikipedia](https://en.wikipedia.org/wiki/Jacob_Bernoulli).\n",
    "\n",
    "![](assets/bernoulli.jpg)\n",
    "\n",
    "The Bernoulli distribution is binary - the outcome is a single **bit of infomation** (0 or 1 - more on this in [entropy.ipynb]()):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bernoulli(prob=0.5):\n",
    "    return np.random.choice([0, 1], p=[1-prob, prob])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value for this binary variable is a simple ratio (an example of frequentist probability):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_value(samples):\n",
    "    return sum(samples) / len(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can sample from our Bernoulli distribution, to get data for a series of coin flips:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 200 \n",
    "prob = 0.4\n",
    "\n",
    "samples = [bernoulli(prob) for _ in range(num)]\n",
    "samples[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These samples can then be used to calculate a p-value (a simple ratio of the coin flips):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis test of a biased coin\n",
    "\n",
    "Now lets calculate the p-value for a biased coin, using a null hypothesis of a fair coin.\n",
    "\n",
    "To do this we need:\n",
    "- data - samples from the distribution\n",
    "- an observed effect in that data (aka a test statistic)\n",
    "- a null hypothesis\n",
    "\n",
    "We can define our test statistic and null hypothesis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coin_test_statistic(samples):\n",
    "    #  zero at 50/50\n",
    "    return np.mean(samples) - 0.5\n",
    "\n",
    "def fair_coin(n):\n",
    "    #  the null hypothesis\n",
    "    return [bernoulli(0.5) for _ in range(n)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [bernoulli(prob=0.6) for _ in range(256)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our observed effect size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_effect_size = coin_test_statistic(samples)\n",
    "\n",
    "observed_effect_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now know the size of effect we see in our sampled data.\n",
    "\n",
    "Next step is to simulate the null hypothesis, the results of which we can compare with our observed effect size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stats = []\n",
    "for expt in range(len(samples)):\n",
    "    data = fair_coin(len(samples))\n",
    "    test_stats.append(coin_test_statistic(data))\n",
    "    \n",
    "test_stats = np.array(test_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_val = sum(test_stats > observed_effect_size) / test_stats.shape[0]\n",
    "\n",
    "print('If the null hypothesis is true, we expect to see the effect of {} {} % of the time'.format(\n",
    "    observed_effect_size, p_val*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Put all of the above together into a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fair_coin_hypothesis_test(data, test_statistic, null_hypothesis):\n",
    "    return p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from answers import fair_coin_hypothesis_test\n",
    "samples = [bernoulli(prob=0.42) for _ in range(256)]\n",
    "p_value = fair_coin_hypothesis_test(samples, coin_test_statistic, fair_coin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the difference in means\n",
    "\n",
    "Above we performed a hypothesis test where our effect was the deviation of our data from a fair coin.\n",
    "\n",
    "Let's do some hypothesis testing on some of the effects we can observe in the Iris dataset.\n",
    "\n",
    "We can look at the average values for our features by class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_iris()\n",
    "features, target = data.features, data.target\n",
    "\n",
    "#  feature statistics per class\n",
    "flowers = target.columns\n",
    "\n",
    "print(' ')\n",
    "for flower in flowers:\n",
    "    print(flower)\n",
    "    print(features[target[flower] == 1].describe().loc['mean', :])\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets investigate the difference in **mean sepal length** for **setosa versus virginica**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for flower in flowers:\n",
    "    print(flower, features[target[flower] == 1].describe().loc['mean', 'sepal length (cm)'])\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see an effect here - a difference in mean sepal length for our two flowers.\n",
    "\n",
    "Let's revisit what we need for a hypothesis test:\n",
    "- data \n",
    "- an observed effect in our data aka a test statistic \n",
    "- a null hypothesis\n",
    "\n",
    "Our test statistic is the difference in mean sepal length\n",
    "\n",
    "Our null hypothesis is that there is no difference\n",
    "- we can model the null hypothesis by **shuffling**\n",
    "- this is also known as **permutation**\n",
    "\n",
    "Shuffling / permutation intitution\n",
    "- if the labels dont matter, then switching them shouldn't change the result\n",
    "- only works with representative samples (non biased sampling)\n",
    "- requires multiple groups\n",
    "\n",
    "Let's run a hypothesis test.  First the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = features[target['setosa'] == 1].loc[:, 'sepal length (cm)'].values\n",
    "second = features[target['virginica'] == 1].loc[:, 'sepal length (cm)'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then an observed effect / test statistic (difference in means):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_means(observed, expected):\n",
    "    return abs(np.mean(observed) - np.mean(expected))\n",
    "\n",
    "observed = test_means(first, second)\n",
    "observed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we simulate the null hypothesis using shuffling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_shuffle(pool):\n",
    "    mask = np.random.randint(0, 2, size=pool.shape[0]).astype(bool)\n",
    "    return pool[mask], pool[~mask]\n",
    "\n",
    "pool = np.concatenate([first, second])\n",
    "experiments = 1000\n",
    "test_stats = []\n",
    "\n",
    "for expt in range(experiments):\n",
    "    data = run_shuffle(pool)\n",
    "    test_stats.append(test_means(*data))\n",
    "test_stats = np.array(test_stats)\n",
    "\n",
    "p_val = sum(test_stats > observed) / test_stats.shape[0]\n",
    "print(p_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical\n",
    "\n",
    "Do the same for sub samples from **setosa** only \n",
    "- what kind of p-value are you expecting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.random.randint(0, 2, size=50).astype(bool)\n",
    "setosa = features[target['setosa'] == 1]\n",
    "first = setosa.loc[mask, 'sepal length (cm)']\n",
    "second = setosa.loc[~mask, 'sepal length (cm)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from answers import flower_mean_hypothesis\n",
    "p_value, test_stats = flower_mean_hypothesis(first, second)   \n",
    "print(p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The confidence interval approach\n",
    "\n",
    "In [Statistics Done Wrong](https://www.statisticsdonewrong.com/), Alex Reinhart advocates for using **confidence intervals** to test for significance.\n",
    "\n",
    "Confidence interval = level of confidence a statistic lies in an interval\n",
    "- range of potential values of the population statistic\n",
    "- check significance by checking that the interval doesn't include zero\n",
    "\n",
    "Lets look at the CDF of the difference in average petal length (under the null hypothesis) for our different & same classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, test_diff = flower_mean_hypothesis(\n",
    "    features[target['setosa'] == 1].loc[:, 'sepal length (cm)'].values,\n",
    "    features[target['virginica'] == 1].loc[:, 'sepal length (cm)'].values\n",
    ")\n",
    "\n",
    "mask = np.random.randint(0, 2, size=50).astype(bool)\n",
    "setosa = features[target['setosa'] == 1]\n",
    "_, test_same = flower_mean_hypothesis(\n",
    "    setosa.loc[mask, 'sepal length (cm)'], \n",
    "    setosa.loc[~mask, 'sepal length (cm)']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, x = zip(*make_cdf(test_diff))\n",
    "\n",
    "def find_nearest_idx(array, value):\n",
    "    array = np.asarray(array)\n",
    "    return (np.abs(array - value)).argmin()\n",
    "\n",
    "y = np.array(y)\n",
    "x = np.array(x)\n",
    "\n",
    "start = x[find_nearest_idx(y, 0.05)]\n",
    "end = x[find_nearest_idx(y, 0.95)]\n",
    "print('different class 95% CI - {} {}'.format(start, end))\n",
    "\n",
    "f, ax = plt.subplots(nrows=2, sharex=True, figsize=(10, 4))\n",
    "ax = ax.reshape(-1)\n",
    "\n",
    "ax[0].plot(x, y)\n",
    "ax[0].axvline(start, color='red')\n",
    "ax[0].axvline(end, color='red')\n",
    "ax[0].set_title('different class')\n",
    "\n",
    "y, x = zip(*make_cdf(test_same))\n",
    "\n",
    "y = np.array(y)\n",
    "x = np.array(x)\n",
    "\n",
    "start = x[find_nearest_idx(y, 0.05)]\n",
    "end = x[find_nearest_idx(y, 0.95)]\n",
    "print('same class 95% CI - {} {}'.format(start, end))\n",
    "\n",
    "ax[1].plot(x, y)\n",
    "ax[1].axvline(start, color='red')\n",
    "ax[1].axvline(end, color='red')\n",
    "_ = ax[1].set_title('same class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical - testing a correlation\n",
    "\n",
    "Use the same structure above to get a p-value for the Pearson correlation\n",
    "- the correlation between on mean sepal width (cm) vs mean sepal length (cm)\n",
    "- calc test stats\n",
    "- how often is our observed difference more extreme -> p-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-resampling approaches to hypothesis intervals\n",
    "\n",
    "Developed in the 1930's when resampling was impossible\n",
    "\n",
    "t-test\n",
    "- standardized version of the test statistic\n",
    "- Student's t distribution\n",
    "- approximates the distribution of a single sample mean\n",
    "- numeric data only\n",
    "\n",
    "The resampling approaches we structure the hypothesis testing problem to reflect the data\n",
    "\n",
    "## Chi-Squared tests\n",
    "\n",
    "A simple variation on how we calculate the test statistic.  **We square the deviations** rather than take the absolute (this gives more weight to outliers):\n",
    "\n",
    "$$ \\chi^{2} = \\sum_{i} \\frac{(observed-expected)^2}{expected} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_sq(observed, expected):\n",
    "    observed, expected = np.mean(observed), np.mean(expected)\n",
    "    return (observed - expected)**2 / expected\n",
    "\n",
    "print(test_means(first, second), chi_sq(first, second))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final thoughts on permutation to test a hypothesis\n",
    "\n",
    "We can imagine variants of the shuffling we do above.  \n",
    "\n",
    "One would be an **exhaustive permutation** (also known as exact tests) where we use every possible combination.  Another would be **bootstrap** sampling, where we sample with replacement.  Using a bootstrap will model not only the random assignment of A or B, but also the random element in the selection of subjects from a population.\n",
    "\n",
    "\n",
    "## Statistical power analysis\n",
    "\n",
    "https://machinelearningmastery.com/statistical-power-and-power-analysis-in-python/\n",
    "\n",
    "**How much data do I need to measure an effect**?\n",
    "- large effect with small data = underpowered\n",
    "\n",
    "Power of a test to detect an effect.  Depends on\n",
    "- size of the effect you are looking for\n",
    "- number of samples\n",
    "- measurement error\n",
    "\n",
    "If we think about false positives & negatives in the context of p-value hypothesis testing\n",
    "- false positive = concluding that no effect is actually an effect\n",
    "- false negative = concluding that a real effect is no effect\n",
    "\n",
    "The false positive rate (how often we see chance as reality) is easy to compute - it is the threshold for significance (say 5%).\n",
    "\n",
    "What about the false negative rate - how often will we miss seeing a real effect?\n",
    "- this is harder to calculate because we need to know the true effect size\n",
    "- what we can do is calculate a false negative rate conditioned on an assumed effect size\n",
    "\n",
    "Let's assume our observed effect size.  How often do we miss this size effect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from answers import flower_mean_hypothesis\n",
    "\n",
    "def calc_statistical_power(first, second, trials=1000):\n",
    "    count = 0\n",
    "    for _ in range(trials):\n",
    "        first_sample = np.random.choice(first, first.shape[0], replace=True)\n",
    "        second_sample = np.random.choice(second, second.shape[0], replace=True)\n",
    "\n",
    "        p_val, test_diff = flower_mean_hypothesis(first_sample, second_sample, iters=100)\n",
    "\n",
    "        if p_val > 0.05:\n",
    "            count += 1\n",
    "\n",
    "    print('If the actual effect size is {:.2f}, we expect to miss it {:.2f} % of the time'.format(\n",
    "        np.mean(first) - np.mean(second), 100 * count / trials\n",
    "    ))\n",
    "    \n",
    "first = features[target['setosa'] == 1].loc[:, 'sepal length (cm)'].values\n",
    "second = features[target['virginica'] == 1].loc[:, 'sepal length (cm)'].values\n",
    "\n",
    "calc_statistical_power(first, second, trials=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.random.randint(0, 2, size=50).astype(bool)\n",
    "setosa = features[target['setosa'] == 1]\n",
    "first = setosa.loc[mask, 'sepal length (cm)']\n",
    "second = setosa.loc[~mask, 'sepal length (cm)']\n",
    "\n",
    "calc_statistical_power(first, second, trials=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base rate fallacy\n",
    "\n",
    "[Chapter 4 of Statistics Done Wrong](https://www.statisticsdonewrong.com/).\n",
    "\n",
    "Low base rate -> many chances for false p\n",
    "\n",
    "P-value tells you how probable your data is\n",
    "- it doesn't tell you the probability of the effect\n",
    "\n",
    "The probability of a false positive is almost always higher than the p-value\n",
    "- in areas with low base rates (i.e. early stage drug trials) it is likely that many p < 0.05 results are false positives\n",
    "\n",
    "## Examples\n",
    "\n",
    "5% false positives, disease effects 1/1000\n",
    "- from Fooled by Randomness (Taleb)\n",
    "- probability of actually having disease when testing positive = sick / total positives = 1 / 51\n",
    "\n",
    "Testing drugs\n",
    "- samples = 1000\n",
    "- threshold p = 0.05\n",
    "- base rate = 1% \n",
    "- statistical power = 80%\n",
    "- we find 130 positives (80 true, balance false)\n",
    "\n",
    "How many of drugs work? 10\n",
    "\n",
    "How many will we detect? 8 = 10 * 0.8\n",
    "\n",
    "False discovery rate = 1 - 8 / 13\n",
    "\n",
    "How many false positives? (1000 - 10) * 0.05\n",
    "\n",
    "What is the false positive rate? = 50 / (1000 - 130)\n",
    "\n",
    "Breast cancer\n",
    "- 1000 samples\n",
    "- p = 0.07\n",
    "- 0.8% base rate\n",
    "- 0.9 statistical power\n",
    "\n",
    "How many true positives are there? 8\n",
    "\n",
    "How many will be discovered? 7 = 0.9 * 0.8 * 1000\n",
    "\n",
    "How many false positives? 70 = 992 * 0.07\n",
    "\n",
    "How many with positive test results actually have cancer? 7 / 77+8\n",
    "\n",
    "## Multiple comparisons\n",
    "\n",
    "More tests = greater chance of false positives\n",
    "\n",
    "Functional MRI scans\n",
    "- divide the brain scans into voxels\n",
    "- compare blood flow in sequences of images\n",
    "- many voxels -> many chances for false positives\n",
    "- 'dead salmon' experiment - p=0.001 on an 81 mm3 area of the brain of a dead fish\n",
    "\n",
    "Look-elsewhere effect an apparently statistically significant observation arises by chance because of the sheer size of the search\n",
    "\n",
    "More tests = more chances for false positives\n",
    "- tracking patient symptoms over 12 weeks = 12 chances for false positives\n",
    "- survey with 20 questions -> one false positive (at p=0.05)\n",
    "\n",
    "Bonferroni correction rate\n",
    "- threshold becomes $p/n$\n",
    "- lowers false positive chance (but also reduces power)\n",
    "\n",
    "Benjamini-Hochberg procedure\n",
    "1. get p-value for $m$ comparisons\n",
    "2. choose a false discovery rate $q$\n",
    "3. find the largest $p$ such that $p \\leq iq/m$ ($i$ = rank in list)\n",
    "4. that value (and all smaller) are significant\n",
    "\n",
    "Gives an upper bound on false posities of $q$\n",
    "\n",
    "Cut off becomes more conservative if you \n",
    "- have a smaller false positive threshold ($q$)\n",
    "- making more comparisons ($m$)\n",
    "\n",
    "## Quiz\n",
    "\n",
    "Why do we need to test the effects we see in data?\n",
    "\n",
    "What does a p-value mean to you?\n",
    "\n",
    "What is a Chi-Squared test?\n",
    "\n",
    "What is statistical power?\n",
    "\n",
    "What is the base rate fallacy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
