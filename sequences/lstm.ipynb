{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTMs\n",
    "\n",
    "The content for this course (including many of the images) comes from the excellent [colah's blog](https://colah.github.io) - specifically the post [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid versus Tanh \n",
    "\n",
    "Tanh\n",
    "- [-1, 1]\n",
    "- naturally suited to create output distributions similar to a normal distribution\n",
    "- are good at capturing statistical features like \"this one is average\", \"this one is in the 10th percentile\"\n",
    "\n",
    "Sigmoid\n",
    "- [0, 1]\n",
    "- naturally suited for probabilities or relative amounts - a relative amount to signify how strongly the incoming values should be considered\n",
    "- they don't add up to 100%. that's what softmax is for\n",
    "\n",
    "##  Challenges with recurrent neural networks\n",
    "\n",
    "Two historical challenges\n",
    "- how to train (vanishing & exploding gradients)\n",
    "- how to remember long term (the long term dependency problem)\n",
    "\n",
    "Why does the RNN architiecture cause these two challenges?\n",
    "\n",
    "LSTMs specifically address both of these problems.  They were introduced in 1997 (see [Hochreiter & Schmidhuber (1997) Long Short-Term Memory](http://www.bioinf.jku.at/publications/older/2604.pdf)), and work well on a wide range of problems.\n",
    "\n",
    "## The LSTM\n",
    "\n",
    "The **Long Short Term Memory (LSTM)** network is composed of memory cells.  These cells are composed of three gates\n",
    "- forget gate = what to discard from the state (reset)\n",
    "- input gate = update the state (write)\n",
    "- output gate = what to output (read)\n",
    "\n",
    "The input & forget gate update the internal state.  The input gate protect the cell from irrelevant inputs, the output gate protects other units.  \n",
    "\n",
    "The rest of the neural network communicates with the cells via these gates.  The gates ensure a stable error signal (known as a *constant error carrousel*).\n",
    "\n",
    "These gates not only help with keeping gradients consistent, they also help with longer term memory.\n",
    "\n",
    "## Limitations of LSTMs\n",
    "\n",
    "They are not a solution to everything!  \n",
    "\n",
    "Time series problems where the relevant infomation is within a small time window - or problems that are well solved by traditional autoregression models (ARMIA etc)\n",
    "\n",
    "## Future of LSTMs\n",
    "\n",
    "**Attention (2013)** - let every step of an RNN pick information to look at from some larger collection of information\n",
    "\n",
    "## Recurrent recap\n",
    "\n",
    "Infomation is fed to the next timestep\n",
    "\n",
    "<img src=\"assets/rec.png\" alt=\"\" width=\"800\"/>\n",
    "\n",
    "## LSTM architecture\n",
    "\n",
    "In the LSTM we have more complexity in the cell\n",
    "\n",
    "![](assets/lstm.png)\n",
    "\n",
    "## Three gates\n",
    "\n",
    "### 1 - Forget gate\n",
    "\n",
    "How much of the cell state to forget based on the next observation in the sequence ($x$) and hidden state ($h$)\n",
    "\n",
    "<img src=\"assets/forget.png\" alt=\"\" width=\"300\"/>\n",
    "\n",
    "Output of sigmoid (between 0 and 1) operates on the cell state\n",
    "- 0 = forget everything\n",
    "- 1 = remember everything\n",
    "\n",
    "$$ f_{t} = sigmoid(W_{f} \\cdot [h_{t-1}, x_t] + B_{f}) $$\n",
    "\n",
    "### 2 - Input gate\n",
    "\n",
    "How to update the cell state based on the next observation in the sequence ($x$) and hidden state ($h$)\n",
    "\n",
    "<img src=\"assets/input.png\" alt=\"\" width=\"300\"/>\n",
    "\n",
    "Decide what to update:\n",
    "\n",
    "$$ i_{t} = sigmoid(W_{i} \\cdot [h_{t-1}, x_t] + B_{i})) $$\n",
    "\n",
    "What the new values should be:\n",
    "\n",
    "$$ \\overset{\\sim}{C}_{t} = tanh(W_{C} \\cdot [h_{t-1}, x_t] + B_{C})) $$\n",
    "\n",
    "We can now put together the results of our forget and input gates:\n",
    "\n",
    "$$ C_{t} = f_{t} * {C}_{t-1} + i_{t} * \\overset{\\sim}{C}_{t} $$\n",
    "\n",
    "### 3 - Output gate\n",
    "\n",
    "<img src=\"assets/output.png\" alt=\"\" width=\"300\"/>\n",
    "\n",
    "Which parts of the cell state to output based on the next observation in the sequence ($x$) and hidden state ($h$)\n",
    "\n",
    "What to output from the cell state:\n",
    "\n",
    "$$ o_{t} = sigmoid(W_{o}[h_{t-1},x_{t}] + b_{o}])$$\n",
    "\n",
    "What the output should be:\n",
    "\n",
    "$$ h_{t} = o_{t} * tanh(C_{t}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gated Recurrent Units (GRUs)\n",
    "\n",
    "Simpler than LSTMs & popular\n",
    "\n",
    "Forget & input gates combined into an update gate\n",
    "\n",
    "Cell state & hidden state merged together\n",
    "\n",
    "[Cho, et al. (2014) Learning Phrase Representations using RNN Encoderâ€“Decoder\n",
    "for Statistical Machine Translation](https://arxiv.org/pdf/1406.1078v3.pdf)\n",
    "\n",
    "![](assets/gru.png)\n",
    "\n",
    "See [Greff et. al (2017) LSTM: A Search Space Odyssey](https://arxiv.org/pdf/1503.04069.pdf) for a comparison of different LSTM variants.\n",
    "\n",
    "## Practical\n",
    "\n",
    "Predict a damped sin wave using an LSTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sin, pi, exp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def generate_sequence(length, period, decay):\n",
    "    # generate damped sine wave in [0,1]\n",
    "    return [0.5 + 0.5 * sin(2 * pi * i / period) * exp(-decay * i) for i in range(length)]\n",
    "\n",
    "f, a = plt.subplots()\n",
    "_ = a.plot(generate_sequence(102, 20, 0.05))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
