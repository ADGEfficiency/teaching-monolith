{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from answers import entropy_from_probs, entropy_from_classes, cross_entropy_from_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infomation Theory\n",
    "\n",
    "## Key takeaways\n",
    "\n",
    "- relationship between probability of an event & the infomation we learn from it\n",
    "- infomation \n",
    "    * how uncertain an event is \n",
    "    * $-log P(x)$\n",
    "- entropy\n",
    "    * how much infomation we learn (on expectation) from a probability distribution\n",
    "    * $H(x) = - \\sum p(x) \\log_{2}p(x)$ \n",
    "\n",
    "## Resources & references\n",
    "\n",
    "- Chapter 3 of [Deep Learning - Benjio, Goodfellow & Courville](https://www.deeplearningbook.org/).\n",
    "- Chapter 1.6 of [Pattern Recongition and ML - Bishop](http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf).\n",
    "- [Visual Information Theory - colah's blog](https://colah.github.io/posts/2015-09-Visual-Information/)\n",
    "- [Elements of Infomation Theory - Cover & Thomas](http://www.cs-114.org/wp-content/uploads/2015/01/Elements_of_Information_Theory_Elements.pdf)\n",
    "- [A Short Introduction to Entropy, Cross-Entropy and KL-Divergence - Aurélien Géron - YouTube](https://www.youtube.com/watch?v=ErfnhcEV1O8)\n",
    "- [Shannon Entropy and Information Gain - Luis Serrano - YouTube](https://www.youtube.com/watch?v=9r7FIXEAGvs)\n",
    "\n",
    "## Why infomation theory\n",
    "\n",
    "- ways of measuring and expressing uncertainty\n",
    "- how different two sets of beliefs are\n",
    "- how much an answer to one question tells us about others\n",
    "- how diffuse probability is\n",
    "- the distance between probability distributions\n",
    "- how dependent two variables are\n",
    "\n",
    "## Entropy\n",
    "\n",
    "> ... von Neumann recommended to Shannon that he use the term entropy, not only because of its similarity to the quantity used in physics, but also because “nobody knows what entropy really is, so in any discussion you will always have an advantage” - Bishop\n",
    "\n",
    "### Thermodynamic entropy\n",
    "\n",
    "1st Law = conservation of mass & energy \n",
    "- **you can only break even**\n",
    "- can never create or destroy - only transform\n",
    "\n",
    "2nd Law = entropy always increases\n",
    "- **you always lose**\n",
    "- processes are irreversible\n",
    "- not all energy is avaiable for useful work\n",
    "- puts limits on the efficiency of heat engines\n",
    "- limit based on the temperatures of the heat source and heat sink\n",
    "\n",
    "$$\\eta_{\\text{carnot}} = \\frac{T_{hot} - T_{cold}}{T_{hot}}$$\n",
    "\n",
    "## Infomation entropy\n",
    "\n",
    "- randomness / unpredictability / uncertantity\n",
    "- disorder / non-uniformity\n",
    "- suprise\n",
    "- infomation content\n",
    "\n",
    "### Suprise\n",
    "\n",
    "Improbable events are more suprising & more informative\n",
    "- the sun rising is a low infomation event\n",
    "- Trump being elected is high infomation\n",
    "- Trump being a traitor is low infomation\n",
    "\n",
    "Rarer events provide more infomation\n",
    "\n",
    "## Quantifying infomation\n",
    "\n",
    "We have a qualitative intuition \n",
    "- **learning an unlikely event has performed has more infomation than learning a likely event has happened**\n",
    "\n",
    "We want a measurement that can quantify infomation\n",
    "- guranteed event = zero\n",
    "- likely events = low\n",
    "- less likely = high\n",
    "\n",
    "Low probability samples have more infomation\n",
    "- biased coin is low entropy, unbiased coin is high entropy\n",
    "- maximized for uniform distributions\n",
    "\n",
    "## Claude Shannon\n",
    "\n",
    "1916 - 2001.  American electrical engineer. [Wikipedia](https://de.wikipedia.org/wiki/Claude_Shannon).\n",
    "\n",
    "![](assets/shannon.jpg)\n",
    "\n",
    "Shannon founded **Infomation Theory** in 1948\n",
    "- introduced the bit\n",
    "- originally developed in the context of sending communication via radio\n",
    "- communicating infomation with 0 or 1 started a revolution in communication\n",
    "\n",
    "### The channel communication context\n",
    "\n",
    "Three elements\n",
    "- data source\n",
    "- noisy communication channel\n",
    "- reciever\n",
    "\n",
    "Reciever understanding the data generated by the source from the signal\n",
    "- the fundamental problem of communication\n",
    "\n",
    "Entropy provides a limit on the shortest possible length of a lossless encoding\n",
    "- encoding of data into signal\n",
    "- this is analagous with the thermodynamic limit on conversion of heat to power\n",
    "\n",
    "### Use of the logarithm\n",
    "\n",
    "Shannon's 1948 paper [A Mathematical Theory of Communication](http://math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf) starts out with a discussion of the logarithm\n",
    "\n",
    "The logarithm transforms **exponential into linear relationships**\n",
    "\n",
    "Practical & intuitive\n",
    "\n",
    "> Parameters of engineering importance such as time, bandwidth, number\n",
    "of relays, etc., tend to vary linearly with the logarithm of the number of possibilities. For example, adding one relay to a group doubles the number of possible states of the relays.\n",
    "\n",
    "Mathematically suitable\n",
    "\n",
    "> Many of the limiting operations are simple in terms of the logarithm but would require clumsy restatement in terms of the number of possibilities.\n",
    "\n",
    "Choice of the logarithm base = determines the unit of infomation\n",
    "- $\\log_{e}$ = nats\n",
    "- $\\log_{2}$ = **bits**\n",
    "\n",
    "### Bits\n",
    "\n",
    "Discrete random variable\n",
    "\n",
    "Bit = 0 or 1\n",
    "- but not all bits are useful\n",
    "- one bit reduces uncertantity by 2 (encoding independent)\n",
    "\n",
    "Byte = eight bits\n",
    "- this is the byte in megabytes\n",
    "- can encode an integer from 0 to 255\n",
    "\n",
    "Note that we are working with a \n",
    "\n",
    "### Infomation content\n",
    "\n",
    "Infomation content of an event $h(x)$ is given by\n",
    "\n",
    "$$h(x) = - \\log_{2}p(x)$$\n",
    "\n",
    "- base 2 means infomation is measured in **bits** [0, 1]\n",
    "\n",
    "### Entropy\n",
    "\n",
    "[Entropy - Wikipedia](https://en.wikipedia.org/wiki/Entropy_(information_theory))\n",
    "\n",
    "Entropy $H(x)$ is the average infomation content of a distribution\n",
    "- how much infomation you get when sampling from a probability distribution\n",
    "- an expectation\n",
    "- probability of occuring * infomation content\n",
    "\n",
    "$$H(x) = - \\sum p(x) \\log_{2}p(x)$$\n",
    "\n",
    "$$H(x) = - \\mathbf{E}_{x \\sim P}[\\log_{2} P(x)]$$\n",
    "\n",
    "Entropy can also be written\n",
    "\n",
    "$$ H(x) =  \\mathbf{E}_{x \\sim P} \\cdot \\log_{2} \\frac{1}{P(x)} $$\n",
    "\n",
    "Using the identity $\\log(1/a) = -\\log(a)$\n",
    "\n",
    "Entropy only accounts for infomation about the probability distribution - not the meaning of the events themselves\n",
    "\n",
    "## Practical - implement entropy from probabilities\n",
    "\n",
    "Let's imagine a probability distribution with three discrete states:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "State = namedtuple('State', ['name', 'prob'])\n",
    "\n",
    "s_bahn = [\n",
    "    State('15 minutes', 0.3), \n",
    "    State('30 minutes', 0.2),\n",
    "    State('60 minutes', 0.5)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to calculate the entropy of the distribution using pure Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check with\n",
    "# entropy_from_probs([state.prob for state in s_bahn])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare this with another, more predictable journey home:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_bahn = [\n",
    "    State('15 minutes', 0.5), \n",
    "    State('25 minutes', 0.5)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#entropy_from_probs([state.prob for state in u_bahn])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The more predictable journey has lower entropy**\n",
    "\n",
    "## Practical - implement entropy from categories\n",
    "\n",
    "Implement the calculation of entropy from a single column of states\n",
    "- you will need to create a probability distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = np.random.choice(('A', 'B', 'C'), size=10000)\n",
    "\n",
    "states[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  the answer\n",
    "#  entropy_from_classes(states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A visual guide to infomation theory\n",
    "\n",
    "The images & content below are taken from the excellent [colah's blog](https://colah.github.io) - specifically the post **[Visual Information Theory](https://colah.github.io/posts/2015-09-Visual-Information/)**\n",
    "\n",
    "### Distributions\n",
    "\n",
    "Imagine you have two probability distributions \n",
    "- one over the weather\n",
    "- one over what clothes you wear\n",
    "\n",
    "<img src=\"assets/info1.png\" alt=\"\" width=\"200\"/>\n",
    "\n",
    "If our distributions are **independent**, we can visualize them as follows\n",
    "\n",
    "<img src=\"assets/info2.png\" alt=\"\" width=\"300\"/>\n",
    "\n",
    "This independence means that we can calculate the probability of two events occuring through simple multiplication - there is no interaction\n",
    "\n",
    "We can calculate the **joint** probability of two events by multiplying them together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = {'rain': 0.25, 'sun': 0.75}\n",
    "clothes = {'coat': 0.38, 'shirt': 0.62}\n",
    "\n",
    "weather['sun'] * clothes['shirt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When our events do interact, we need to reassign the probability mass and speak in terms of **conditional** probabilities\n",
    "- these conditional probabilities are extra infomation\n",
    "\n",
    "We can now see that our lines do not intersect both distributions at the same place  - they cut at different points\n",
    "- incorporating these conditional probabilities requires more infomation\n",
    "- $P(coat | rain) = 0.75$\n",
    "- this reassigns the probability mass\n",
    "\n",
    "<img src=\"assets/info3.png\" alt=\"\" width=\"350\"/>\n",
    "\n",
    "These conditional probabilities can be factored out\n",
    "\n",
    "$$P(rain, coat) = P(rain) \\cdot P(coat | rain)$$\n",
    "\n",
    "To calculate the probability that it is raining and we are wearing a coat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.25 * 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can also do the opposite\n",
    "- this feels wierd because it goes in the opposite direction of causality\n",
    "- but it still works\n",
    "\n",
    "We need some additional infomation \n",
    "- $P(rain|coat) = 0.5$ \n",
    "\n",
    "<img src=\"assets/info4.png\" alt=\"\" width=\"300\"/>\n",
    "\n",
    "$$P(coat, rain) = P(coat) \\cdot P(rain | coat)$$\n",
    "\n",
    "The probabilities over clothes are now **marginal** \n",
    "- the probability of wearing clothes without considering the weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.38 * 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "\n",
    "Imagine the problem of communication using bits with a vocabulary of four words\n",
    "\n",
    "<img src=\"assets/info5.png\" alt=\"\" width=\"500\"/>\n",
    "\n",
    "The problem is that communication is expensive \n",
    "- the cost varies with the number of bits\n",
    "\n",
    "The oppourtunity is that some words are more common than others\n",
    "- we can create an encoding that takes advantage of this\n",
    "\n",
    "<img src=\"assets/info6.png\" alt=\"\" width=\"400\"/>\n",
    "\n",
    "We can visualize our old and new encodings\n",
    "\n",
    "<img src=\"assets/info7.png\" alt=\"\" width=\"700\"/>\n",
    "\n",
    "This average message length is the entropy\n",
    "\n",
    "$$ H(x) = - \\mathbf{E}_{x \\sim P}[\\log P(x)] $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_from_probs([1/2, 1/4, 1/8, 1/8], base=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Entropy\n",
    "\n",
    "[Cross-Entropy - Wikipedia](https://en.wikipedia.org/wiki/Cross_entropy)\n",
    "\n",
    "We now have two different distributions over words\n",
    "\n",
    "<img src=\"assets/info8.png\" alt=\"\" width=\"400\"/>\n",
    "\n",
    "We can calculate the average length of a communication when \n",
    "- using the code optimized for one distribution \n",
    "- to communicate events for a second\n",
    "\n",
    "This is the **cross-entropy**\n",
    "\n",
    "$$ H(P,Q) = - \\mathbf{E}_{x \\sim P}\\log Q(x)$$\n",
    "\n",
    "$$ H_{p}(q) = - \\sum\\limits_{x} q(x) \\cdot \\log_{2} p(x) $$\n",
    "\n",
    "\n",
    "Average number of bits needed to \n",
    "- identify a sample \n",
    "- with a coding scheme optimized for an estimated distribution $q$ \n",
    "- rather than the true distribution $p$\n",
    "\n",
    "Minimizing the KLD is the same as minimizing the cross entropy\n",
    "\n",
    "Cross_entropy = entropy + KLD\n",
    "\n",
    "$$H(P,Q) = H(P) + D_{KL}(P||Q)$$\n",
    "\n",
    "If true = predicted -> entropy = cross entropy\n",
    "\n",
    "## Practical - cross entropy\n",
    "\n",
    "Write a function to calculate the cross-entropy between the two distributions above \n",
    "- both $H_{p}(q)$ and $H_{q}(p)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answers\n",
    "\n",
    "# cross_entropy_from_probs(\n",
    "#     [1/2, 1/4, 1/8, 1/8], [1/8, 1/2, 1/4, 1/8]\n",
    "# )\n",
    "\n",
    "# cross_entropy_from_probs(\n",
    "#     [1/8, 1/2, 1/4, 1/8], [1/2, 1/4, 1/8, 1/8]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross entropy tells us \n",
    "- how much longer our messages will be if we use an inappropriate code\n",
    "- how different our distributions are\n",
    "\n",
    "## Kullback–Leibler divergence (KLD)\n",
    "\n",
    "Cross entropy gives us a difference in average infomation content for two encodings\n",
    "- this difference is known as the **Kullback–Leibler divergence**\n",
    "\n",
    "$$D_{KL}(P||Q) = \\mathbf{E}_{x \\sim P}[\\log P(x) - \\log Q(x)] $$\n",
    "\n",
    "The extra amount of infomation needed to send a message containing symbols from $P$ when using a code designed to minimize the length of messages for $Q$\n",
    "\n",
    "## Practical\n",
    "\n",
    "Implement a function to calculate the KLD between our two distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the exercise above you can see that \n",
    "\n",
    "$$H_{p}(q) \\neq H_{q}(p)$$\n",
    "\n",
    "This means that **cross-entropy / KLD is not symmetric**.  Why is this important?\n",
    "- many people refer to the KLD as a distance\n",
    "- this is technically not correct\n",
    "\n",
    "Cross entropy & KLD are very useful in machine learning\n",
    "- cross entropy loss - minimize the difference between a prediction & a prediction\n",
    "- reinforcement policy gradient methods use a constraint or penalty on KLD to stop catastrophic policy updates\n",
    "\n",
    "Suppose we have two distributions $P(x)$ and $Q(x)$\n",
    "- example = a parameterized neural net & the function we are trying to learn\n",
    "\n",
    "We can measure the difference between the two using the **Kullback-Leiber divergence (KLD)** (it is not a true distance measure - not symmetric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimizing cross entropy\n",
    "\n",
    "A common operation in modern ML is minimizing cross entropy between a one hot encoded label & a softmax.\n",
    "\n",
    "The **softmax** is a less aggressive form of one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    exps = np.exp(X)\n",
    "    return exps / np.sum(exps)\n",
    "\n",
    "def normalize(x):\n",
    "    return (x - np.min(x)) / (np.max(x) - np.min(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_map = np.random.normal(size=5)\n",
    "feature_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, a = plt.subplots(nrows=2, sharey=True)\n",
    "a[0].bar(np.arange(len(feature_map)), normalize(feature_map), label='normalized')\n",
    "a[1].bar(np.arange(len(feature_map)), softmax(feature_map), label='softmax')\n",
    "_ = f.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = np.zeros(feature_map.shape[0])\n",
    "label[1] = 1\n",
    "\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy_from_probs(softmax(feature_map), label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
