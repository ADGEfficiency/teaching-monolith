{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy\n",
    "\n",
    "Chapter 3 of [Deep Learning](https://www.deeplearningbook.org/).\n",
    "\n",
    "## Thermodynamic entropy\n",
    "\n",
    "1st Law = conservation of mass & energy = you can only break even\n",
    "- can never create or destroy energy\n",
    "\n",
    "2nd Law = entropy always decreases =  you will always lose\n",
    "- processes are irreversible\n",
    "- puts limits on the efficiency of heat engines\n",
    "\n",
    "## Claude Shannon\n",
    "\n",
    "1916 - 2001.  American electrical engineer. [Wikipedia](https://de.wikipedia.org/wiki/Claude_Shannon).\n",
    "\n",
    "![](assets/shannon.jpg)\n",
    "\n",
    "Shannon founded **Infomation Theory** in 1948.  Shannon's paper [A Mathematical Theory of Communication](http://math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf) starts out with a discussion of the logarithm - I will as well.\n",
    "\n",
    "## Use of the logarithm\n",
    "\n",
    "The logarithm transforms exponential into linear relationships\n",
    "\n",
    "Practical & intuitive\n",
    "\n",
    "> Parameters of engineering importance such as time, bandwidth, number\n",
    "of relays, etc., tend to vary linearly with the logarithm of the number of possibilities. For example,\n",
    "adding one relay to a group doubles the number of possible states of the relays.\n",
    "\n",
    "Mathematically suitable\n",
    "\n",
    "> Many of the limiting operations are simple in terms of the logarithm but would require clumsy restatement in terms of the number of possibilities.\n",
    "\n",
    "Choice of the logarithm base = determines the unit of infomation\n",
    "- $\\log_{e}$ = nats\n",
    "- $\\log_{2}$ = **bits**\n",
    "\n",
    "## Bits of infomation\n",
    "\n",
    "Bit = 0 or 1\n",
    "- but not all bits are useful\n",
    "- one bit reduces uncertantity by 2 (encoding independent)\n",
    "\n",
    "Byte = eight bits\n",
    "- this is the byte in megabytes\n",
    "- can encode an integer from 0 to 255\n",
    "\n",
    "## Infomation Theory\n",
    "\n",
    "Originally developed in the context of sending communication via radio\n",
    "\n",
    "Intuition = **learning an unlikely event has performed is more useful than learning a likely event has happened**\n",
    "\n",
    "We want to quantify this intuition\n",
    "- guranteed event = zero\n",
    "- likely events = low\n",
    "- less likely = high\n",
    "\n",
    "## Shannon Entropy\n",
    "\n",
    "Measurement of \n",
    "- randomness \n",
    "- unpredictability \n",
    "- uncertantity\n",
    "\n",
    "$$H(x) = \\mathbf{E}_{x \\sim P}[\\log P(x)] $$\n",
    "\n",
    "Measurement of infomation\n",
    "- how much infomation (on expectation) you get when sampling from a probability distribution\n",
    "\n",
    "Low probability samples have more infomation\n",
    "- biased coin is low entropy, unbiased coin is high entropy\n",
    "- maximized for uniform distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import entropy as entropyr\n",
    "\n",
    "from common import make_pmf\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def entropy(probs, base=2):\n",
    "    assert sum(probs) == 1.0\n",
    "    return sum([-prob * math.log(prob, base) for prob in probs])\n",
    "\n",
    "np.testing.assert_allclose(entropyr([0.1, 0.9], base=2), entropy([0.1, 0.9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy([0.5, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy([0.1, 0.9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kullback-Leiber divergence (KLD)\n",
    "\n",
    "Suppose we have two distributions $P(x)$ and $Q(x)$\n",
    "- example = a parameterized neural net & the function we are trying to learn\n",
    "\n",
    "We can measure the difference between the two using the **Kullback-Leiber divergence (KLD)** (it is not a true distance measure - not symetric)\n",
    "\n",
    "$$D_{KL}(P||Q) = \\mathbf{E}_{x \\sim P}[\\log P(x) - \\log Q(x)] $$\n",
    "\n",
    "The extra amount of infomation needed to send a message containing symbols from $P$ when using a code designed to minimize the length of messages for $Q$\n",
    "\n",
    "## Cross entropy\n",
    "\n",
    "$$ H(P,Q) = - \\mathbf{E}_{x \\sim P}\\log Q(x)$$\n",
    "\n",
    "Average number of bits needed to \n",
    "- identify a sample \n",
    "- with a coding scheme optimized for an estimated distribution $q$ \n",
    "- rather than the true distribution $p$\n",
    "\n",
    "Minimizing the KLD is the same as minimizing the cross entropy\n",
    "\n",
    "Cross_entropy = entropy + KLD\n",
    "\n",
    "$$H(P,Q) = H(P) + D_{KL}(P||Q)$$\n",
    "\n",
    "If true = predicted -> entropy = cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(p, q):\n",
    "    epsilon = 1e-16\n",
    "    return sum([-tr * math.log(est + epsilon, 2) for tr, est in zip(p, q)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy([0.1, 0.9], [0.1, 0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy([0.5, 0.5], [0.1, 0.9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimizing cross entropy\n",
    "\n",
    "A common operation in modern ML is minimizing cross entropy between a one hot encoded label & a softmax.\n",
    "\n",
    "The **softmax** is a less aggressive form of one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    exps = np.exp(X)\n",
    "    return exps / np.sum(exps)\n",
    "\n",
    "def normalize(x):\n",
    "    return (x - np.min(x)) / (np.max(x) - np.min(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_map = np.random.normal(size=5)\n",
    "feature_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, a = plt.subplots(nrows=2, sharey=True)\n",
    "a[0].bar(np.arange(len(feature_map)), normalize(feature_map), label='normalized')\n",
    "a[1].bar(np.arange(len(feature_map)), softmax(feature_map), label='softmax')\n",
    "_ = f.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = np.zeros(feature_map.shape[0])\n",
    "label[1] = 1\n",
    "\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy(softmax(feature_map), label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now go through two examples to demonstrate the concept of entropy further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The weather example\n",
    "\n",
    "[A Short Introduction to Entropy, Cross-Entropy and KL-Divergence - Aurélien Géron](https://www.youtube.com/watch?v=ErfnhcEV1O8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  weather in two places\n",
    "weather = ['sunny', 'rainy']\n",
    "_, probs = make_pmf(weather)\n",
    "\n",
    "entropy(probs, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_size = len('rainy') * 8\n",
    "message_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  uncertantity_reduction\n",
    "entropy(make_pmf(['sunny', 'rainy'])[1]) - entropy(make_pmf(['rainy'])[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  weather in eight states - equally likely\n",
    "weather = np.arange(8)\n",
    "weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  uncertantity_reduction\n",
    "entropy(make_pmf(weather)[1]) - entropy(make_pmf(['rainy'])[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  what about different probabilities?\n",
    "weather = ['sunny', 'sunny', 'sunny', 'rainy']\n",
    "entropy(make_pmf(weather)[1], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  amount of infomation you gain if you find out it is rainy\n",
    "-math.log(0.25, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  amount of infomation you gain if you find out it is sunny\n",
    "-math.log(0.75, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  average infomation when you learn the weather\n",
    "0.25 * -math.log(0.25, 2) + 0.75 * -math.log(0.75, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets encode our weather using three bits - we have an average message length of three bits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = [\n",
    "    '000', '001', '010', '011', '100', '101', '110', '111'\n",
    "]\n",
    "entropy(weather, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if our weather distribution changes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = [\n",
    "    0.35, 0.35, 0.1, 0.1, 0.04, 0.04, 0.01, 0.01\n",
    "]\n",
    "entropy = sum([-prob * math.log(prob, 2) for prob in probs])\n",
    "entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our weather station is sending an average of 3 bits per message, when the weather's entropy is only 2.23 bits (i.e. we only get 2.23 useful bits)\n",
    "\n",
    "Lets encode our weather smarter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = [\n",
    "    '00', '01', '100', '101', '1100', '1101', '11100', '11101'\n",
    "]\n",
    "\n",
    "cross_entropy = np.sum([len(msg) * prob for msg, prob in zip(weather, probs)])\n",
    "cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets imagine the weather is reversed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = np.sum([len(msg) * prob for msg, prob in zip(weather, reversed(probs))])\n",
    "cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The bucket example\n",
    "\n",
    "https://www.youtube.com/watch?v=9r7FIXEAGvs\n",
    "\n",
    "Lets imagine we have three buckets:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = ['red'] * 4\n",
    "b2 = ['red'] * 3 + ['green']\n",
    "b3 = ['red'] * 2 + ['green'] * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to look an entropy is to consider how many different ways we can rearrange this set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations \n",
    "\n",
    "set(permutations(b1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(permutations(b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(permutations(b3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can be more precise of we think in terms of infomation.\n",
    "\n",
    "How much do we know about what ball we will pick from each bucket?\n",
    "- b1 = high knowledge\n",
    "- b2 = medium knowledge\n",
    "- b3 = low knowledge\n",
    "\n",
    "Let's imagine we play a game where we win if we pick balls out in a given specific order (i.e. as the lists are defined above).  \n",
    "\n",
    "What is the probability of winning for each of our buckets?\n",
    "\n",
    "If we sample with replacement, we are sampling independently\n",
    "- therefore the probability is a product of all the events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1_odds = 1 * 1 * 1 * 1\n",
    "\n",
    "b2_odds = 0.75 * 0.75 * 0.75 * 0.25\n",
    "\n",
    "b3_odds = 0.5 ** 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we don't like the products\n",
    "- if we have many probabilities, the product becomes very small\n",
    "- changing one number can change the entire product by an unknown amount\n",
    "\n",
    "Use a log to change the product into a sum\n",
    "\n",
    "$$ \\log(ab) = \\log(a) + \\log(b) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-(log(1, 2) * 4) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-(log(0.75, 2) * 3 + log(0.25, 2)) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(-log(0.5, 2) * 4) / 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another case of 5 red balls, 3 green:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- (5/8 * log(5/8, 2) + 3/8 * log(3/8, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if there are more classes?\n",
    "- here we can connect entropy with infomation gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = 'a' * 8\n",
    "s2 = 'a' * 4 + 'b' * 2 + 'c' + 'd'\n",
    "s3 = 'a' * 2 + 'b' * 2 + 'c' * 2 + 'd' * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we order these in terms of how easy it is to guess a random letter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- (1 * log(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-(1/2 * log(1/2, 2) + 1/4 * log(1/4, 2) + 1/8 * log(1/8, 2) + 1/8 * log(1/8, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the third sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- log(1/4, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz\n",
    "\n",
    "For s1 - we only need a single question -> 0 entropy\n",
    "\n",
    "For s3 - how can we ask (on average) 2 questions?\n",
    "\n",
    "For s2 - how can we ask 1.75 questions?\n",
    "\n",
    "The entropy is the average number of questions we need to ask\n",
    "- if we use a smart series of questions\n",
    "- height = num of questions we need to ask to figure out letter\n",
    "- if height = k, we have 2^k letters on the bottom\n",
    "\n",
    "![](assets/log.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
