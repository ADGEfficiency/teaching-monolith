{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# seq2seq\n",
    "\n",
    "seq2seq is a branding of the encoder-decoder sequence model introduced in 2014\n",
    "- key contribution = variable input & output sequences\n",
    "- can be used with LSTMs or attention\n",
    "\n",
    "Sutskever et. al (2014) Sequence to Sequence Learning with Neural Networks - [arvix](https://arxiv.org/abs/1409.3215)\n",
    "- 12 million Eng -> French sentences\n",
    "- 10 days to train across 8 GPUs\n",
    "- reverses the order of words in the source (not the target)\n",
    "\n",
    "Applications\n",
    "- translation, question answering, image captioning\n",
    "\n",
    "One limitation of seq2seq is knowning the set of possible output elements in advance\n",
    "- limits seq2seq to solve problems like sorting or travelling salesman\n",
    "\n",
    "\n",
    "## Embeddings\n",
    "\n",
    "Machines understand numbers \n",
    "- if we are working with text, we need to transform the text into discrete tokens (usually individual words)\n",
    "- these tokens are then mapped to a vector representation\n",
    "\n",
    "Special tokens include:\n",
    "- `<EOS>` = end of sentence\n",
    "- `<PAD>` = padding (if you require constant length sequences)\n",
    "\n",
    "One-hot encoding is a type of embedding\n",
    "- simple, sparse, no context infomation about position in the sequence\n",
    "\n",
    "Frequency based embeddings include bag of words or TIDF\n",
    "- simple, accounts for frequency, sparse, no context\n",
    "\n",
    "The word vector representation is more common with neural nets:\n",
    "- each word in corpus is mapped to a dense vector (often length 300 or more)\n",
    "- can be learnt unsupervised by predicting current word - know as Continuous Bag-of-Words (CBOW)\n",
    "\n",
    "Word2Vec (all Mikolov et. al (2013)) - Efficient Estimation of Word Representations in Vector Space + Linguistic Regularities in Continuous Space Word Representations (two papers)\n",
    "\n",
    "<img src=\"assets/word-embed.png\" width=\"50%\" />\n",
    "*From Linguistic Regularities in Continuous Space Word Representations*\n",
    "\n",
    "These word embeddings can be reused for other tasks\n",
    "- transfer learning in NLP = very important\n",
    "- you can use pretrained word embeddings on your task (classification, clustering etc)\n",
    "- driving lots of recent progress in NLP\n",
    "\n",
    "Further reading\n",
    "- What are Word Embeddings - ML Mastery - [text](https://machinelearningmastery.com/what-are-word-embeddings/)\n",
    "\n",
    "## How does a word embedding work?\n",
    "\n",
    "Let's build a simple pipeline.  First we need a corpus (we need to know this ahead of time):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  see page 312 of https://www.physixfan.com/wp-content/files/GEBen.pdf\n",
    "text = 'In short, in using chunked high-level models, we sacrifice determinism for simplicity'\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A naive tokenization based on whitespace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = text.split(' ')\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the word vectors (one for each word).  These will be updated as we train a neural net that uses these vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 4 \n",
    "vecs = np.random.normal(size=(len(tokens), embedding_dim))\n",
    "vecs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encode the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn -Uq\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder()\n",
    "oh = enc.fit_transform(np.array(tokens).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now make a sentence & one hot encode it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = np.array('determinism for simplicity'.split(' ')).reshape(-1, 1)\n",
    "\n",
    "oh = enc.transform(sent)\n",
    "\n",
    "oh.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can finally get our vectorized sentence representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oh * vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a model that can't deal with variable length sequences, here it is common to average over all word vectors to get a fixed length representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder-decoder architecture\n",
    "\n",
    "Now we understand a little about how to embed sequences into vectors, we can look at the downstream model architecture of the seq2seq model:\n",
    "- process the input sequence in the encoder to a fixed length context vector\n",
    "- the encoder output is like a sentence embedding\n",
    "- use that context vector to initialize the decoder and generate the output sequence\n",
    "\n",
    "<img src=\"assets/seq2seq.png\" width=\"70%\" />\n",
    "\n",
    "*From Sutskever et. al (2014)*\n",
    "\n",
    "<img src=\"assets/enc-dec.png\" width=\"70%\" />\n",
    "\n",
    "*From Attention? Attention! - Lilian Wang*\n",
    "\n",
    "\n",
    "## Resources used\n",
    "\n",
    "Attention? Attention! - Lilian Wang - [text](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)\n",
    "\n",
    "What are Word Embeddings - ML Mastery - [text](https://machinelearningmastery.com/what-are-word-embeddings/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
