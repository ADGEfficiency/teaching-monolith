{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# seq2seq\n",
    "\n",
    "Sequence to sequence allows variable length input & output sequences\n",
    "- can be used with LSTMs or attention\n",
    "\n",
    "Sutskever et. al (2014) Sequence to Sequence Learning with Neural Networks - [arvix](https://arxiv.org/abs/1409.3215)\n",
    "- 12 million Eng -> French sentences\n",
    "- 10 days to train across 8 GPUs\n",
    "- reverses the order of words in the source (not the target)\n",
    "\n",
    "Applications\n",
    "- translation, question answering, image captioning\n",
    "\n",
    "One limitation of seq2seq is knowning the set of possible output elements in advance\n",
    "- limits seq2seq to solve problems like sorting or travelling salesman\n",
    "\n",
    "\n",
    "## Embeddings\n",
    "\n",
    "Machines understand numbers - part of any NLP pipeline is tokenization of the text into discrete classes\n",
    "- transforming the tokens (that form our corpus) into a vector\n",
    "\n",
    "Special tokens include:\n",
    "- `<EOS>` = end of sentence\n",
    "- `<PAD>` = padding (if you require constant length sequences)\n",
    "\n",
    "Frequency based embeddings include bag of words or TIDF\n",
    "- these are commonly used to generate features for non-neural network NLP approaches\n",
    "\n",
    "It is more common with neural networks to use vector embeddings\n",
    "- each word in corpus is mapped to a vector\n",
    "- can be learnt unsupervised by predicting current word - know as Continuous Bag-of-Words (CBOW)\n",
    "\n",
    "Word2Vec (all Mikolov et. al (2013)) - Efficient Estimation of Word Representations in Vector Space + Linguistic Regularities in Continuous Space Word Representations (two papers)\n",
    "\n",
    "<img src=\"assets/word-embed.png\" width=\"50%\" />\n",
    "*From Linguistic Regularities in Continuous Space Word Representations*\n",
    "\n",
    "These word embeddings can be reused for other tasks\n",
    "- transfer learning\n",
    "- driving lots of recent progress in NLP\n",
    "\n",
    "Further reading\n",
    "- What are Word Embeddings - ML Mastery - [text](https://machinelearningmastery.com/what-are-word-embeddings/)\n",
    "\n",
    "\n",
    "## Encoder-Decoder architecture\n",
    "\n",
    "<img src=\"assets/seq2seq.png\" width=\"50%\" />\n",
    "\n",
    "*From Sutskever et. al (2014)*\n",
    "\n",
    "Encoder output is like a sentence embedding\n",
    "- entire sequence is processed by an LSTM to produce a fixed length embedding (known as a context vector)\n",
    "- decoder is initialized with this context, then generates the output sequence\n",
    "\n",
    "<img src=\"assets/enc-dec.png\" width=\"50%\" />\n",
    "\n",
    "*From Attention? Attention! - Lilian Wang*\n",
    "\n",
    "\n",
    "## Resources used\n",
    "\n",
    "Attention? Attention! - Lilian Wang - [text](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)\n",
    "\n",
    "What are Word Embeddings - ML Mastery - [text](https://machinelearningmastery.com/what-are-word-embeddings/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
