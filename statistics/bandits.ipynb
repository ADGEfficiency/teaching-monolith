{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading on A/B testing\n",
    "\n",
    "[Udacity A/B testing](https://www.udacity.com/course/ab-testing--ud257)\n",
    "\n",
    "[A/B Testing at Scale Tutorial](https://exp-platform.com/2017abtestingtutorial/)\n",
    "\n",
    "# Multiple Testing\n",
    "\n",
    "Chapter 3 of [Practical Statistics for Data Scientists](https://www.oreilly.com/library/view/practical-statistics-for/9781491952955/), Chapter 2 of [Sutton & Barto - Reinforcement Learning: An Introduction](http://incompleteideas.net/book/RLbook2018.pdf).\n",
    "\n",
    "## Why is classical hypothesis testing not enough?\n",
    "\n",
    "The hypothesis testing we looked at previously was of the form\n",
    "- run an experiment, collecting a dataset of A & B\n",
    "- perform a hypothesis test on any observed effects seen in the dataset\n",
    "\n",
    "There are a few problems with this\n",
    "- we only compare two options (one of which is usually a default)\n",
    "- we might lose money by showing customers a suboptimal option \n",
    "- the hypothesis test only shows us if an observed effect is unlikely, not if the effect is large\n",
    "- in a real business, there is no 'experiment over' date\n",
    "- the real world is non-stationary - the results we collect might be from a distribution that is changing\n",
    "\n",
    "We want an experiment where we can take advantage of the results as we learn\n",
    "\n",
    "In a business context we are not concerned with statistical significance\n",
    "- we are concerned with optimizing user experience as quickly as possible\n",
    "\n",
    "## The mulit-armed bandit\n",
    "\n",
    "Bandits allow\n",
    "- testing mulitple options at once\n",
    "- reach conclusions faster\n",
    "\n",
    "The term bandit comes from slot machines \n",
    "- known as one armed bandits for their ability to extract money from gambles\n",
    "\n",
    "The goal of a multi armed bandit problem is to vin as much money as possible\n",
    "- this is the same as figuring out which arm is best as quick as possible\n",
    "\n",
    "## The reinforcement learning approach\n",
    "\n",
    "Is supervised learning we use data to predict\n",
    "- in a bandit problem we could predict the expected value of each arm\n",
    "- this is prediction / evaluation\n",
    "\n",
    "Use data (the results we get from pulling an arm) to select an action\n",
    "- this is control\n",
    "\n",
    "We can define the value of an action (pulling a specific arm) as an expectation\n",
    "- the expected reward of an action\n",
    "\n",
    "$q_{*}(a) = \\mathbb{E}[r(a)]$\n",
    "\n",
    "Reinforcement has a convienent goal - maximizing expected reward\n",
    "- if we did know the true expectation of each action, maximization is an argmax\n",
    "\n",
    "The bandit problem is one step short of the full reinforcement learning problem\n",
    "- the bandit is a single state, no transitions of states happen\n",
    "\n",
    "Bandits share in common with reinforcement learning\n",
    "- exploration verses exploitation\n",
    "- potentially non-stationary\n",
    "\n",
    "## Exploration versus exploitation\n",
    "\n",
    "todo\n",
    "\n",
    "## Example\n",
    "\n",
    "Let's imagine you have the following results from comparing three different landing pages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "Param = namedtuple('Parameter', ['loc', 'scale', 'initial_size'])\n",
    "\n",
    "params = {\n",
    "    'A': Param(24, 20, 1),\n",
    "    'B': Param(25, 20, 1),\n",
    "    'C': Param(26, 20, 1),\n",
    "    'D': Param(24, 10, 1),\n",
    "    'E': Param(25, 10, 1)\n",
    "}\n",
    "\n",
    "start = 10\n",
    "end = 50\n",
    "num_options = 20\n",
    "\n",
    "params = {\n",
    "    str(option): Param(loc, scale, 1) \n",
    "    for option, (loc, scale) \n",
    "    in enumerate(zip(np.linspace(start, end, num_options), np.random.uniform(10, size=num_options)))\n",
    "}\n",
    "\n",
    "results = {\n",
    "    arm: list(np.random.normal(*stats))\n",
    "    for arm, stats in params.items()\n",
    "}\n",
    "\n",
    "def expectation(results):\n",
    "    return {arm: np.mean(data) for arm, data in results.items()}\n",
    "\n",
    "expectation(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One approach to the results above would be to conclude that one option is optimal and send all our users there.  This would be a **greedy** solution to the exploration & exploitation dilemma.\n",
    "\n",
    "Another solution would be to favour the option that appears optimal, while still sampling from the options that appear sub-optimal.\n",
    "\n",
    "## epsilon-greedy\n",
    "\n",
    "A simple algorithm to tackle the exploration-exploitation dilemma is known as **epsilon-greedy** - it is the method used for exploration in DeepMind's 2013 DQN.\n",
    "\n",
    "The algorithm has a single parameter $\\epsilon$, which controls how greedy we are.  The basic algorithm is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance(results):\n",
    "    d = []\n",
    "    for arm, data in results.items():\n",
    "        d.extend(data)\n",
    "    return np.mean(d)\n",
    "\n",
    "results = {\n",
    "    arm: list(np.random.normal(*stats))\n",
    "    for arm, stats in params.items()\n",
    "}\n",
    "\n",
    "eps = 0.3\n",
    "choices = list(params.keys())\n",
    "\n",
    "steps = 1000\n",
    "values = np.zeros((steps, len(choices)))\n",
    "actions = np.empty((steps)).astype(str)\n",
    "eps_performance = np.zeros(steps)\n",
    "\n",
    "for step in range(steps):\n",
    "    prob = np.random.rand()\n",
    "    if prob < eps:\n",
    "        strat = 'random'\n",
    "        action = np.random.choice(choices)\n",
    "\n",
    "    else:\n",
    "        strat = 'greedy'\n",
    "        expectations = expectation(results)\n",
    "        values[step, :] = list(expectations.values())\n",
    "        action = max(expectations, key=expectations.get)\n",
    "        \n",
    "    actions[step] = action\n",
    "    \n",
    "    p = params[action]\n",
    "    results[action].append(float(np.random.normal(p.loc, p.scale, 1)))\n",
    "    eps_performance[step] = get_performance(results)\n",
    "    \n",
    "plt.plot(eps_performance, label='eps {}'.format(eps))\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from above that\n",
    "- $\\epsilon$ = 1 -> standard A/B test\n",
    "- $\\epsilon$ = 0 -> greedy\n",
    "\n",
    "In reinforcement learning $\\epsilon$ is often decayed from 1 to a 0.05 over an agents lifetime.  Proper selection of $\\epsilon$ depends on\n",
    "- how accurate your greedy estimate is\n",
    "- how non-stationary the process is\n",
    "\n",
    "## Upper Confidence Bound (UCB)\n",
    "\n",
    "2.7 in [Sutton & Barto - Reinforcement Learning: An Introduction](http://incompleteideas.net/book/RLbook2018.pdf)\n",
    "\n",
    "Select an action based on it's historical mean + an exploration bonus\n",
    "\n",
    "$a = \\underset{x}{\\text{argmax}} \\left[ q(a) + c \\cdot \\sqrt{\\frac{\\ln t}{N(a)}} \\right] $\n",
    "\n",
    "$t$ = timestep\n",
    "\n",
    "$N(a)$ = number of times action $a$ taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 5\n",
    "\n",
    "def ucb(results, step):\n",
    "    return {\n",
    "        arm: np.mean(data)+ c * np.sqrt(np.log(step)/len(data))\n",
    "        for arm, data in results.items()\n",
    "    }\n",
    "\n",
    "def get_performance(results):\n",
    "    d = []\n",
    "    for arm, data in results.items():\n",
    "        d.extend(data)\n",
    "    return np.mean(d)\n",
    "\n",
    "results = {\n",
    "    arm: list(np.random.normal(*stats))\n",
    "    for arm, stats in params.items()\n",
    "}\n",
    "\n",
    "steps = 1000\n",
    "values = np.zeros((steps, len(choices)))\n",
    "actions = np.empty((steps)).astype(str)\n",
    "ucb_performance = np.zeros(steps)\n",
    "\n",
    "for step in range(steps):\n",
    "    ucbs = ucb(results, 2)\n",
    "\n",
    "    action = max(ucbs, key=ucbs.get)\n",
    "    actions[step] = action\n",
    "    values[step, :] = list(ucbs.values())\n",
    "    \n",
    "    p = params[action]\n",
    "    results[action].append(float(np.random.normal(p.loc, p.scale, 1)))\n",
    "    ucb_performance[step] = get_performance(results)\n",
    "\n",
    "plt.plot(eps_performance, label='eps') \n",
    "plt.plot(ucb_performance, label='ucb')\n",
    "_ = plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
