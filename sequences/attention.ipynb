{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention\n",
    "\n",
<<<<<<< HEAD
    "## Context of attention\n",
    "\n",
    "One of the most important recent developments in deep learning\n",
    "- 2015 = Attention mechanism introduced\n",
    "- 2017 = the Transformer (entire network architecture)\n",
    "\n",
    "The Transformer has been crucial to the impressive developments in NLP\n",
    "- Transformer based language models such as BERT, GPT3 etc are the reason why NLP is so hot right now\n",
=======
    "## Context\n",
    "\n",
    "One of the most important recent developments in deep learning\n",
    "- 2015 = attention \n",
    "- 2017 = the Transformer (entire network architecture)\n",
    "\n",
    "The Transformer has been crucial to the impressive developments in NLP\n",
    "- Transformer based language models such as BERT, GPT3 etc\n",
>>>>>>> 1378f2aecd219e1172b05e1f713b00d908a08b43
    "\n",
    "Applications\n",
    "- machine translation (text or audio)\n",
    "- question answering\n",
    "- parsing sentences into grammar trees\n",
    "- generating solutions to symbolic math problems\n",
<<<<<<< HEAD
    "- time series\n",
    "\n",
    "\n",
    "## Attention\n",
    "\n",
    "The intuition behind attention is that some parts of the sequence are more relevant than others\n",
    "- pay attention to different parts of a sentence (skim reading)\n",
    "- pay attention to different parts of an image \n",
    "- pay attention to parts of the sequence are relevant\n",
    "\n",
    "A fundamental problem in sequence modeling historically is sequence length\n",
    "\n",
    "**Attention removes any restrictions based on length**\n",
=======
    "\n",
    "## Attention\n",
    "\n",
    "**Attention removes any restrictions based on distance** in sequence data\n",
>>>>>>> 1378f2aecd219e1172b05e1f713b00d908a08b43
    "- deal with the whole sentence at once\n",
    "- (learnable) shortcuts between the context vector & input sequence\n",
    "- no need for recurrence!\n",
    "\n",
<<<<<<< HEAD
    "Attention can be thought of as search \n",
    "- searching for something, paying attention to something\n",
    "- searching learned embeddings\n",
    "- allows alignment (what other words are similar to this word?)\n",
=======
    "Attention allows alignment\n",
    "- searching learned embeddings\n",
    "- LSTM produces the embeddings\n",
>>>>>>> 1378f2aecd219e1172b05e1f713b00d908a08b43
    "\n",
    "Used with images & text\n",
    "- attention is a vector of importance weights\n",
    "- to predict, we multiply the other elements in the sequence by these attention weights\n",
    "\n",
<<<<<<< HEAD
    "Possible to use with LSTMS, or without\n",
    "- without = self attention = the Transformer\n",
    "\n",
=======
    "We can also think about attention as search\n",
    "- searching for something, paying attention to something\n",
    "\n",
    "The intuition \n",
    "- pay attention to different parts of a sentence (skim reading)\n",
    "- pay attention to different parts of an image \n",
    "- pay attention to parts of the sequence are relevant\n",
    "\n",
    "\n",
    "Possible to use with LSTMS, or without\n",
    "- without = self attention = the Transformer\n",
    "\n",
    "\n",
>>>>>>> 1378f2aecd219e1172b05e1f713b00d908a08b43
    "## Attention + seq2seq\n",
    "\n",
    "In seq2seq we build a single context vector\n",
    "- from the encoders last hidden state\n",
    "- acts like a sentence embedding\n",
    "- all infomation from the encoder flows through the fixed length context vector\n",
    "\n",
    "With attention, we create shortcut between the entire input sequence and the context vector\n",
    "- these shortcuts are weighted\n",
    "- weights = the strength of attention between the input & context\n",
    "\n",
    "\n",
    "### Additive Attention\n",
    "\n",
    "Bahdanau et. al (2015) Neural Machine Translation by Jointly Learning to Align and Translate - [arxiv](https://arxiv.org/abs/1409.0473) \n",
<<<<<<< HEAD
    "- using attention with RNN encoder-decoder\n",
    "- bidirectional RNN to produce the encoder hidden state (fwd & bwd hidden states concatenated)\n",
    "- all of these hidden states are used to generate the context vector via Additive Attention\n",
    "\n",
    "<img src=\"assets/enc-dec-attention.png\" width=\"70%\"/>"
=======
    "Using attention with RNN encoder-decoder\n",
    "\n",
    "![](assets/enc-dec-attention.png)\n",
    "\n",
    "Bidirectional RNN to produce the encoder hidden state (fwd & bwd hidden states concatenated)\n",
    "\n",
    "All of these hidden states are used to generate the context vector\n",
    "- via additive attention \n",
    "- note that additive attention allows\n"
>>>>>>> 1378f2aecd219e1172b05e1f713b00d908a08b43
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "## Additive Attention layer details\n",
    "\n",
    "Here we will attempt to reproduce the Additive Attention mechanism\n",
    "- [Additive Attention layer TF documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/AdditiveAttention)"
=======
    "## Additive Attention\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/layers/AdditiveAttention"
>>>>>>> 1378f2aecd219e1172b05e1f713b00d908a08b43
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#  shape = (batch, time, features)\n",
    "qry = np.random.normal(size=128).reshape(4, 8, -1).astype(np.float32)\n",
    "val = np.random.normal(size=128).reshape(4, 8, -1).astype(np.float32)\n",
    "key = val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the built in TF AdditiveAttention layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow -q\n",
    "import tensorflow as tf\n",
    "\n",
    "net = tf.keras.layers.AdditiveAttention(use_scale=False)\n",
    "out = net([qry, val, key])\n",
    "\n",
    "print(np.sum(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reproduce this using Tensorflow components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = qry.reshape(4, 8, 1, -1)\n",
    "val = val.reshape(4, 1, 8, -1)\n",
    "\n",
    "scores = tf.reduce_sum(tf.tanh(qry + val), axis=-1)\n",
    "dist = tf.nn.softmax(scores)\n",
    "out = tf.matmul(dist, val.reshape(4, 8, -1))\n",
    "\n",
    "print(np.sum(scores), np.var(dist), np.sum(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Implement Additive Attention without using Tensorflow\n",
    "- use numpy, scipy etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Transformer\n",
    "\n",
<<<<<<< HEAD
    "A really important architecture\n",
    "- [This Word Doesn't Exist](https://www.thisworddoesnotexist.com/)\n",
    "- OpenAI's GPT-3 may be the biggest thing since bitcoin - [blog-post](https://maraoz.com/2020/07/18/openai-gpt3/)\n",
    "\n",
    "Transformer\n",
    "- sequences without recurrent nn\n",
    "- much faster to train than recurrent models (no backprop through time)\n",
    "- this ability to train faster means the size of the model can be scaled up massively\n",
    "- introduced in 2017 - Attention is All you Need - Vaswani, et al.\n",
    "\n",
    "\n",
    "## Architecture \n",
    "\n",
    "<img src=\"assets/transformer.png\" width=\"70%\"/>\n",
    "\n",
    "In order to be able to understand the transformer, we first need to understand\n",
    "- Dot-Product Attention\n",
    "- Self-Attention\n",
    "\n",
    "## The Dot-Product as similarity\n",
    "\n",
    "Below we compare with cosine distance:\n",
    "\n",
    "$a \\cdot b = \\sum_{i} a_i b_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib -Uq\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "data = defaultdict(list)\n",
    "for _ in range(100):\n",
    "    a = np.random.normal(size=128).reshape(-1).astype(np.float32)\n",
    "    b = np.random.normal(size=128).reshape(-1).astype(np.float32)\n",
    "    data['cosine'].append(cosine(a, b))\n",
    "    data['matmul'].append(np.matmul(a, b))\n",
    "    \n",
    "_ = plt.scatter(data['cosine'], data['matmul'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
=======
    "This word doesn't exist\n",
    "- OpenAI's GPT-3 may be the biggest thing since bitcoin - [blog-post](https://maraoz.com/2020/07/18/openai-gpt3/)\n",
    "\n",
    "![](assets/transformer.png)\n",
    "\n",
    "Transformer\n",
    "- sequences without recurrent nn\n",
    "- much faster to train than recurrent models\n",
    "\n",
    "We first need to understand\n",
    "- Dot-Product Attention\n",
    "- Self-Attention\n",
    "\n",
    "“Attention is All you Need” (Vaswani, et al., 2017)\n",
    "\n",
    "\n",
    "## The Dot-Product as similarity\n",
    "\n",
    "Compare with cosine distance\n",
    "\n",
    "$a \\cdot b = \\sum_{i} a_i b_i$\n",
    "\n",
    "\n",
>>>>>>> 1378f2aecd219e1172b05e1f713b00d908a08b43
    "## Dot-Product Attention\n",
    "\n",
    "Above we have seen how we can use additive attention to create a context vector\n",
    "- after 2015 other attention mechanisms were developed\n",
    "\n",
    "Dot-Product Attention is important\n",
    "- it powers the transformer\n",
<<<<<<< HEAD
    "- weighted sum of values\n",
    "- weight = dot prod of Q with all keys (similarity)\n",
    "- values = the values of the input\n",
    "\n",
    "$score(s, h) = \\frac{s^{T} \\cdot h}{sqrt(n)}$\n",
    "\n",
=======
    "\n",
    "$score(s, h) = s^{T} h / sqrt(n)$\n",
    "\n",
    "- weight sum of values\n",
    "- weight = dot prod of Q with all keys (similarity)\n",
    "- values = the values of the input\n",
>>>>>>> 1378f2aecd219e1172b05e1f713b00d908a08b43
    "\n",
    "## Self-Attention\n",
    "\n",
    "Relate a sequence to itself\n",
    "- learn the relationship between the words in a sentence\n",
    "- how similar is this word to other words in this sentence\n",
    "- rather than learning the relationship between two different sequences (such as source + target)\n",
    "- useful in machine reading, summarization & image caption generation\n",
    "\n",
    "Introspective attention\n",
    "- attend to a networks own internal state (rather than data)\n",
    "\n",
    "Cheng et. al (2016) LSTM for Machine Reading\n",
    "\n",
    "\n",
    "## Key, value, query\n",
    "\n",
    "Input = (key, value) pairs\n",
    "- length = input seq. length\n",
    "- key is a way to index the value\n",
    "- for translation, both the key and value are the same (encoder hidden states)\n",
    "\n",
    "Encoder\n",
    "\n",
    "Decoder produces the query for the next layer\n",
    "\n",
    "- decoder compresses previous output into a query, and maps this query to the (k, v) to produce output\n",
    "\n",
    "Output embedding = the predicted sentence so far\n",
    "\n",
    "## Multihead\n",
    "\n",
    "Just running the Dot-Product attention in parallel\n",
    "- outputs of all the heads are concatenated\n",
    "\n",
    "\n",
<<<<<<< HEAD
    "\n",
=======
    "https://www.tensorflow.org/tutorials/text/nmt_with_attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
>>>>>>> 1378f2aecd219e1172b05e1f713b00d908a08b43
    "## Dot-Product Attention\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/layers/Attention\n",
    "\n",
    "The meaning of query, value and key depend on the application. In the case of text similarity, for example, query is the sequence embeddings of the first piece of text and value is the sequence embeddings of the second piece of text. key is usually the same tensor as value.\n",
    "\n",
    "First lets generate some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#  shape = (batch, time, features)\n",
    "qry = np.random.normal(size=128).reshape(4, 8, -1).astype(np.float32)\n",
    "val = np.random.normal(size=128).reshape(4, 8, -1).astype(np.float32)\n",
    "key = val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the dot-product attention in Tensorflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = tf.keras.layers.Attention(use_scale=False)\n",
    "out = net([qry, val], training=False)\n",
    "np.sum(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we reproduce the above using the lower level Tensorflow components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  similarity between the query and our keys\n",
    "scores = tf.matmul(qry, key, transpose_b=True)\n",
    "\n",
    "#  softmax to normalize\n",
    "dist = tf.nn.softmax(scores)\n",
    "\n",
    "#  apply our attention scores to the values\n",
    "out = tf.matmul(dist, val)\n",
    "print(np.sum(scores), np.var(dist), np.sum(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exerices\n",
    "\n",
    "Implement Dot-Product Attention without Tensorflow\n",
    "- use the `scipy` softmax to be able to softmax over the last dimension\n",
    "- transpose the array in a way that we don't transpose the batch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "#  see https://stackoverflow.com/questions/48100954/why-does-tf-matmula-b-transpose-b-true-work-but-not-tf-matmula-tf-transpos\n",
    "scores = np.matmul(qry, np.transpose(key, (0, 2, 1)))\n",
    "dist = softmax(scores, axis=-1)\n",
    "out = np.matmul(dist, val)\n",
    "print(np.sum(scores), np.var(dist), np.sum(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
<<<<<<< HEAD
    "Try to get the [Neural machine translation with attention](https://www.tensorflow.org/tutorials/text/nmt_with_attention) tutorial working on Google Colab"
=======
    "https://www.tensorflow.org/tutorials/text/nmt_with_attention\n"
>>>>>>> 1378f2aecd219e1172b05e1f713b00d908a08b43
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of attention\n",
    "\n",
    "Attention by location\n",
    "- look at the position in the sequence only\n",
    "- weighted distributions of different offsets\n",
    "\n",
    "SNAIL (Mishra et al. (2017))\n",
    "- improvements to the encoding of position\n",
    "\n",
    "Attention by content (min 42)\n",
    "- associative content\n",
    "- key vector $a$, compared to all glimpses $g$, using similarity function $S$\n",
    "- output a vector (key) that represents (green stones), then do cosine similarity \n",
    "\n",
    "Content based addressing\n",
    "- attention based on vector similarity\n",
    "- cosine similarity -> softmax\n",
    "\n",
    "$S$ can be learned (MLP) or fixed (dot product, cosine similarity etc)\n",
    "\n",
    "\n",
    "## Attention on images\n",
    "\n",
    "Visual attention = images\n",
    "\n",
    "image capitioning example\n",
    "- encoder = convnet\n",
    "- decoder = lstm\n",
    "- min 2:38 (https://www.youtube.com/watch?v=W2rWgXJBZhU)\n",
    "\n",
    "\n",
    "Attention and Memory in Deep Learning - DeepMind 2018 Lecture - [youtube](https://www.youtube.com/watch?v=Q57rzaHHO0k)\n",
    "\n",
    "Attention can turn fixed data into a sequence\n",
    "- feed the pixels of an image as a sequence\n",
    "\n",
    "![](assets/neural-attention.png)\n",
    "\n",
    "Glimpse = fixed size\n",
    "- probability distribution over the data\n",
    "- discretize image & softmax\n",
    "\n",
    "Extended into CV in 2015\n",
    "- (Xu et al. 2015)\n",
    "\n",
    "## Hard versus soft\n",
    "\n",
    "Hard attention = fixed size windows\n",
    "- sample over all possible glimpses\n",
    "- soft attention = more on one region, less on another\n",
    "\n",
    "Soft attention\n",
    "- expectation over all possible glimpses\n",
    "- becomes differentiable\n",
    "\n",
    "Hard = only one subregion\n",
    "\n",
    "Soft vs hard attention\n",
    "- soft = all parts of image\n",
    "- hard = only one part of image at a time\n",
    "\n",
    "\n",
    "## Global vs local attention\n",
    "\n",
    "- global = similar to soft\n",
    "- local = blend of hard & soft (also differentiable)\n",
    "- local = first predict a window, then do attention within that window\n",
    "\n",
    "\n",
    "Xu et. al (2015) Show, Attend and Tell: Neural Image CaptionGeneration with Visual Attention\n",
    "- generating captions for images\n",
    "\n",
    "Encoder\n",
    "\n",
    "- embedding layers used to transform the input & target sequences to 512\n",
    "- sin wave used to encode input (summed with embedding output)\n",
    "- softmax & linear at the decoder output\n",
    "\n",
    "\n",
    "## Resources Used\n",
    "\n",
    "Attention? Attention! - Lilian Wang - [text](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)\n",
    "\n",
    "C5W3L07 Attention Model Intuition - [youtube](https://youtu.be/SysgYptB198)\n",
    "\n",
    "Attention and Memory in Deep Learning - DeepMind 2018 Lecture - [youtube](https://www.youtube.com/watch?v=Q57rzaHHO0k)\n",
    "\n",
<<<<<<< HEAD
    "Attention Is All You Need - [youtube](https://www.youtube.com/watch?v=iDulhoQ2pro)\n",
    "\n",
    "Are we in an AI overhang? - Andy Jones [blog post](https://www.lesswrong.com/posts/N6vZEnCn6A95Xn39p/are-we-in-an-ai-overhang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
=======
    "Attention Is All You Need - [youtube](https://www.youtube.com/watch?v=iDulhoQ2pro)"
   ]
>>>>>>> 1378f2aecd219e1172b05e1f713b00d908a08b43
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
