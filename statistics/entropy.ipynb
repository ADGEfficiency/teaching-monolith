{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from answers import entropy_from_probs, entropy_from_classes, cross_entropy_from_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Theory\n",
    "\n",
    "## Key takeaways\n",
    "\n",
    "- relationship between probability of an event & the information we learn from it\n",
    "- information \n",
    "    * how uncertain an event is \n",
    "    * $-log P(x)$\n",
    "- entropy\n",
    "    * how much information we learn (on expectation) from a probability distribution\n",
    "    * $H(x) = - \\sum p(x) \\log_{2}p(x)$ \n",
    "- cross-entropy\n",
    "- KLD\n",
    "\n",
    "## Resources & references\n",
    "\n",
    "- Chapter 3 of [Deep Learning - Benjio, Goodfellow & Courville](https://www.deeplearningbook.org/).\n",
    "- Chapter 1.6 of [Pattern Recongition and ML - Bishop](http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf).\n",
    "- [Visual Information Theory - colah's blog](https://colah.github.io/posts/2015-09-Visual-Information/)\n",
    "- [Elements of Information Theory - Cover & Thomas](http://www.cs-114.org/wp-content/uploads/2015/01/Elements_of_Information_Theory_Elements.pdf)\n",
    "- [A Short Introduction to Entropy, Cross-Entropy and KL-Divergence - Aurélien Géron - YouTube](https://www.youtube.com/watch?v=ErfnhcEV1O8)\n",
    "- [Shannon Entropy and Information Gain - Luis Serrano - YouTube](https://www.youtube.com/watch?v=9r7FIXEAGvs)\n",
    "\n",
    "## Why information theory\n",
    "\n",
    "- ways of measuring and expressing uncertainty\n",
    "- how different two sets of beliefs are\n",
    "- how much an answer to one question tells us about others\n",
    "- how diffuse probability is\n",
    "- the distance between probability distributions\n",
    "- how dependent two variables are\n",
    "\n",
    "## Entropy\n",
    "\n",
    "> ... von Neumann recommended to Shannon that he use the term entropy, not only because of its similarity to the quantity used in physics, but also because “nobody knows what entropy really is, so in any discussion you will always have an advantage” - Bishop\n",
    "\n",
    "### Thermodynamic entropy\n",
    "\n",
    "1st Law = conservation of mass & energy \n",
    "- **you can only break even**\n",
    "- can never create or destroy - only transform\n",
    "\n",
    "2nd Law = entropy always increases\n",
    "- **you always lose**\n",
    "- processes are irreversible\n",
    "- not all energy is avaiable for useful work\n",
    "- puts limits on the efficiency of heat engines\n",
    "- limit based on the temperatures of the heat source and heat sink\n",
    "\n",
    "$$\\eta_{\\text{carnot}} = \\frac{T_{hot} - T_{cold}}{T_{hot}}$$\n",
    "\n",
    "## Information entropy\n",
    "\n",
    "- randomness / unpredictability / uncertainty\n",
    "- disorder / non-uniformity\n",
    "- surprise\n",
    "- information content\n",
    "\n",
    "### Surprise\n",
    "\n",
    "Improbable events are more surprising & more informative\n",
    "- the sun rising is a low information event\n",
    "- Trump being elected is high information\n",
    "- Trump being a traitor is low information\n",
    "\n",
    "Rarer events provide more information\n",
    "\n",
    "## Quantifying information\n",
    "\n",
    "We have a qualitative intuition \n",
    "- **learning an unlikely event has performed has more information than learning a likely event has happened**\n",
    "\n",
    "We want a measurement that can quantify information\n",
    "- guaranteed event = zero\n",
    "- likely events = low\n",
    "- less likely = high\n",
    "\n",
    "Low probability samples have more information\n",
    "- biased coin is low entropy, unbiased coin is high entropy\n",
    "- maximized for uniform distributions\n",
    "\n",
    "## Claude Shannon\n",
    "\n",
    "1916 - 2001.  American electrical engineer. [Wikipedia](https://de.wikipedia.org/wiki/Claude_Shannon).\n",
    "\n",
    "![](assets/shannon.jpg)\n",
    "\n",
    "Shannon founded **Information Theory** in 1948\n",
    "- introduced the bit\n",
    "- originally developed in the context of sending communication via radio\n",
    "- communicating information with 0 or 1 started a revolution in communication\n",
    "\n",
    "### The channel communication context\n",
    "\n",
    "Three elements\n",
    "- data source\n",
    "- noisy communication channel\n",
    "- receiver\n",
    "\n",
    "Receiver understanding the data generated by the source from the signal\n",
    "- the fundamental problem of communication\n",
    "\n",
    "Entropy provides a limit on the shortest possible length of a lossless encoding\n",
    "- encoding of data into signal\n",
    "- this is analogous with the thermodynamic limit on conversion of heat to power\n",
    "\n",
    "### Use of the logarithm\n",
    "\n",
    "Shannon's 1948 paper [A Mathematical Theory of Communication](http://math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf) starts out with a discussion of the logarithm\n",
    "\n",
    "The logarithm transforms **exponential into linear relationships**\n",
    "\n",
    "Practical & intuitive\n",
    "\n",
    "> Parameters of engineering importance such as time, bandwidth, number\n",
    "of relays, etc., tend to vary linearly with the logarithm of the number of possibilities. For example, adding one relay to a group doubles the number of possible states of the relays.\n",
    "\n",
    "Mathematically suitable\n",
    "\n",
    "> Many of the limiting operations are simple in terms of the logarithm but would require clumsy restatement in terms of the number of possibilities.\n",
    "\n",
    "Choice of the logarithm base = determines the unit of information\n",
    "- $\\log_{e}$ = nats\n",
    "- $\\log_{2}$ = **bits**\n",
    "\n",
    "### Bits\n",
    "\n",
    "Discrete random variable\n",
    "\n",
    "Bit = 0 or 1\n",
    "- but not all bits are useful\n",
    "- one bit reduces uncertainty by 2 (encoding independent)\n",
    "\n",
    "### Information content\n",
    "\n",
    "Information content of an event $h(x)$ is given by\n",
    "\n",
    "$$h(x) = - \\log_{2}p(x)$$\n",
    "\n",
    "- base 2 means information is measured in **bits** [0, 1]\n",
    "\n",
    "### Entropy\n",
    "\n",
    "[Entropy - Wikipedia](https://en.wikipedia.org/wiki/Entropy_(information_theory))\n",
    "\n",
    "Entropy $H(x)$ is the average information content of a distribution\n",
    "- how much information you get when sampling from a probability distribution\n",
    "- an expectation\n",
    "- probability of occuring * information content\n",
    "\n",
    "$$H(x) = - \\sum p(x) \\log_{2}p(x)$$\n",
    "\n",
    "$$H(x) = - \\mathbf{E}_{x \\sim P}[\\log_{2} P(x)]$$\n",
    "\n",
    "Entropy can also be written\n",
    "\n",
    "$$ H(x) =  \\mathbf{E}_{x \\sim P} \\cdot \\log_{2} \\frac{1}{P(x)} $$\n",
    "\n",
    "Using the identity $\\log(1/a) = -\\log(a)$\n",
    "\n",
    "Entropy only accounts for information about the probability distribution - not the meaning of the events themselves\n",
    "\n",
    "## Practical - implement entropy from probabilities\n",
    "\n",
    "Let's imagine a probability distribution with three discrete states:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "State = namedtuple('State', ['name', 'prob'])\n",
    "\n",
    "s_bahn = [\n",
    "    State('15 minutes', 0.3), \n",
    "    State('30 minutes', 0.2),\n",
    "    State('60 minutes', 0.5)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to calculate the entropy of the distribution using pure Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3, 0.2, 0.5]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[state.prob for state in s_bahn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4854752972273344"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expect = []\n",
    "for prob in [state.prob for state in s_bahn]:\n",
    "    expect.append(-prob * np.log2(prob))\n",
    "    \n",
    "sum(expect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4854752972273346"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check with\n",
    "entropy_from_probs([state.prob for state in s_bahn], base=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare this with another, more predictable journey home:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_bahn = [\n",
    "    State('15 minutes', 0.5), \n",
    "    State('25 minutes', 0.5)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6931471805599453"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy_from_probs([state.prob for state in u_bahn])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The more predictable journey has lower entropy**\n",
    "\n",
    "## Practical - implement entropy from categories\n",
    "\n",
    "Implement the calculation of entropy from a single column of states\n",
    "- you will need to create a probability distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['B', 'A', 'A', 'B', 'C', 'A', 'B', 'C', 'C', 'A'], dtype='<U1')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states = np.random.choice(('A', 'B', 'C'), size=10000)\n",
    "\n",
    "states[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0984077904013774"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  the answer\n",
    " entropy_from_classes(states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A visual guide to information theory\n",
    "\n",
    "The images & content below are taken from the excellent [colah's blog](https://colah.github.io) - specifically the post **[Visual Information Theory](https://colah.github.io/posts/2015-09-Visual-Information/)**\n",
    "\n",
    "### Distributions\n",
    "\n",
    "Imagine you have two probability distributions \n",
    "- one over the weather\n",
    "- one over what clothes you wear\n",
    "\n",
    "<img src=\"assets/info1.png\" alt=\"\" width=\"200\"/>\n",
    "\n",
    "If our distributions are **independent**, we can visualize them as follows\n",
    "\n",
    "<img src=\"assets/info2.png\" alt=\"\" width=\"300\"/>\n",
    "\n",
    "This independence means that we can calculate the probability of two events occurring through simple multiplication - there is no interaction\n",
    "\n",
    "We can calculate the **joint** probability of two events by multiplying them together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46499999999999997"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather = {'rain': 0.25, 'sun': 0.75}\n",
    "clothes = {'coat': 0.38, 'shirt': 0.62}\n",
    "\n",
    "weather['sun'] * clothes['shirt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When our events do interact, we need to reassign the probability mass and speak in terms of **conditional** probabilities\n",
    "- these conditional probabilities are extra information\n",
    "\n",
    "We can now see that our lines do not intersect both distributions at the same place  - they cut at different points\n",
    "- incorporating these conditional probabilities requires more information\n",
    "- $P(coat | rain) = 0.75$\n",
    "- this reassigns the probability mass\n",
    "\n",
    "<img src=\"assets/info3.png\" alt=\"\" width=\"350\"/>\n",
    "\n",
    "These conditional probabilities can be factored out\n",
    "\n",
    "$$P(rain, coat) = P(rain) \\cdot P(coat | rain)$$\n",
    "\n",
    "To calculate the probability that it is raining and we are wearing a coat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1875"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.25 * 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can also do the opposite\n",
    "- this feels wierd because it goes in the opposite direction of causality\n",
    "- but it still works\n",
    "\n",
    "We need some additional information \n",
    "- $P(rain|coat) = 0.5$ \n",
    "\n",
    "<img src=\"assets/info4.png\" alt=\"\" width=\"300\"/>\n",
    "\n",
    "$$P(coat, rain) = P(coat) \\cdot P(rain | coat)$$\n",
    "\n",
    "The probabilities over clothes are now **marginal** \n",
    "- the probability of wearing clothes without considering the weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.38 * 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "\n",
    "Imagine the problem of communication using bits with a vocabulary of four words\n",
    "\n",
    "<img src=\"assets/info5.png\" alt=\"\" width=\"500\"/>\n",
    "\n",
    "The problem is that communication is expensive \n",
    "- the cost varies with the number of bits\n",
    "\n",
    "The opportunity is that some words are more common than others\n",
    "- we can create an encoding that takes advantage of this\n",
    "\n",
    "<img src=\"assets/info6.png\" alt=\"\" width=\"400\"/>\n",
    "\n",
    "We can visualize our old and new encodings\n",
    "\n",
    "<img src=\"assets/info7.png\" alt=\"\" width=\"700\"/>\n",
    "\n",
    "This average message length is the entropy\n",
    "\n",
    "$$ H(x) = - \\mathbf{E}_{x \\sim P}[\\log P(x)] $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.75"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy_from_probs([1/2, 1/4, 1/8, 1/8], base=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Entropy\n",
    "\n",
    "[Cross-Entropy - Wikipedia](https://en.wikipedia.org/wiki/Cross_entropy)\n",
    "\n",
    "We now have two different distributions over words\n",
    "\n",
    "<img src=\"assets/info8.png\" alt=\"\" width=\"400\"/>\n",
    "\n",
    "We can calculate the average length of a communication when \n",
    "- using the code optimized for one distribution \n",
    "- to communicate events for a second\n",
    "\n",
    "This is the **cross-entropy**\n",
    "\n",
    "$$ H(P,Q) = - \\mathbf{E}_{x \\sim P}\\log Q(x)$$\n",
    "\n",
    "$$ H_{p}(q) = - \\sum\\limits_{x} q(x) \\cdot \\log_{2} p(x) $$\n",
    "\n",
    "\n",
    "Average number of bits needed to \n",
    "- identify a sample \n",
    "- with a coding scheme optimized for an estimated distribution $q$ \n",
    "- rather than the true distribution $p$\n",
    "\n",
    "Minimizing the KLD is the same as minimizing the cross entropy\n",
    "\n",
    "Cross_entropy = entropy + KLD\n",
    "\n",
    "$$H(P,Q) = H(P) + D_{KL}(P||Q)$$\n",
    "\n",
    "If true = predicted -> cross entropy = entropy\n",
    "\n",
    "## Practical - cross entropy\n",
    "\n",
    "Write a function to calculate the cross-entropy between the two distributions above \n",
    "- both $H_{p}(q)$ and $H_{q}(p)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.374999999999999"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# answers\n",
    "\n",
    "cross_entropy_from_probs(\n",
    "    [1/2, 1/4, 1/8, 1/8], [1/8, 1/2, 1/4, 1/8]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.249999999999999"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_from_probs(\n",
    "    [1/8, 1/2, 1/4, 1/8], [1/2, 1/4, 1/8, 1/8]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross entropy tells us \n",
    "- how much longer our messages will be if we use an inappropriate code\n",
    "- how different our distributions are\n",
    "\n",
    "## Kullback–Leibler divergence (KLD)\n",
    "\n",
    "Cross entropy gives us a difference in average information content for two encodings\n",
    "- this difference is known as the **Kullback–Leibler divergence**\n",
    "\n",
    "$$D_{KL}(P||Q) = \\mathbf{E}_{x \\sim P}[\\log P(x) - \\log Q(x)] $$\n",
    "\n",
    "The extra amount of information needed to send a message containing symbols from $P$ when using a code designed to minimize the length of messages for $Q$\n",
    "\n",
    "## Practical\n",
    "\n",
    "Implement a function to calculate the KLD between our two distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.625"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def kld(q, p):\n",
    "    out = []\n",
    "    for Q, P in zip(q, p):\n",
    "        out.append(P * (np.log2(P) - np.log2(Q)))\n",
    "    return sum(out)\n",
    "        \n",
    "p, q = [1/8, 1/2, 1/4, 1/8], [1/2, 1/4, 1/8, 1/8]\n",
    "kld(p, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6249999999999991"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_from_probs(q, p) - entropy_from_probs(p, base=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the exercise above you can see that \n",
    "\n",
    "$$H_{p}(q) \\neq H_{q}(p)$$\n",
    "\n",
    "This means that **cross-entropy / KLD is not symmetric**.  Why is this important?\n",
    "- many people refer to the KLD as a distance\n",
    "- this is technically not correct\n",
    "\n",
    "Cross entropy & KLD are very useful in machine learning\n",
    "- cross entropy loss - minimize the difference between a prediction & true sample\n",
    "- reinforcement policy gradient methods use a constraint or penalty on KLD to stop catastrophic policy updates\n",
    "\n",
    "Suppose we have two distributions $P(x)$ and $Q(x)$\n",
    "- example = a parameterized neural net & the function we are trying to learn\n",
    "\n",
    "We can measure the difference between the two using the **Kullback-Leiber divergence (KLD)** (it is not a true distance measure - not symmetric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimizing cross entropy\n",
    "\n",
    "A common operation in modern ML is minimizing cross entropy between a one hot encoded label & a softmax.\n",
    "\n",
    "The **softmax** is a less aggressive form of one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    exps = np.exp(X)\n",
    "    return exps / np.sum(exps)\n",
    "\n",
    "def normalize(x):\n",
    "    return (x - np.min(x)) / (np.max(x) - np.min(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.05593051, -0.14746982, -0.18588279,  0.97584119, -0.22097437])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_map = np.random.normal(size=5)\n",
    "feature_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZoAAAEWCAYAAABfdFHAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAF2pJREFUeJzt3XtwlfWdx/HPNwkBXGLkEoEm4AERYrgEIRNvtbW1zmhhcFqWWZDdla0t9cLolm0VO9N1bC31MnWrYr1UVNjdEbboHxEcWqdFofVGgmBz4ya3UCxBuZiCgZDv/pGDmw2pOcD5nYec837NnPE85zx5zucnA588T375PebuAgAglKyoAwAA0htFAwAIiqIBAARF0QAAgsqJOgAAhFZVVXV+Tk7Os5LGiG+wQ2mVVN3S0vLtiRMn7m3/BkUDIO3l5OQ8O2jQoIsLCgr2Z2VlMdU2gNbWVmtsbCz58MMPn5U0pf17NDuATDCmoKDgECUTTlZWlhcUFBxU21nj/38vgjwAkGpZlEx48f/HJ/UKRQMAGaC8vHzU6tWrz5GkL3/5yyP27duXfSbHW758ed5XvvKVEYnsy89oAGSc2LwVE5N5vO0PTKpK5vE6OnbsmHr06JG0473xxhtbknawBHBGAwApsHHjxtzhw4ePnj59+gUjRowYfeWVV17U1NRkb775Zu/S0tLikSNHllx77bUXNjY2ZkttZyDf+ta3howZM+bi+++/f+DUqVNjM2fOHFpaWlpcVFQ0dvny5XnTpk2LDR8+fPTUqVNjJz5n5syZQ8eMGXPxiBEjRn/ve9/7QmdZCgsLx+7ZsyfnoYceKiguLi4pLi4uKSwsHHvppZeOlKSXX3753PHjxxeXlJRcfP311w8/ePBgliQtW7bs3GHDho0uKSm5eNmyZeclOnaKBgBSZOfOnb3uuOOOvVu2bKnJz88/vnjx4r6zZs0aNn/+/IZNmzbVjh49+sjdd9/9WTkcPXrUqqur6+67776/SNLBgwdz3nvvvfoHHnhg1/Tp00f84Ac/+MvmzZtr6uvre7/55pu9JemRRx7ZXV1dXVdfX1/zxz/+Me+dd97p/bfy3HXXXY319fW1GzZsqBs0aNDRO++88y979uzJmT9//uDVq1dvqq2trZswYcLhn/zkJwMPHz5sc+bMiVVUVGyprq6u27t3b8KnWBQNAKRIYWFh8xVXXHFEki655JLDW7du7fnJJ59kT5o0qUmSvvOd73z09ttv9zmx/4wZMz5u//WTJk06kJWVpQkTJhzu37//sfLy8iPZ2dkaOXLkka1bt/aUpEWLFvUrKSm5uKSkpGTz5s29NmzY0KurXDfffPOQL33pS5/ceOONB19//fW/27p1a6/y8vLi4uLikiVLlvTfuXNn7vr163sVFRU1jx07tjkrK0szZ878KNFx8zMaAEiR3Nzcz2a+ZWdn+4EDBz73rCAvL6+1/XavXr08/rX/71hZWVlqaWmx+vr63AULFgysqqqqKygoOD516tTYp59++rknFI899lj/hoaG3EWLFu2UJHfXF7/4xUOvvPLKtvb7nThjOh2c0QBARPLz84+fe+65x1euXNlHkhYuXNj/8ssvbzrd4+3fvz+7d+/erf369Tu+a9eunNdffz3/8/Zfs2bNOY8//vigX//619uys9smoV199dV/rays7FNdXd1Tkg4dOpT1/vvv9xw/fvynu3fvzq2pqekpSUuWLOmXaC7OaAAgQs8///y2W2+99YI77rgja+jQoc0vvvji9tM91uWXX35kzJgxhy+88MIxgwcPPjpx4sTPLa1HH330/IMHD2ZfddVVoySptLT0r0uXLt3x9NNPb58+ffrwo0ePmiTde++9u8eNG9f8+OOP75g8efKI3r17t1566aVNTU1NCU2RNm58BiDdbdiwYXtpaem+qHNkgg0bNgwoLS2NtX+NS2cAgKAoGgBAUBQNACAoigYAEBRFAwAIiqIBAARF0QDAWWTlypV9RowYMbq4uLhk3bp1vZ566qmEfzHybMUvbALIOGfzbQIWL17cb+7cuXtuu+22j5cvX563dOnSfrfccsvHXX/l2YuiAYDADh06lDVlypThe/bsyW1tbbW77rrrz+eff37LvHnzhhw/flylpaWHFy9evOPJJ5/sv2LFin5vvPFG/sqVK/N37NjR84MPPuhVXFxcMmPGjH19+/Y9XlFRcd7hw4ezduzY0ev222//8OjRo1lLly7tn5ub2/rb3/5288CBA4///Oc/H/D8888XHDt2zGKxWPOyZcu25eXltV5zzTUXfuMb3zgwZ86cjx5++OEBa9asyauoqNjW9QjODJfOACCwl19++dxBgwYd27hxY+3mzZtrvvnNbx767ne/O2zp0qVbN23aVNvS0qKHH364YO7cufu+9rWvHbj//vsbKioqtv30pz/dXVZW1lRfX19777337pWkTZs29V6xYsXWtWvX1v3sZz8rPOecc1rr6upqy8rK/vr000/3l6SZM2fur66urtu4cWPtqFGjjjz22GMDJOmFF17Y8dBDDw1euXJlnyeeeGLQr371q52pGD9FAwCBTZgw4ciaNWvOvfXWWwtXrlzZZ9OmTblFRUXN48aNa5akWbNmffSHP/whL5FjXXHFFZ/07du39Qtf+EJLnz59jk+bNu2AJI0dO/bw9u3be0pSVVVV74kTJ44aOXJkyUsvvdS/pqamlyQNGTKk5Yc//OGfJ0+ePGr+/Pm7Bg4ceDzUmNujaAAgsHHjxjWvW7euduzYsUd+9KMfFZ7K3Sk76nh7gBO3DjhxqwBJmj179rAFCxbs3LRpU+3dd9/95+bm5s/+rf/Tn/7UOz8/v2X37t3Juzd0FygaAAhs+/btPfLy8lpvu+22j+fOnfvhu+++22f37t25J5biX7x4cf+rrrrqk45fl5+ffzzRFZLbO3z4cNbQoUOPNTc3W/vl/FetWnXO7373u/yqqqraBQsWDKqvr889s5ElhskAABBYVVVV73vuuacoKytLOTk5/stf/nLH/v37s6dNm3bhickA3//+9xs7fl38Dpo+atSokhtvvHFf3759E7rUNW/evD+Xl5df3K9fv5YJEyY0NTU1ZR85csRuueWW2MKFC7fHYrFj8+fP33XTTTfF3nrrrU1ZWWHPObhNAIC0x20CUofbBAAAUo6iAQAERdEAAIKKbDLAgAEDPBaLRfXxADLIgw8+qJqamgvMLOooZ6y5ubnlkksu2RB1js60traapNaOr3dZNGb2nKTJkva6+5hO3jdJj0r6uqTDkma5+7qujhuLxVRZWZlAdAA4M9u2bVNeXp769++v7l421dXVR6PO0JnW1lZrbGzMl1Td8b1EzmhekLRA0uK/8f71ki6KPy6V9GT8vwBwVigqKlJDQ4MaG0+aQdztfPjhhznHjx8fEHWOTrRKqm5pafl2xze6LBp3X21msc/Z5QZJi71tnvTbZnaemQ129z2nmxYAkqlHjx4aNmxY1DGSoqSk5E/uXhZ1jlORjMkAhZJ2tdtuiL8GAEBqJwOY2WxJsyVp6NChp32c2LwVyYoUue0PTIo6AgAElYwzmt2ShrTbLoq/dhJ3f8bdy9y9rKCgIAkfDQA42yWjaCok/bO1uUzSQX4+AwA4IZHpzS9KulrSADNrkHSvpB6S5O5PSXpVbVObt6htevO/hAoLAOh+Epl1NqOL913S7UlLBABIKyxBAwAIivvRAN1Iusy4ZLZlZuGMBgAQFEUDAAiKogEABEXRAACComgAAEFRNACAoCgaAEBQFA0AICiKBgAQFEUDAAiKogEABEXRAACComgAAEFRNACAoCgaAEBQFA0AICiKBgAQFEUDAAiKogEABEXRAACComgAAEFRNACAoCgaAEBQFA0AICiKBgAQFEUDAAiKogEABEXRAACComgAAEFRNACAoCgaAEBQFA0AICiKBgAQFEUDAAiKogEABEXRAACComgAAEElVDRmdp2ZbTSzLWY2r5P3Z5lZo5mtjz++nfyoAIDuKKerHcwsW9ITkq6V1CBprZlVuHtth12XuvucABkBAN1YImc05ZK2uPsH7n5U0hJJN4SNBQBIF4kUTaGkXe22G+KvdTTVzN43s2VmNqSzA5nZbDOrNLPKxsbG04gLAOhukjUZ4BVJMXcfJ+k1SYs628ndn3H3MncvKygoSNJHAwDOZokUzW5J7c9QiuKvfcbdP3L35vjms5ImJiceAKC7S6Ro1kq6yMyGmVmupOmSKtrvYGaD221OkVSXvIgAgO6sy1ln7t5iZnMk/UZStqTn3L3GzH4sqdLdKyTdYWZTJLVI+ljSrICZAQDdSJdFI0nu/qqkVzu89u/tnt8j6Z7kRgMApANWBgAABEXRAACComgAAEFRNACAoCgaAEBQFA0AICiKBgAQFEUDAAiKogEABEXRAACComgAAEFRNACAoCgaAEBQFA0AICiKBgAQFEUDAAiKogEABEXRAACComgAAEFRNACAoCgaAEBQFA0AICiKBgAQFEUDAAiKogEABEXRAACComgAAEFRNACAoCgaAEBQFA0AICiKBgAQFEUDAAiKogEABEXRAACComgAAEFRNACAoHKiDoBTF5u3IuoISbH9gUlRRwCQApzRAACCSqhozOw6M9toZlvMbF4n7/c0s6Xx998xs1iygwIAuqcuL52ZWbakJyRdK6lB0lozq3D32na73Sxpv7uPMLPpkh6U9A8hAiOzpctlQ4lLh8gciZzRlEva4u4fuPtRSUsk3dBhnxskLYo/XybpGjOz5MUEAHRXiUwGKJS0q912g6RL/9Y+7t5iZgcl9Ze0r/1OZjZb0uz4ZpOZbTyd0Ck0QB3GkGz2YMijnxHGHlgmjz+Tx54EF0Qd4FSldNaZuz8j6ZlUfuaZMLNKdy+LOkcUGHtmjl3K7PFn8thDSuTS2W5JQ9ptF8Vf63QfM8uRlC/po2QEBAB0b4kUzVpJF5nZMDPLlTRdUkWHfSok3RR//veSfu/unryYAIDuqstLZ/GfucyR9BtJ2ZKec/caM/uxpEp3r5C0UNJ/mtkWSR+rrYzSQbe5zBcAY89cmTz+TB57MMaJBwAgJFYGAAAERdEAAIKiaDrR1ZI76czMnjOzvWZWHXWWVDOzIWa2ysxqzazGzO6MOlOqmFkvM3vXzDbEx35f1JmiYGbZZvaemS2POks6oWg6aLfkzvWSSiTNMLOSaFOl1AuSros6RERaJP2bu5dIukzS7Rn0Z98s6avuXippvKTrzOyyiDNF4U5JdVGHSDcUzckSWXInbbn7arXNHMw47r7H3dfFn3+itn9wCqNNlRrepim+2SP+yKiZQmZWJGmSpGejzpJuKJqTdbbkTkb8Y4P/E1+B/BJJ70SbJHXil43WS9or6TV3z5ixx/1C0l2SWqMOkm4oGqADM+sj6SVJ/+ruh6LOkyruftzdx6tt9Y9yMxsTdaZUMbPJkva6e1XUWdIRRXOyRJbcQZoysx5qK5n/dveXo84TBXc/IGmVMutndVdKmmJm29V2ufyrZvZf0UZKHxTNyRJZcgdpKH5ri4WS6tz9kajzpJKZFZjZefHnvdV2/6n6aFOljrvf4+5F7h5T29/537v7P0YcK21QNB24e4ukE0vu1En6H3eviTZV6pjZi5LekjTKzBrM7OaoM6XQlZL+SW3fza6PP74edagUGSxplZm9r7Zvtl5zd6b4IilYggYAEBRnNACAoCgaAEBQFA0AIKiU3sq5vQEDBngsFovq4wGgW6qqqtrn7gVR5zgVXRaNmT0n6cQvM530C1zxKaGPSvq6pMOSZp1YxuPzxGIxVVZWnnpiAMhgZrYj6gynKpFLZy/o839x63pJF8UfsyU9eeaxAADposuiSWCRxRskLY4vyve2pPPMbHCyAgIAurdkTAZgEUoAwN+U0llnZjbbzCrNrLKxsTGVHw0AiEgyiibhRSjd/Rl3L3P3soKCbjVpAgBwmpJRNBWS/tnaXCbpoLvvScJxAQBpIJHpzS9KulrSADNrkHSv2u6+J3d/StKrapvavEVt05v/JVRYAED302XRuPuMLt53SbcnLREAIK2wBA0AICiKBgAQFEUDAAiKogEABEXRAACComgAAEFRNACAoCgaAEBQFA0AICiKBgAQFEUDAAiKogEABEXRAACComgAAEFRNACAoCgaAEBQFA0AICiKBgAQFEUDAAiKogEABEXRAACComgAAEFRNACAoCgaAEBQFA0AICiKBgAQFEUDAAiKogEABEXRAACComgAAEFRNACAoCgaAEBQFA0AICiKBgAQFEUDAAiKogEABEXRAACComgAAEElVDRmdp2ZbTSzLWY2r5P3Z5lZo5mtjz++nfyoAIDuKKerHcwsW9ITkq6V1CBprZlVuHtth12XuvucABkBAN1YImc05ZK2uPsH7n5U0hJJN4SNBQBIF4kUTaGkXe22G+KvdTTVzN43s2VmNiQp6QAA3V6yJgO8Iinm7uMkvSZpUWc7mdlsM6s0s8rGxsYkfTQA4GyWSNHsltT+DKUo/tpn3P0jd2+Obz4raWJnB3L3Z9y9zN3LCgoKTicvAKCbSaRo1kq6yMyGmVmupOmSKtrvYGaD221OkVSXvIgAgO6sy1ln7t5iZnMk/UZStqTn3L3GzH4sqdLdKyTdYWZTJLVI+ljSrICZAQDdiLl7JB9cVlbmlZWVkXw2AHRXZlbl7mVR5zgVrAwAAAiKogEABEXRAACComgAAEFRNACAoCgaAEBQFA0AICiKBgAQVJcrA5yNYvNWRB0habY/MCnqCAAQFGc0AICgKBoAQFDd8tIZkKnS5bIxl4wzC2c0AICgKBoAQFAUDQAgKIoGABAURQMACIqiAQAERdEAAIKiaAAAQVE0AICgKBoAQFAUDQAgKIoGABAURQMACIqiAQAERdEAAIKiaAAAQVE0AICgKBoAQFAUDQAgKIoGABBUTtQBcOpi81ZEHSEptj8w6ZS/Jl3GLp3e+DMZf/bdF2c0AICgKBoAQFAUDQAgKIoGABAURQMACCqhojGz68xso5ltMbN5nbzf08yWxt9/x8xiyQ4KAOieuiwaM8uW9ISk6yWVSJphZiUddrtZ0n53HyHpPyQ9mOygAIDuKZEzmnJJW9z9A3c/KmmJpBs67HODpEXx58skXWNmlryYAIDuKpGiKZS0q912Q/y1Tvdx9xZJByX1T0ZAAED3ltKVAcxstqTZ8c0mM9uYys8/DQMk7Qv5AXb2XmRk7IFl8vgzeezSGY//giTFSJlEima3pCHttovir3W2T4OZ5UjKl/RRxwO5+zOSnjm9qKlnZpXuXhZ1jigw9swcu5TZ48/ksYeUyKWztZIuMrNhZpYrabqkig77VEi6Kf787yX93t09eTEBAN1Vl2c07t5iZnMk/UZStqTn3L3GzH4sqdLdKyQtlPSfZrZF0sdqKyMAABL7GY27vyrp1Q6v/Xu7559KmpbcaGeFbnOZLwDGnrkyefyZPPZgjCtcAICQWIIGABAURdOJrpbcSWdm9pyZ7TWz6qizpJqZDTGzVWZWa2Y1ZnZn1JlSxcx6mdm7ZrYhPvb7os4UBTPLNrP3zGx51FnSCUXTQYJL7qSzFyRdF3WIiLRI+jd3L5F0maTbM+jPvlnSV929VNJ4SdeZ2WURZ4rCnZLqog6RbiiakyWy5E7acvfVaps5mHHcfY+7r4s//0Rt/+B0XAUjLXmbpvhmj/gjo36Aa2ZFkiZJejbqLOmGojlZIkvuIM3FVyC/RNI70SZJnfhlo/WS9kp6zd0zZuxxv5B0l6TWqIOkG4oG6MDM+kh6SdK/uvuhqPOkirsfd/fxalv9o9zMxkSdKVXMbLKkve5eFXWWdETRnCyRJXeQpsysh9pK5r/d/eWo80TB3Q9IWqXM+lndlZKmmNl2tV0u/6qZ/Ve0kdIHRXOyRJbcQRqK39pioaQ6d38k6jypZGYFZnZe/HlvSddKqo82Veq4+z3uXuTuMbX9nf+9u/9jxLHSBkXTQfw2ByeW3KmT9D/uXhNtqtQxsxclvSVplJk1mNnNUWdKoSsl/ZPavptdH398PepQKTJY0ioze19t32y95u5M8UVSsDIAACAozmgAAEFRNACAoCgaAEBQFA0AICiKBgAQFEUDAAiKogEABEXRAACC+l/JHdMUnJWq8QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, a = plt.subplots(nrows=2, sharey=True)\n",
    "a[0].bar(np.arange(len(feature_map)), normalize(feature_map), label='normalized')\n",
    "a[1].bar(np.arange(len(feature_map)), softmax(feature_map), label='softmax')\n",
    "_ = f.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 0., 0.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = np.zeros(feature_map.shape[0])\n",
    "label[1] = 1\n",
    "\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47.43440854369629"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_from_probs(softmax(feature_map), label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.35829903, 0.10755126, 0.10349824, 0.33072215, 0.09992931])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(feature_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
