# Sequences


## Key takeaways

- key problem in sequences is long sequence lengths
- LSTM helps with long term memory with two places to remember (cell & hidden state)
- seq2seq allowed variable length sequences
- transformer removes dependency on location


## Course notes

[keras.ipynb](https://github.com/ADGEfficiency/teaching-monolith/blob/master/sequences/keras.ipynb)
- sequential API
- MNIST
- functional API

[recurrent.ipynb](https://github.com/ADGEfficiency/teaching-monolith/blob/master/sequences/recurrent.ipynb)
- recurrent neural network - motivations & mechanics
- character level language modelling

[lstm.ipynb](https://github.com/ADGEfficiency/teaching-monolith/blob/master/sequences/lstm.ipynb)
- LSTMs - motivations & mechanics
- GRUs
- sin wave prediction

[seq2seq.ipynb](https://github.com/ADGEfficiency/teaching-monolith/blob/master/sequences/seq2seq.ipynb)
- introduction & detail on the seq2seq architecture

[attention.ipynb](https://github.com/ADGEfficiency/teaching-monolith/blob/master/sequences/attention.ipynb)
- intitution behind attention
- Additive Attention
- Dot-Product Attention
- the Transformer


## Further reading

Attention? Attention! - Lilian Wang - [text](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html) 

[Visualizing memorization in RNNs](https://distill.pub/2019/memorization-in-rnns/)

[When Recurrent Models Don't Need to be Recurrent](https://bair.berkeley.edu/blog/2018/08/06/recurrent/)

Recurrent Neural Networks (RNN) and Long Short-Term Memory (LSTM) - Brandon Rohrer - [youtube](https://youtu.be/WCUNPb-5EYI)
