{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Estimation\n",
    "\n",
    "See Chapter 8 of [Think Stats 2nd Edition](https://greenteapress.com/wp/think-stats-2e/).\n",
    "\n",
    "Error != mistake\n",
    "\n",
    "Sources of error in estimation\n",
    "- sampling error = arises from using statistics of a subset of a larger population - usually impossible to measure exactly\n",
    "- sampling bias = samples having different probabilities than others\n",
    "- measurement error = difference between measurement & true value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from common import load_iris\n",
    "\n",
    "features, target = load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical\n",
    "\n",
    "Explore the Iris dataset - what are possible sources of error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating sample mean & variance\n",
    "\n",
    "Let's imagine we have a process that generates some samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = np.random.choice(features.loc[:, 'sepal length (cm)'], size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create some estimators (aka models) by parameterizing Gaussians.  \n",
    "\n",
    "The question is - what to use for the mean ($\\mu$) and variance ($\\sigma^2)$?  \n",
    "\n",
    "We can use a simple **sample variance**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(samples)\n",
    "\n",
    "sum([(sample - mean)**2 for sample in samples]) / len(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with the sample variance above is that it is biased, and will be too small for low numbers of samples.  \n",
    "\n",
    "We can get an **unbiased sample variance** by removing a degree of freedom:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([(sample - mean)**2 for sample in samples]) / (len(samples) - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can estimate the **central tendency** of the distribution, using either the mean or median.  Lets use the mean.\n",
    "\n",
    "What should we do with our estimated statistics?  Let's parameterize a Gaussian & sample from it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = np.mean(samples)\n",
    "sigma = np.var(samples, ddof=1)\n",
    "\n",
    "n = 10\n",
    "\n",
    "estimate = np.mean([random.gauss(mu, sigma) for _ in range(n)])\n",
    "\n",
    "estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compare this with the actual sample mean via the **root mean squared error (RMSE)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt((estimate - mu)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard error gives us the expected error for this specific distribution if:\n",
    "- we use the median as a statistic\n",
    "- with a sample size of 10\n",
    "\n",
    "## Practical\n",
    "\n",
    "The purpose of this exercise is to compare two methods of approximating the central tendency.\n",
    "\n",
    "Above we used the mean.  Now do this using the median as the central tendency statistic, and run the error estimate `m` times (we only ran it once above).\n",
    "\n",
    "Plot the standard error for each sample, along with a running average.  After the experiment is over, plot a CDF of the estimates.\n",
    "\n",
    "You will need functions from `common.py` to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating sampling error\n",
    "\n",
    "Small number of samples -> **sampling error**\n",
    "\n",
    "We can can estimate the sampling error through simulation\n",
    "- we don't know the true statistics\n",
    "- lets instead use estimates from our small number of samples\n",
    "\n",
    "The question we are asking is\n",
    "- if the true stats were the same as the population stats\n",
    "- and we ran this experiment many times\n",
    "- how much would our estimated mean vary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10\n",
    "\n",
    "samples = np.random.choice(features.loc[:, 'sepal length (cm)'], size=num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = np.mean(samples)\n",
    "var = np.var(samples)\n",
    "\n",
    "num_simulations = 500\n",
    "num_samp = 500\n",
    "means = []\n",
    "for idx in range(num_simulations):\n",
    "    samp = np.random.normal(mu, np.sqrt(var), 50)\n",
    "    means.append(np.mean(samp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cdf(samples):\n",
    "    #  duplicate ot function in distributions.ipynb\n",
    "    samples = sorted(samples)\n",
    "    return [(percentile_rank(s, samples), s) for s in sorted(samples)]\n",
    "\n",
    "def percentile_rank(value, samples):\n",
    "    count = 0\n",
    "    return sum([count + 1 for s in samples if s <= value]) / len(samples)\n",
    "\n",
    "make_cdf(means[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 90th percentile is then:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, x = zip(*make_cdf(means))\n",
    "\n",
    "y = np.array(y)\n",
    "x = np.array(x)\n",
    "\n",
    "start = x[y == 0.1]\n",
    "end = x[y == 0.95]\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "ax.plot(x, y)\n",
    "ax.axvline(start, color='red')\n",
    "ax.axvline(end, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative to the 90% confidence interval is the standard error\n",
    "- the expected error\n",
    "- describes variability in the estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(np.mean((x - mu)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudoreplication\n",
    "\n",
    "[Chapter 3 of Statistics Done Wrong - Alex Reinhart](https://www.statisticsdonewrong.com/)\n",
    "\n",
    "Counting the same sample multiple times\n",
    "- dependence is the problem here (non independent sampling)\n",
    "\n",
    "Additional measurements that depend on previous data don't prove your results generalize \n",
    "- they only increase certainty about specific sample studied\n",
    "\n",
    "Eliminate hidden sources of correlation between variables\n",
    "- meausure 1,000 paitients rather than 100 paitents 10 times\n",
    "- 100's neurons in two animals\n",
    "- comparing growth rates of different crops in different fields\n",
    "\n",
    "Solutions\n",
    "- average dependent data points \n",
    "- analyze each point separately - don't combine, analyze only a subset (ie day 5)\n",
    "\n",
    "Doing PCA on different batches of results\n",
    "- if the number of the batch is important, then you have problems with the distribution for each time\n",
    "\n",
    "## Peer based learning\n",
    "\n",
    "[Chapter 12 of Statistics Done Wrong - Alex Reinhart](https://www.statisticsdonewrong.com/)\n",
    "\n",
    "Discuss (& take positions) on the following:\n",
    "\n",
    "As the sample size increases, what happens to the\n",
    "- standard error\n",
    "- standard deviation\n",
    "\n",
    "What sources of error are we not accounting for with these two statistics?\n",
    "\n",
    "Which of the three sources of error (sampling error, sampling bias or measurement) is pseudoreplication?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
