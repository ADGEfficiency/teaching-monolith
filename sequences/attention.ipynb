{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention\n",
    "\n",
    "## Context\n",
    "\n",
    "Attention is one of the most important recent developments in deep learning\n",
    "- 2015 - attention introduced (Bahdanau et. al)\n",
    "- 2017 - the transformer - an entire network architecture (Vaswani et al.)\n",
    "\n",
    "The transformer has been crucial to the revolution in NLP \n",
    "- transformer based language models such as BERT, GPT3 are the biggest story in ML/AI recently \n",
    "\n",
    "![](assets/so-hot.jpg)\n",
    "\n",
    "Applications (any sequence problem)\n",
    "- machine translation (text or audio)\n",
    "- question answering\n",
    "- parsing sentences into grammar trees\n",
    "- generating solutions to symbolic math problems\n",
    "- time series\n",
    "\n",
    "Xu et. al (2015) Show, Attend and Tell: Neural Image Caption Generation with Visual Attention\n",
    "- generating captions for images\n",
    "\n",
    "Possible to use with LSTMS, or without\n",
    "- without = self attention = the Transformer\n",
    "\n",
    "Can be used with images or text\n",
    "\n",
    "\n",
    "## What & why of attention\n",
    "\n",
    "**Attention removes any restrictions based on distance** \n",
    "- deal with the entire sequence at once\n",
    "- (learnable) shortcuts between the input & output sequence\n",
    "- no need for recurrence - no backprop through time -> faster to train than RNN\n",
    "\n",
    "The intuition \n",
    "- pay attention to different parts of a sentence (skim reading)\n",
    "- pay attention to different parts of an image \n",
    "- pay attention to parts of the sequence are relevant\n",
    "\n",
    "We can think about attention as search\n",
    "- searching for something, paying attention to something\n",
    "- searching learned embeddings for words that are similar \n",
    "\n",
    "Attention is modelled using importance weights\n",
    "- paying attention is about multiplying these weights by something else (i.e. a word vector)\n",
    "\n",
    "\n",
    "## Types of attention\n",
    "\n",
    "Attention by location\n",
    "- look at the position in the sequence only\n",
    "- weighted distributions of different offsets\n",
    "\n",
    "Attention by content (min 42)\n",
    "- associative content\n",
    "- key vector $a$, compared to all glimpses $g$, using similarity function $S$\n",
    "- output a vector (key) that represents (green stones), then do cosine similarity \n",
    "\n",
    "Content based addressing\n",
    "- attention based on vector similarity\n",
    "- cosine similarity -> softmax\n",
    "\n",
    "## Attention + seq2seq\n",
    "\n",
    "In seq2seq we build a single context vector\n",
    "- from the encoders last hidden state - acts like a sentence embedding\n",
    "- all infomation from the encoder flows through the fixed length context vector\n",
    "\n",
    "With attention, we create shortcut between the entire input sequence and the context vector\n",
    "- these shortcuts are weighted\n",
    "- weights = the strength of attention between the input & context\n",
    "\n",
    "There are many attention mechanisms\n",
    "- the first was introduced in 2015 (Bahdanau et. al (2015) Neural Machine Translation by Jointly Learning to Align and Translate - [arxiv](https://arxiv.org/abs/1409.0473))\n",
    "- using attention with RNN encoder-decoder\n",
    "- bidirectional RNN to produce the encoder hidden state (fwd & bwd hidden states concatenated)\n",
    "- all of these hidden states are used to generate the context vector\n",
    "\n",
    "<img src=\"assets/enc-dec-attention.png\" width=\"60%\" />\n",
    "\n",
    "Below we will take a look at how the additive attention mechanism works - ([docs for Tensorflow AdditiveAttention layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/AdditiveAttention))\n",
    "\n",
    "First generate some data (notice the shape (batch, seq len, feature dim)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#  shape = (batch, time, features)\n",
    "qry = np.random.normal(size=128).reshape(4, 8, -1).astype(np.float32)\n",
    "val = np.random.normal(size=128).reshape(4, 8, -1).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the built in TF AdditiveAttention layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow -q\n",
    "import tensorflow as tf\n",
    "\n",
    "net = tf.keras.layers.AdditiveAttention(use_scale=False)\n",
    "out = net([qry, val])\n",
    "\n",
    "print(np.sum(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reproduce this using Tensorflow components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = qry.reshape(4, 8, 1, -1)\n",
    "val = val.reshape(4, 1, 8, -1)\n",
    "\n",
    "scores = tf.reduce_sum(tf.tanh(qry + val), axis=-1)\n",
    "dist = tf.nn.softmax(scores)\n",
    "out = tf.matmul(dist, val.reshape(4, 8, -1))\n",
    "\n",
    "print(np.sum(scores), np.var(dist), np.sum(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Implement additive attention without using Tensorflow (using numpy, scipy etc):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The transformer\n",
    "\n",
    "The most important trend in deep learning recently\n",
    "- so important that some think GPT3 is the first true AI \n",
    "\n",
    "My favourite example of GPT2 is [This Word Does Not Exist](https://www.thisworddoesnotexist.com/)\n",
    "\n",
    "More reading\n",
    "- OpenAI's GPT-3 may be the biggest thing since bitcoin - [blog-post](https://maraoz.com/2020/07/18/openai-gpt3/)\n",
    "- [Are we in an AI overhang?](https://www.lesswrong.com/posts/N6vZEnCn6A95Xn39p/are-we-in-an-ai-overhang)\n",
    "\n",
    "So what is so special about the transformer?\n",
    "- introduced in by Vaswani et al. (2017) Attention is All you Need\n",
    "- use of attention to model the sequence\n",
    "- much faster to train than recurrent models (this has allowed the models to scale)\n",
    "\n",
    "## Transformer architecture\n",
    "\n",
    "Below is the entire transformer architecture:\n",
    "\n",
    "- first embed the source & target sequences into the same dimension (512)\n",
    "- location infomation embedded using a sine wave embedding\n",
    "- softmax at the end to predict a word\n",
    "\n",
    "<img src=\"assets/transformer.png\" width=\"50%\" />\n",
    "\n",
    "*From Attention? Attention! - Lilian Wang*\n",
    "\n",
    "Below, we will look at some of the components of this model\n",
    "- dot-product attention\n",
    "- self-attention\n",
    "\n",
    "\n",
    "## The Dot-Product as similarity\n",
    "\n",
    "Above we introduced the idea of attention as similarity\n",
    "- we are searching for words that are similar\n",
    "\n",
    "A common way to measure similarity between vectors is the cosine distance\n",
    "- we can also use matrix multiplication to approximate this measure of similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "data = defaultdict(list)\n",
    "for _ in range(100):\n",
    "    a = np.random.normal(size=128)\n",
    "    b = np.random.normal(size=128)\n",
    "    data['cosine'].append(cosine(a, b))\n",
    "    data['dot'].append(np.dot(a, b))\n",
    "\n",
    "_ = plt.scatter(data['cosine'], data['dot'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key, value, query\n",
    "\n",
    "Key to understanding how these attention layers work\n",
    "\n",
    "The input to the layer is a sequence of (key, value) pairs\n",
    "- the key is used to index the value\n",
    "- for translation, both the key and value are the same\n",
    "\n",
    "Also input to the layer is a query\n",
    "- this is produced by the previous layer (often the decoder)\n",
    "- decoder compresses previous output into a query, and maps this query to the (k, v) to produce output\n",
    "\n",
    "We use attention to find keys that are similar to the query\n",
    "\n",
    "\n",
    "## Dot-product attention\n",
    "\n",
    "Above we have seen how we can use additive attention to create a context vector\n",
    "- after 2015 other attention mechanisms were developed\n",
    "\n",
    "Dot-product attention is important, as it powers the transformer\n",
    "- the dot product is used to measure similarity between the query & all our keys (between the current word and all other words)\n",
    "- this similarity can be converted into a weighted sum (via a softmax)\n",
    "- output created by multiplying the two together\n",
    "\n",
    "[Attention layer in Tensorflow](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Attention)\n",
    "\n",
    "> The meaning of query, value and key depend on the application. In the case of text similarity, for example, query is the sequence embeddings of the first piece of text and value is the sequence embeddings of the second piece of text. key is usually the same tensor as value.\n",
    "\n",
    "Let's model this dot-product attention. First lets generate some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#  shape = (batch, time, features)\n",
    "qry = np.random.normal(size=128).reshape(4, 8, -1).astype(np.float32)\n",
    "val = np.random.normal(size=128).reshape(4, 8, -1).astype(np.float32)\n",
    "key = val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the dot-product attention in Tensorflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = tf.keras.layers.Attention(use_scale=False)\n",
    "out = net([qry, val], training=False)\n",
    "np.sum(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we reproduce the above using the lower level Tensorflow components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  similarity between the query and our keys\n",
    "scores = tf.matmul(qry, key, transpose_b=True)\n",
    "\n",
    "#  softmax to normalize\n",
    "dist = tf.nn.softmax(scores)\n",
    "\n",
    "#  apply our attention scores to the values\n",
    "out = tf.matmul(dist, val)\n",
    "print(np.sum(scores), np.var(dist), np.sum(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Implement dot-product attention without Tensorflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from answers import dot_product_attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention\n",
    "\n",
    "Relate a sequence to itself\n",
    "- learn the relationship between the words in a sentence\n",
    "- how similar is this word to other words in this sentence\n",
    "- rather than learning the relationship between two different sequences (such as source + target)\n",
    "- useful in machine reading, summarization & image caption generation\n",
    "\n",
    "Also known as introspective attention\n",
    "- attend to a networks own internal state (rather than data)\n",
    "\n",
    "## Multihead attention\n",
    "\n",
    "Just running the dot-product attention in parallel\n",
    "- outputs of all the heads are concatenated\n",
    "\n",
    "## Exercise\n",
    "\n",
    "Try to get the Tensorflow tutorial for translation with attention working on Colab\n",
    "- [Tutorial here](https://www.tensorflow.org/tutorials/text/nmt_with_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A few more things\n",
    "\n",
    "That don't fit in above :)\n",
    "\n",
    "\n",
    "### Hard versus soft attention\n",
    "\n",
    "Hard attention = fixed size windows\n",
    "- only one part of sequence at a time\n",
    "\n",
    "Soft attention = across entire sequence\n",
    "\n",
    "\n",
    "### Global vs local attention\n",
    "\n",
    "Global attenion is similar to soft attention\n",
    "\n",
    "Local attention\n",
    "- blend of hard & soft (also differentiable)\n",
    "- first predict a window, then do attention within that window\n",
    "\n",
    "\n",
    "## Resources Used\n",
    "\n",
    "Attention? Attention! - Lilian Wang - [text](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html) (read this if you don't have lots of time)\n",
    "\n",
    "Attention and Memory in Deep Learning - DeepMind 2018 Lecture - [youtube](https://www.youtube.com/watch?v=Q57rzaHHO0k)\n",
    "\n",
    "C5W3L07 Attention Model Intuition - [youtube](https://youtu.be/SysgYptB198)\n",
    "\n",
    "Attention and Memory in Deep Learning - DeepMind 2018 Lecture - [youtube](https://www.youtube.com/watch?v=Q57rzaHHO0k)\n",
    "\n",
    "Attention Is All You Need - [youtube](https://www.youtube.com/watch?v=iDulhoQ2pro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
